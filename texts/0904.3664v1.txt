Introduction to Machine Learning
67577 - Fall, 2008
Amnon Shashua
School of Computer Science and Engineering
The Hebrew University of Jerusalem
Jerusalem, Israel
arXiv:0904.3664v1  [cs.LG]  23 Apr 2009
Contents
1
Bayesian Decision Theory
page 1
1.1
Independence Constraints
5
1.1.1
Example: Coin Toss
7
1.1.2
Example: Gaussian Density Estimation
7
1.2
Incremental Bayes Classiï¬er
9
1.3
Bayes Classiï¬er for 2-class Normal Distributions
10
2
Maximum Likelihood/ Maximum Entropy Duality
12
2.1
ML and Empirical Distribution
12
2.2
Relative Entropy
14
2.3
Maximum Entropy and Duality ML/MaxEnt
15
3
EM Algorithm: ML over Mixture of Distributions
19
3.1
The EM Algorithm: General
21
3.2
EM with i.i.d. Data
24
3.3
Back to the Coins Example
24
3.4
Gaussian Mixture
26
3.5
Application Examples
27
3.5.1
Gaussian Mixture and Clustering
27
3.5.2
Multinomial Mixture and â€bag of wordsâ€ Application 27
4
Support Vector Machines and Kernel Functions
30
4.1
Large Margin Classiï¬er as a Quadratic Linear Programming 31
4.2
The Support Vector Machine
34
4.3
The Kernel Trick
36
4.3.1
The Homogeneous Polynomial Kernel
37
4.3.2
The non-homogeneous Polynomial Kernel
38
4.3.3
The RBF Kernel
39
4.3.4
Classifying New Instances
39
iii
iv
Contents
5
Spectral Analysis I: PCA, LDA, CCA
41
5.1
PCA: Statistical Perspective
42
5.1.1
Maximizing the Variance of Output Coordinates
43
5.1.2
Decorrelation: Diagonalization of the Covariance
Matrix
46
5.2
PCA: Optimal Reconstruction
47
5.3
The Case n >> m
49
5.4
Kernel PCA
49
5.5
Fisherâ€™s LDA: Basic Idea
50
5.6
Fisherâ€™s LDA: General Derivation
52
5.7
Fisherâ€™s LDA: 2-class
54
5.8
LDA versus SVM
54
5.9
Canonical Correlation Analysis
55
6
Spectral Analysis II: Clustering
58
6.1
K-means Algorithm for Clustering
59
6.1.1
Matrix Formulation of K-means
60
6.2
Min-Cut
62
6.3
Spectral Clustering: Ratio-Cuts and Normalized-Cuts
63
6.3.1
Ratio-Cuts
64
6.3.2
Normalized-Cuts
65
7
The Formal (PAC) Learning Model
69
7.1
The Formal Model
69
7.2
The Rectangle Learning Problem
73
7.3
Learnability of Finite Concept Classes
75
7.3.1
The Realizable Case
76
7.3.2
The Unrealizable Case
77
8
The VC Dimension
80
8.1
The VC Dimension
81
8.2
The Relation between VC dimension and PAC Learning
85
9
The Double-Sampling Theorem
89
9.1
A Polynomial Bound on the Sample Size m for PAC
Learning
89
9.2
Optimality of SVM Revisited
95
10
Appendix
97
Bibliography
105
1
Bayesian Decision Theory
During the next few lectures we will be looking at the inference from training
data problem as a random process modeled by the joint probability distribu-
tion over input (measurements) and output (say class labels) variables. In
general, estimating the underlying distribution is a daunting and unwieldy
task, but there are a number of constraints or â€tricks of the tradeâ€ so to
speak that under certain conditions make this task manageable and fairly
eï¬€ective.
To make things simple, we will assume a discrete world, i.e., that the
values of our random variables take on a ï¬nite number of values. Consider
for example two random variables X taking on k possible values x1, ..., xk
and H taking on two values h1, h2. The values of X could stand for a Body
Mass Index (BMI) measurement weight/height2 of a person and H stands
for the two possibilities h1 standing for the â€person being over-weightâ€ and
h2 as the possibility â€person of normal weightâ€. Given a BMI measurement
we would like to estimate the probability of the person being over-weight.
The joint probability P(X, H) is a two dimensional array (2-way array)
with 2k entries (cells). Each training example (xi, hj) falls into one of those
cells, therefore P(X = xi, H = hj) = P(xi, hj) holds the ratio between the
number of hits into cell (i, j) and the total number of training examples
(assuming the training data arrive i.i.d.). As a result P
ij P(xi, hj) = 1.
The projections of the array onto its vertical and horizontal axes by sum-
ming over columns or over rows is called marginalization and produces
P(hj) = P
i P(xi, hj) the sum over the jâ€™th row is the probability P(H = hj),
i.e., the probability of a person being over-weight (or not) before we see any
measurement â€” these are called priors. Likewise, P(xi) = P
j P(xi, hj)
is the probability P(X = xi) which is the probability of receiving such
a BMI measurement to begin with â€” this is often called evidence. Note
1
2
Bayesian Decision Theory
h1
2
5
4
2
1
h2
0
0
3
3
2
x1
x2
x3
x4
x5
Fig. 1.1. Joint probability P(X, H) where X ranges over 5 discrete values and H
over two values. Each entry contains the number of hits for the cell (xi, hj). The
joint probability P(xi, hj) is the number of hits divided by the total number of hits
(22). See text for more details.
that, by deï¬nition, P
j P(hj) = P
i P(xi) = 1. In Fig. 1.1 we have that
P(h1) = 14/22, P(h2) = 8/22 that is there is a higher prior probability of a
person being over-weight than being of normal weight. Also P(x3) = 7/22
is the highest meaning that we encounter BMI = x3 with the highest prob-
ability.
The conditional probability P(hj | xi) = P(xi, hj)/P(xi) is the ratio be-
tween the number of hits in cell (i, j) and the number of hits in the iâ€™th
column, i.e., the probability that the outcome is H = hj given the measure-
ment X = xi. In Fig. 1.1 we have P(h2 | x3) = 3/7. Note that
X
j
P(hj | xi) =
X
j
P(xi, hj)
P(xi)
=
1
P(xi)
X
j
P(xi, hj) = P(xi)/P(xi) = 1.
Likewise, the conditional probability P(xi | hj) = P(xi, hj)/P(hj) is the
number of hits in cell (i, j) normalized by the number of hits in the jâ€™th row
and represents the probability of receiving BMI = xi given the class label
H = hj (over-weight or not) of the person. In Fig. 1.1 we have P(x3 | h2) =
3/8 which is the probability of receiving BMI = x3 given that the person is
known to be of normal weight. Note that P
i P(xi | hj) = 1.
The Bayes formula arises from:
P(xi | hj)P(hj) = P(xi, hj) = P(hj | xi)P(xi),
from which we get:
P(hj | xi) = P(xi | hj)P(hj)
P(xi)
.
The left hand side P(hj | xi) is called the posterior probability and P(xi | hj)
is called the class conditional likelihood.
The Bayes formula provides a
way to estimate the posterior probability from the prior, evidence and class
likelihood. It is useful in cases where it is natural to compute (or collect
data of) the class likelihood, yet it is not quite simple to compute directly
Bayesian Decision Theory
3
the posterior.
For example, given a measurement â€12â€ we would like to
estimate the probability that the measurement came from tossing a pair
of dice or from spinning a roulette table. If x = 12 is our measurement,
and h1 stands for â€pair of diceâ€ and h2 for â€rouletteâ€ then it is natural
to compute the class conditional: P(â€12â€ | â€pair of diceâ€) = 1/36 and
P(â€12â€ | â€rouletteâ€) = 1/38.
Computing the posterior directly is much
more diï¬ƒcult. As another example, consider medical diagnosis. Once it is
known that a patient suï¬€ers from some disease hj, it is natural to evaluate
the probabilities P(xi | hj) of the emerging symptoms xi. As a result, in
many inference problems it is natural to use the class conditionals as the
basic building blocks and use the Bayes formula to invert those to obtain
the posteriors.
The Bayes rule can often lead to unintuitive results â€” the one in particu-
lar is known as â€base rate fallacyâ€ which shows how an nonuniform prior can
inï¬‚uence the mapping from likelihoods to posteriors. On an intuitive basis,
people tend to ignore priors and equate likelihoods to posteriors. The follow-
ing example is typical: consider the â€Cancer test kitâ€ problemâ€  which has the
following features: given that the subject has Cancer â€Câ€, the probability
of the test kit producing a positive decision â€+â€ is P(+ | C) = 0.98 (which
means that P(âˆ’| C) = 0.02) and the probability of the kit producing a neg-
ative decision â€-â€ given that the subject is healthy â€Hâ€ is P(âˆ’| H) = 0.97
(which means also that P(+ | H) = 0.03). The prior probability of Cancer
in the population is P(C) = 0.01. These numbers appear at ï¬rst glance
as quite reasonable, i.e, there is a probability of 98% that the test kit will
produce the correct indication given that the subject has Cancer. What
we are actually interested in is the probability that the subject has Cancer
given that the test kit generated a positive decision, i.e., P(C | +). Using
Bayes rule:
P(C | +) = P(+ | C)P(C)
P(+)
=
P(+ | C)P(C)
P(+ | C)P(C) + P(+ | H)P(H) = 0.266
which means that there is a 26.6% chance that the subject has Cancer given
that the test kit produced a positive response â€” by all means a very poor
performance.
If we draw the posteriors P(h1 |x) and P(h2 | x) using the probability
distribution array in Fig. 1.1 we will see that P(h1 |x) > P(h2 | x) for all
values of X smaller than a value which is in between x3 and x4. Therefore
the decision which will minimize the probability of misclassiï¬cation would
â€  This example is adopted from Yishai Mansourâ€™s class notes on Machine Learning.
4
Bayesian Decision Theory
be to choose the class with the maximal posterior:
hâˆ—= argmax
j
P(hj | x),
which is known as the Maximal A Posteriori (MAP) decision principle. Since
P(x) is simply a normalization factor, the MAP principle is equivalent to:
hâˆ—= argmax
j
P(x | hj)P(hj).
In the case where information about the prior P(h) is not known or it is
known that the prior is uniform, the we obtain the Maximum Likelihood
(ML) principle:
hâˆ—= argmax
j
P(x | hj).
The MAP principle is a particular case of a more general principle, known
as â€proper Bayesâ€, where a loss is incorporated into the decision process.
Let l(hi, hj) be the loss incurred by deciding on class hi when in fact hj is
the correct class. For example, the â€0/1â€ loss function is:
l(hi, hj) =
 1
i Ì¸= j
0
i = j

The least-squares loss function is: l(hi, hj) = âˆ¥hi âˆ’hjâˆ¥2 typically used when
the outcomes are vectors in some high dimensional space rather than class
labels. We deï¬ne the expected risk:
R(hi | x) =
X
j
l(hi, hj)P(hj | x).
The proper Bayes decision policy is to minimize the expected risk:
hâˆ—= argmin
j
R(hj | x).
The MAP policy arises in the case l(hi, hj) is the 0/1 loss function:
R(hi | x) =
X
jÌ¸=i
P(hj | x) = 1 âˆ’P(hi | x),
Thus,
argmin
j
R(hj | x) = argmax
j
P(hj | x).
1.1 Independence Constraints
5
1.1 Independence Constraints
At this point we may pause and ask what have we obtained?
well, not
much. Clearly, the inference problem is captured by the joint probability
distribution and we do not need all these formulas to see this.
How do
we obtain the necessary data to ï¬ll in the probability distribution array to
begin with? Clearly without additional simplifying constraints the task is
not practical as the size of these kind of arrays are exponential in the number
of variables. There are three families of simplifying constraints used in the
literature:
â€¢ statistical independence constraints,
â€¢ parametric form of the class likelihood P(xi | hj) where the inference
becomes a density estimation problem,
â€¢ structural assumptions â€” latent (hidden) variables, graphical models.
Today we will focus on the ï¬rst of these simplifying constraints â€” statistical
independence properties.
Consider two random variables X and Y . The variables are statistically
independent XâŠ¥Y if P(X | Y ) = P(X) meaning that information about
the value of Y does not add anything about X. The independence condition
is equivalent to the constraint: P(X, Y ) = P(X)P(Y ). This can be easily
proven: if XâŠ¥Y then P(X, Y ) = P(X | Y )P(Y ) = P(X)P(Y ). On the
other hand, if P(X, Y ) = P(X)P(Y ) then
P(X | Y ) = P(X, Y )
P(Y )
= P(X)P(Y )
P(Y )
= P(X).
Let the values of X range over x1, ..., xk and the values of Y range over
y1, ..., yl. The associated k Ã— l 2-way array, P(X = xi, Y = yj) is repre-
sented by the outer product P(xi, yj) = P(xi)P(yj) of two vectors P(X) =
(P(x1), ..., P(xk)) and P(Y ) = (P(y1), ..., P(yl)). In other words, the 2-way
array viewed as a matrix is of rank 1 and is determined by k + l (minus 2
because the sum of each vector is 1) parameters rather than kl (minus 1)
parameters.
Likewise, if X1âŠ¥X2âŠ¥....âŠ¥Xn are n statistically independent random vari-
ables where Xi ranges over ki discrete and distinct values, then the n-way
array P(X1, ..., Xn) = P(X1) Â· ... Â· P(Xn) is an outer-product of n vectors
and is therefore determined by k1 + ... + kn (minus n) parameters instead
of k1k2...kn (minus 1) parametersâ€ . Viewed as a tensor, the joint probabil-
â€  I am a bit over simplifying things because we are ignoring here the fact that the entries of
the array should be non-negative. This means that there are additional non-linear constraints
which eï¬€ectively reduce the number of parameters â€” but nevertheless it stays exponential.
6
Bayesian Decision Theory
ity is a rank 1 tensor. The main point is that the statistical independence
assumption reduced the representation of the multivariate joint distribution
from exponential to linear size.
Since our variables are typically divided to measurement variables and
an output/class variable H (or in general H1, ..., Hl), it is useful to intro-
duce another, weaker form, of independence known as conditional indepen-
dence. Variables X, Y are conditionally independent given H, denoted by
XâŠ¥Y | H, iï¬€P(X | Y, H) = P(X | H) meaning that given H, the value of Y
does not add any information about X. This is equivalent to the condition
P(X, Y | H) = P(X | H)P(Y | H). The proof goes as follows:
â€¢ If P(X | Y, H) = P(X | H), then
P(X, Y | H)
=
P(X, Y, H)
P(H)
= P(X | Y, H)P(Y, H)
P(H)
=
P(X | Y, H)P(Y | H)P(H)
P(H)
= P(X | H)P(Y | H)
â€¢ If P(X, Y | H) = P(X | H)P(Y | H), then
P(X | Y, H) = P(X, Y, H)
P(Y, H)
= P(X, Y | H)
P(Y | H)
= P(X | H).
Consider as an example, Joe and Mo live on opposite sides of the city.
Joe goes to work by train and Mo by car. Let X be the event â€Joe is late
to workâ€ and Y be the event â€Mo is late for workâ€. Clearly X and Y are
not independent because there could be other factors. For example, a train
strike will cause Joe to be late, but because of the strike there would be
extra traï¬ƒc (people using their car instead of the train) thus causing Mo to
be pate as well. Therefore, a third variable H standing for the event â€train
strikeâ€ would decouple X and Y .
From a computational standpoint, the conditional independence assump-
tion has a similar eï¬€ect to the unconditional independence. Let X range
over k distinct value, Y range over r distinct values and H range over s
distinct values. Then P(X, Y, H) is a 3-way array of size k Ã— r Ã— s. Given
that XâŠ¥Y | H means that P(X, Y | H = hi), a 2-way â€sliceâ€ of the 3-way
array along the H axis is represented by the outer-product of two vectors
P(X | H = hi)P(Y | H = hi). As a result the 3-way array is represented by
s(k +r âˆ’2) parameters instead of skr âˆ’1. Likewise, if X1âŠ¥....âŠ¥Xn | H then
the n-way array P(X1, ..., Xn | H = hi) (which is a slice along the H axis of
the (n + 1)-array P(X1, ..., Xn, H)) is represented by an outer-product of n
vectors, i.e., by k1 + .. + kn âˆ’n parameters.
1.1 Independence Constraints
7
1.1.1 Example: Coin Toss
We will use the ML principle to estimate the bias of a coin. Let X be a
random variable taking the value {0, 1} and H would be our hypothesis
taking a real value in [0, 1] standing for the coinâ€™s bias. If the coinâ€™s bias is
q then P(X = 0 | H = q) = q and P(X = 1 | H = q) = 1 âˆ’q. We receive m
i.i.d. examples x1, ..., xm where xi âˆˆ{0, 1}. We wish to determine the value
of q. Given that x1âŠ¥...âŠ¥xm | H, the ML problem we must solve is:
qâˆ—= argmax
q
P(x1, ..., xm | H = q) =
m
Y
i=1
P(xi | q) = argmax
q
X
i
log P(xi | q).
Let 0 â‰¤Î» â‰¤m stand for the number of â€™0â€™ instances, i.e., Î» = |{xi = 0 | i =
1, ..., m}|. Therefore our ML problem becomes:
qâˆ—= argmax
q
{Î» log q + (n âˆ’Î») log(1 âˆ’q)}
Taking the partial derivative with respect to q and setting it to zero:
âˆ‚
âˆ‚q[Î» log q + (n âˆ’Î») log(1 âˆ’q)] = Î»
qâˆ—âˆ’n âˆ’Î»
1 âˆ’qâˆ—= 0,
produces the result:
qâˆ—= Î»
n.
1.1.2 Example: Gaussian Density Estimation
So far we considered constraints induced by conditional independent state-
ments among the random variables as a means to reduce the space and time
complexity of the multivariate distribution array. Another approach would
be to assume some form of parametric form governing the entries of the array
â€” the most popular assumption is Gaussian distribution P(X1, ..., Xn) âˆ¼
N(Âµ, E) with mean vector Âµ and covariance matrix E. The parameters of
the density function are denoted by Î¸ = (Âµ, E) and for every vector x âˆˆRn
we have:
P(x | Î¸) =
1
(2Ï€)n/2|E|1/2 expâˆ’1
2 (xâˆ’Âµ)âŠ¤Eâˆ’1(xâˆ’Âµ) .
Assume we are given an i.i.d sample of k points S = {x1, ..., xk}, xi âˆˆRn,
and we would like to ï¬nd the Bayes optimal Î¸:
Î¸âˆ—= argmax
Î¸
P(S | Î¸),
8
Bayesian Decision Theory
by maximizing the likelihood (here we are assuming that the the priors P(Î¸)
are equal, thus the maximum likelihood and the MAP would produce the
same result). Because the sample was drawn i.i.d. we can assume that:
P(S | Î¸) =
k
Y
i=1
P(xi | Î¸).
Let L(Î¸) = log P(S | Î¸) = P
i log P(xi | Î¸) and since Log is monotonously
increasing we have that Î¸âˆ—= argmax
Î¸
L(Î¸). The parameter estimation would
be recovered by taking derivatives with respect to Î¸, i.e., âˆ‡Î¸L = 0. We have:
L(Î¸) = âˆ’1
2 log |E| âˆ’
k
X
i=1
n
2 log(2Ï€) âˆ’
X
i
1
2(xi âˆ’Âµ)âŠ¤Eâˆ’1(xi âˆ’Âµ).
(1.1)
We will start with a simple scenario where E = Ïƒ2I, i.e., all the covariances
are zero and all the variances are equal to Ïƒ2.
Thus, Eâˆ’1 = Ïƒâˆ’2I and
|E| = Ïƒ2n. After substitution (and removal of items which do not depend
on Î¸) we have:
L(Î¸) = âˆ’nk log Ïƒ âˆ’1
2
X
i
âˆ¥xi âˆ’Âµâˆ¥2
Ïƒ2
.
The partial derivative with respect to Âµ:
âˆ‚L
âˆ‚Âµ = Ïƒâˆ’2 X
i
(Âµ âˆ’xi) = 0
from which we obtain:
Âµ = 1
k
k
X
i=1
xi.
The partial derivative with respect to Ïƒ is:
âˆ‚L
âˆ‚Ïƒ = nk
Ïƒ âˆ’Ïƒâˆ’3 X
i
âˆ¥xi âˆ’Âµâˆ¥2 = 0,
from which we obtain:
Ïƒ2 = 1
kn
k
X
i=1
âˆ¥xi âˆ’Âµâˆ¥2.
Note that the reason for dividing by n is due to the fact that Ïƒ2
1 = ... =
Ïƒ2
n = Ïƒ2, so that:
1
k
k
X
i=1
âˆ¥xi âˆ’Âµâˆ¥2 =
n
X
j=1
Ïƒ2
j = nÏƒ2.
1.2 Incremental Bayes Classiï¬er
9
In the general case, E is a full rank symmetric matrix, then the derivative
of eqn. (1.1) with respect to Âµ is:
âˆ‚L
âˆ‚Âµ = Eâˆ’1 X
i
(Âµ âˆ’xi) = 0,
and since Eâˆ’1 is full rank we obtain Âµ = (1/k) P
i xi. For the derivative
with respect to E we note two auxiliary items:
âˆ‚|E|
âˆ‚E = |E|Eâˆ’1,
âˆ‚
âˆ‚E trace(AEâˆ’1) = âˆ’(Eâˆ’1AEâˆ’1)âŠ¤.
Using the fact that xâŠ¤y = trace(xyâŠ¤) we can transform zâŠ¤Eâˆ’1z to trace(zzâŠ¤Eâˆ’1)
for any vector z. Given that Eâˆ’1 is symmetric, then:
âˆ‚
âˆ‚E trace(zzâŠ¤Eâˆ’1) = âˆ’Eâˆ’1zzâŠ¤Eâˆ’1.
Substituting z = x âˆ’Âµ we obtain:
âˆ‚L
âˆ‚E = âˆ’kEâˆ’1 + Eâˆ’1
 X
i
(xi âˆ’Âµ)(xi âˆ’Âµ)âŠ¤
!
Eâˆ’1 = 0,
from which we obtain:
E = 1
k
k
X
i=1
(xi âˆ’Âµ)(xi âˆ’Âµ)âŠ¤.
1.2 Incremental Bayes Classiï¬er
Consider another application of conditional dependence which is the Bayes
incremental rule. Suppose we have processed n examples X(n) = {X1, ..., Xn}
and computed somehow P(H | X(n)). We are given a new measurement X
and wish to compute (update) the posterior P(H | X(n), X). We will use
the chain ruleâ€ :
P(X | Y, Z) = P(X, Y, Z)
P(Y, Z
) = P(Z | X, Y )P(X | Y )P(Y )
P(Z | Y )P(Y )
= P(Z | X, Y )P(X | Y )
P(Z | Y )
to obtain:
P(H | X(n), X) = P(X | X(n), H)P(H | X(n))
P(X | X(n))
from conditional independence, P(X | X(n), H) = P(X | H). The term
P(X | X(n)) can expanded as follows:
â€  this is based on the rule P(X1, ..., Xn)
=
P(X1 | X2, ..., Xn)P(X2 | X3, ..., Xn) Â· Â· Â·
P(Xnâˆ’1 | Xn)P(Xn)
10
Bayesian Decision Theory
P(X | X(n))
=
X
i
P(X, X(n) | H = hi)P(H = hi)
P(X(n))
=
X
i
P(X | H = hi)P(X(n) | H = hi)P(H = hi)
P(X(n))
=
X
i
P(X | H = hi)P(H = hi | X(n))
After substitution we obtain:
P(H = hi | X(n), X) =
P(X | H = hi)P(H = hi | X(n))
P
j P(X | H = hj)P(H = hj | X(n)).
The old posterior P(H | X(n)) is now the prior for the updated formula.
Consider the following exampleâ€ : We have a coin which could be either fair
or biased towards Head at a probability of 0.6. Let H = h1 be the event
that the coin is fair, and H = h2 that the coin is biased. We start with prior
probabilities P(h1) = 0.75 and P(h2) = 0.25 (we have a higher initial belief
that the coin is fair). Suppose our ï¬rst coin toss is a Head, i.e., X1 = â€0â€.
Then,
P(h1 | x1) = P(x1 | h1)P(h1)
P(x1)
=
0.5 âˆ—0.75
0.5 âˆ—0.75 + 0.6 âˆ—0.25 = 0.714
and P(h2 | x1) = 0.286. Our posterior belief that the coin is fair has gone
down after a Head toss. Assume we have another measurement X2 = â€0â€,
then:
P(h1 | x1, x2) = P(x2 | h1)P(h1 | x1)
normalization
=
0.5 âˆ—0.714
0.5 âˆ—0.714 + 0.6 âˆ—0.286 = 0.675,
and P(h2 | x1, x2) = 0.325, thus our belief that the coin is fair continues to
go down after Head tosses.
1.3 Bayes Classiï¬er for 2-class Normal Distributions
For the last topic in this lecture consider the 2-class inference problem. We
will encountered this problem in this course in the context of SVM and
LDA. In the Bayes framework, if H = {h1, h2} denotes the â€class memberâ€
variable with two possible outcomes, then the MAP decision policy calls for
â€  adopted from Ron Rivestâ€™s 1994 class notes.
1.3 Bayes Classiï¬er for 2-class Normal Distributions
11
making the decision based on data x:
hâˆ—= argmax
h1,h2
{P(h1 | x), P(h2 | x)} ,
or in other words the class h1 would be chosen if P(h1 | x) > P(h2 | x).
The decision surface (as a function of x) is therefore described by:
P(h1 | x) âˆ’P(h2 | x) = 0.
The questions we ask here is what would the Bayes optimal decision sur-
face be like if we assume that the two classes are normally distributed with
diï¬€erent means and the same covariance matrix? What we will see is that
under the condition of equal priors P(h1) = P(h2) the decision surface is
a hyperplane â€” and not only that, it is the same hyperplane produced by
LDA.
Claim 1 If P(h1) = P(h2) and P(x | h1) âˆ¼N(Âµ1, E) and P(x | h1) âˆ¼
N(Âµ2, E), the the Bayes optimal decision surface is a hyperplane wâŠ¤(x âˆ’
Âµ) = 0 where Âµ = (Âµ1 + Âµ2)/2 and w = Eâˆ’1(Âµ1 âˆ’Âµ2). In other words, the
decision surface is described by:
xâŠ¤Eâˆ’1(Âµ1 âˆ’Âµ2) âˆ’1
2(Âµ1 + Âµ2)Eâˆ’1(Âµ1 âˆ’Âµ2) = 0.
(1.2)
Proof:
The decision surface is described by P(h1 | x) âˆ’P(h2 | x) = 0
which is equivalent to the statement that the ratio of the posteriors is 1, or
equivalently that the log of the ratio is zero, and using Bayes formula we
obtain:
0 = log P(x | h1)P(h1)
P(x | h2)P(h2) = log P(x | h1)
P(x | h2).
In other words, the decision surface is described by
log P(x | h1)âˆ’log P(x | h2) = âˆ’1
2(xâˆ’Âµ1)âŠ¤Eâˆ’1(xâˆ’Âµ1)+1
2(xâˆ’Âµ2)âŠ¤Eâˆ’1(xâˆ’Âµ2) = 0.
After expanding the two terms we obtain eqn. (1.2).
2
Maximum Likelihood/ Maximum Entropy Duality
In the previous lecture we deï¬ned the principle of Maximum Likelihood
(ML): suppose we have random variables X1, ..., Xn form a random sample
from a discrete distribution whose joint probability distribution is P(x | Ï†)
where x = (x1, ..., xn) is a vector in the sample and Ï† is a parameter from
some parameter space (which could be a discrete set of values â€” say class
membership). When P(x | Ï†) is considered as a function of Ï† it is called the
likelihood function. The ML principle is to select the value of Ï† that maxi-
mizes the likelihood function over the observations (training set) x1, ..., xm.
If the observations are sampled i.i.d. (a common, not always valid, assump-
tion), then the ML principle is to maximize:
Ï†âˆ—= argmax
Ï†
m
Y
i=1
P(xi | Ï†) = argmax log
m
Y
i=1
P(xi | Ï†) = argmax
m
X
i=1
log P(xi | Ï†)
which due to the product nature of the problem it becomes more convenient
to maximize the log likelihood.
We will take a closer look today at the
ML principle by introducing a key element known as the relative entropy
measure between distributions.
2.1 ML and Empirical Distribution
The ML principle states that the empirical distribution of an i.i.d. sequence
of examples is the closest possible (in terms of relative entropy which would
be deï¬ned later) to the true distribution.
To make this statement clear
let X be a set of symbols {a1, ..., an} and let P(a | Î¸) be the probability
(belonging to a parametric family with parameter Î¸) of drawing a symbol
a âˆˆX. Let x1, ..., xm be a sequence of symbols drawn i.i.d. according to P.
The occurrence frequency f(a) measures the number of draws of the symbol
12
2.1 ML and Empirical Distribution
13
a:
f(a) = |{i : xi = a}|,
and let the empirical distribution be deï¬ned by
Ë†P(a) =
1
P
Î±âˆˆX f(Î±)f(a) =
1
âˆ¥fâˆ¥1
f(a) = (1/m)f(a).
The joint probability P(x1, ..., xm | Ï†) is equal to the product Q
i P(xi | Ï†)
which according to the deï¬nitions above is equal to:
P(x1, ..., xm | Ï†) =
m
Y
i=1
p(xi | Î¸) =
Y
aâˆˆX
P(a | Ï†)f(a).
The ML principle is therefore equivalent to the optimization problem:
max
PâˆˆQ
Y
aâˆˆX
P(a | Ï†)f(a)
(2.1)
where Q = {q âˆˆRn : q â‰¥0, P
i qi = 1} denote the set of n-dimensional
probability vectors (â€probability simplexâ€). Let pi stand for P(ai | Ï†) and
fi stand for f(ai). Since argmaxxz(x) = argmaxx ln z(x) and given that
ln Q
i pfi
i = P
i fi ln pi the solution to this problem can be found by setting
the partial derivative of the Lagrangian to zero:
L(p, Î», Âµ) =
n
X
i=1
fi ln pi âˆ’Î»(
X
i
pi âˆ’1) âˆ’
X
i
Âµipi,
where Î» is the Lagrange multiplier associated with the equality constraint
P
i pi âˆ’1 = 0 and Âµi â‰¥0 are the Lagrange multipliers associated with the
inequality constraints pi â‰¥0. We also have the complementary slackness
condition that sets Âµi = 0 if pi > 0.
After setting the partial derivative with respect to pi to zero we get:
pi =
1
Î» + Âµi
fi.
Assume for now that fi > 0 for i = 1, ..., n. Then from complementary
slackness we must have Âµi = 0 (because pi > 0).
We are left therefore
with the result pi = (1/Î»)fi. Following the constraint P
i p1 = 1 we obtain
Î» = P
i fi. As a result we obtain: P(a | Ï†) = Ë†P(a). In case fi = 0 we could
use the convention 0 ln 0 = 0 and from continuity arrive to pi = 0.
We have arrived to the following theorem:
Theorem 1 The empirical distribution estimate Ë†P is the unique Maximum
14
Maximum Likelihood/ Maximum Entropy Duality
Likelihood estimate of the probability model Q on the occurrence frequency
f().
This seems like an obvious result but it actually runs deep because the result
holds for a very particular (and non-intuitive at ï¬rst glance) distance mea-
sure between non-negative vectors. Let dist(f, p) be some distance measure
between the two vectors. The result above states that:
Ë†P = argmin
p
dist(f, p) s.t. p â‰¥0,
X
i
pi = 1,
(2.2)
for some (family?)
of distance measures dist().
It turns out that there
is only oneâ€  such distance measure, known as the relative-entropy, which
satisï¬es the ML result stated above.
2.2 Relative Entropy
The relative-entropy (RE) measure D(x||y) between two non-negative vec-
tors x, y âˆˆRn is deï¬ned as:
D(x||y) =
n
X
i=1
xi ln xi
yi
âˆ’
X
i
xi +
X
i
yi.
In the deï¬nition we use the convention that 0 ln 0
0 = 0 and based on con-
tinuity that 0 ln 0
y = 0 and x ln x
0 = âˆ.
When x, y are also probability
vectors, i.e., belong to Q, then D(x||y) = P
i xi ln xi
yi is also known as the
Kullback-Leibler divergence. The RE measure is not a distance metric as
it is not symmetric, D(x||y) Ì¸= D(y||x), and does not satisfy the triangle
inequality. Nevertheless, it has several interesting properties which make it
a fundamental measure in statistical inference.
The relative entropy is always non-negative and is zero if and only if
x = y. This comes about from the log-sum inequality:
X
i
xi ln xi
yi
â‰¥(
X
i
xi) ln
P
i xi
P
i yi
Thus,
D(x||y) â‰¥(
X
i
xi) ln
P
i xi
P
i yi
âˆ’
X
i
xi +
X
i
yi = Â¯x ln Â¯x
Â¯y âˆ’Â¯x + Â¯y
â€  not exactly â€” the picture is a bit more complex.
Csiszarâ€™s 1972 measures:
dist(p, f) =
P
i fiÏ†(pi/fi) will satisfy eqn. 2.2 provided that Ï†â€²âˆ’1 is an exponential. However, dist(f, p)
(parameters positions are switched) will not do it, whereas the relative entropy will satisfy
eqn. 2.2 regardless of the order of the parameters p, f.
2.3 Maximum Entropy and Duality ML/MaxEnt
15
But a ln(a/b) â‰¥a âˆ’b for a, b â‰¥0 iï¬€ln(a/b) â‰¥1 âˆ’(b/a) which follows from
the inequality ln(x + 1) > x/(x + 1) (which holds for x > âˆ’1 and x Ì¸= 0).
We can state the following theorem:
Theorem 2 Let f â‰¥0 be the occurrence frequency on a training sample.
Ë†P âˆˆQ is a ML estimate iï¬€
Ë†P = argmin
p
D(f||p) s.t. p â‰¥0,
X
i
pi = 1.
Proof:
D(f||p) = âˆ’
X
i
fi ln pi +
X
i
fi ln fi âˆ’
X
i
fi + 1,
and
argmin
p
D(f||p) = argmax
p
X
i
fi ln pi = argmax
p
ln
Y
i
pfi
i .
There are two (related) interesting points to make here. First, from the
proof of Thm. 1 we observe that the non-negativity constraint p â‰¥0 need
not be enforced - as long as f â‰¥0 (which holds by deï¬nition) the closest p
to f under the constraint P
i pi = 1 must come out non-negative. Second,
the fact that the closest point p to f comes out as a scaling of f (which is by
deï¬nition the empirical distribution Ë†P) arises because of the relative-entropy
measure.
For example, if we had used a least-squares distance measure
âˆ¥f âˆ’pâˆ¥2 the result would not be a scaling of f.
In other words, we are
looking for a projection of the vector f onto the probability simplex, i.e.,
the intersection of the hyperplane xâŠ¤1 = 1 and the non-negative orthant
x â‰¥0. Under relative-entropy the projection is simply a scaling of f (and
this is why we do not need to enforce non-negativity). Under least-sqaures,
a projection onto the hyper-plane xâŠ¤1 = 1 could take us out of the non-
negative orthant (see Fig. 2.1 for illustration). So, relative-entropy is special
in that regard â€” it not only provides the ML estimate, but also simpliï¬es
the optimization processâ€  (something which would be more noticeable when
we handle a latent class model next lecture).
2.3 Maximum Entropy and Duality ML/MaxEnt
The relative-entropy measure is not symmetric thus we expect diï¬€erent out-
comes of the optimization minx D(x||y) compared to miny D(x||y). The lat-
â€  The fact that non-negativity â€comes for freeâ€ does not apply for all class (distribution) models.
This point would be reï¬ned in the next lecture.
16
Maximum Likelihood/ Maximum Entropy Duality
f
p
^
p
2
Fig. 2.1. Projection of a non-neagtaive vector f onto the hyperplane P
i xi âˆ’1 = 0.
Under relative-entropy the projection Ë†P is a scaling of f (and thus lives in the
probability simplex).
Under least-squares the projection p2 lives outside of the
probability simplex, i.e., could have negative coordinates.
ter of the two, i.e., minPâˆˆQ D(P0||P), where P0 is some empirical evidence
and Q is some model, provides the ML estimation. For example, in the
next lecture we will consider Q the set of low-rank joint distributions (called
latent class model) and see how the ML (via relative-entropy minimization)
solution can be found.
Let H(p) = âˆ’P
i pi ln pi denote the entropy function. With regard to
minx D(x||y) we can state the following observation:
Claim 2
argmin
pâˆˆQ
D(p|| 1
n1) = argmax
pâˆˆQ
H(p).
Proof:
D(p|| 1
n1) =
X
i
pi ln pi + (
X
i
pi) ln(n) = ln(n) âˆ’H(p),
which follows from the condition P
i pi = 1.
In other words, the closest distribution to uniform is achieved by maxi-
mizing the entropy. To make this interesting we need to add constraints.
Consider a linear constraint on p such as P
i Î±ipi = Î². To be concrete, con-
2.3 Maximum Entropy and Duality ML/MaxEnt
17
sider a die with six faces thrown many times and we wish to estimate the
probabilities p1, ..., p6 given only the average P
i ipi. Say, the average is 3.5
which is what one would expect from an unbiased die. The Laplaceâ€™s prin-
ciple of insuï¬ƒcient reasoning calls for assuming uniformity unless there is
additional information (a controversial assumption in some cases). In other
words, if we have no information except that each pi â‰¥0 and that P
i pi = 1
we should choose the uniform distribution since we have no reason to choose
any other distribution. Thus, employing Laplaceâ€™s principle we would say
that if the average is 3.5 then the most â€likelyâ€ distribution is the uniform.
What if Î² = 4.2? This kind of problem can be stated as an optimization
problem:
max
p H(p) s.t.,
X
i
pi = 1,
X
i
Î±ipi = Î²,
where Î±i = i and Î² = 4.2. We have now two constraints and with the aid
of Lagrange multipliers we can arrive to the result:
pi = expâˆ’(1âˆ’Î») expÂµÎ±i .
Note that because of the exponential pi â‰¥0 and again â€non-negativity
comes for freeâ€â€ . Following the constraint P
i pi = 1 we get expâˆ’(1âˆ’Î») =
1/ P
i expÂµÎ±i from which obtain:
pi = 1
Z expÂµÎ±i,
where Z (a function of Âµ) is a normalization factor and Âµ needs to be set by
using Î² (see later). There is nothing special about the uniform distribution,
thus we could be seeking a probability vector p as close as possible to some
prior probability p0 under the constraints above:
min
p D(p||p0) s.t.,
X
i
pi = 1,
X
i
Î±ipi = Î²,
with the result:
pi = 1
Z p0i expÂµÎ±i .
We could also consider adding more linear constraints on p of the form:
P
i fijpi = bj, j = 1, ..., k. The result would be:
pi = 1
Z p0i exp
Pk
j=1 Âµjfij .
Probability distributions of this form are called Gibbs Distributions.
In
â€ 
Any measure of the class dist(p, p0) = P
i p0iÏ†(pi/p0i) minimized under linear constraints
will satisfy the result of pi â‰¥0 provided that Ï†â€²âˆ’1 is an exponential.
18
Maximum Likelihood/ Maximum Entropy Duality
practical applications the linear constraints on p could arise from average
information about the system such as temperature of a ï¬‚uid (where pi are
the probabilities of the particles moving at various velocities), rainfall data
or general environmental data (where pi represent the probability of ï¬nding
animal colonies at discrete locations in a 3D map).
A constraint of the
form P
i fijpi = bj states that the expectation Ep[fj] should be equal to
the empirical distribution Î² = E Ë†P [fj] where Ë†P is either uniform or given as
input. Let
P = {p âˆˆRn : p â‰¥0,
X
i
pi = 1, Ep[fj] = EË†p[fj], j = 1, ..., k},
and
Q = {q âˆˆRn ; q is a Gibbs distribution}
We could therefore consider looking for the ML solution for the parameters
Âµ1, ..., Âµk of the Gibbs distribution:
min
qâˆˆQ D(Ë†p||q),
where if Ë†p is uniform then min D(Ë†p||q) can be replaced by max P
i ln qi
(because D((1/n)1||x) = âˆ’ln(n) âˆ’P
i ln xi).
As it turns out, the MaxEnt and ML are duals of each other and the
intersection of the two sets P âˆ©Q contains only a single point which solves
both problems.
Theorem 3 The following are equivalent:
â€¢ MaxEnt: qâˆ—= argminpâˆˆPD(p||p0)
â€¢ ML: qâˆ—= argminqâˆˆQD(Ë†p||q)
â€¢ qâˆ—âˆˆP âˆ©Q
In practice, the duality theorem is used to recover the parameters of the
Gibbs distribution using the ML route (second line in the theorem above)
â€” the algorithm for doing so is known as the iterative scaling algorithm
(which we will not get into).
3
EM Algorithm: ML over Mixture of Distributions
In Lecture 2 we saw that the Maximum Likelihood (ML) principle over i.i.d.
data is achieved by minimizing the relative entropy between a model Q and
the occurrence-frequency of the training data. Speciï¬cally, let x1, .., xm be
i.i.d. where each xi âˆˆX d is a d-tupple of symbols taken from an alphabet X
having n diï¬€erent letters {a1, ..., an}. Let Ë†P be the empirical joint distribu-
tion, i.e., an array with d dimensions where each axis has n entries, i.e., each
entry Ë†Pi1,...,id, where ij = 1, ..., n, represents the (normalized) co-occurrence
of the d-tupe ai1, ..., aid in the training set x1, ..., xm. We wish to ï¬nd a
joint distribution P âˆ—(also a d-array) which belongs to some model family
of distributions Q closest as possible to Ë†P in relative-entropy:
P âˆ—= argmin
PâˆˆQ
D( Ë†P||P).
In this lecture we will focus on a model of distributions Q which represents
mixtures of simple distributions Hâ€” known as latent class models. A latent
class model arises when the joint probability P(X1, ..., Xd) we observe (i.e.,
from which Ë†P is generated by observing samples x1, ..., xm) is in fact a
marginal of P(X1, ..., Xd, Y ) where Y is a â€hiddenâ€ (or â€latentâ€) random
variable which has k diï¬€erent discrete values Î±1, .., Î±k. Then,
P(X1, ..., Xd) =
k
X
j=1
P(X1, ..., Xd | Y = Î±j)P(Y = Î±j).
The idea is that given the value of the hidden variable H the problem of
recovering the model P(X1, ..., Xd | Y = Î±j), which belongs to some family
of joint distributions H, is a relatively simple problem. To make this idea
clearer we consider the following example: Assume we have two coins. The
ï¬rst coin has a probability of heads (â€0â€) equal to p and the second coin
has a probability of heads equal to q. At each trial we choose to toss coin 1
19
20
EM Algorithm: ML over Mixture of Distributions
with probability Î» and coin 2 with probability 1 âˆ’Î». Once a coin has been
chosen it is tossed 3 times, producing an observation x âˆˆ{0, 1}3. We are
given a set of such observations D = {x1, ..., xm} where each observation xi
is a triplet of coin tosses (the same coin). Given D, we can construct the
empirical distribution Ë†P which is a 2 Ã— 2 Ã— 2 array deï¬ned as:
Ë†Pi1,i2,i3 = 1
m|{xi = {i1, i2, i3}, i = 1, ..., m}|.
Let yi âˆˆ{1, 2} be a random variable associated with the observation xi such
that yi = 1 if xi was generated by coin 1 and yi = 2 if xi was generated
by coin 2.
If we knew the values of yi then our task would be simply
to estimate two separate Bernoulli distributions by separating the triplets
generated from coin 1 from those generated by coin 2. Since yi is not known,
we have the marginal:
P(x = (x1, x2, x3))
=
P(x = (x1, x2, x3) | y = 1)P(y = 1)
+
P(x = (x1, x2, x3) | y = 2)P(y = 2)
=
Î»pni(1 âˆ’p)(3âˆ’ni) + (1 âˆ’Î»)qni(1 âˆ’q)(3âˆ’ni),(3.1)
where (x1, x2, x3) âˆˆ{0, 1}3 is a triplet coin toss and 0 â‰¤ni â‰¤3 is the
number of heads (â€0â€) in the triplet of tosses. In other words, the likelihood
P(x) of triplet of tosses x = (x1, x2, x3) is a linear combination (â€mixtureâ€)
of two Bernoulli distributions. Let H stand for Bernoulli distributions:
H = {uâŠ—d : u â‰¥0,
n
X
i=1
ui = 1}
where uâŠ—d stands for the outer-product of u âˆˆRn with itself d times, i.e.,
an n- way array indexed by i1, ..., id, where ij âˆˆ{1, ..., n}, and whose value
there is equal to ui1 Â· Â· Â· uid. The model family Q is a mixture of Bernoulli
distributions:
Q = {
k
X
j=1
Î»jPj : Î» â‰¥0,
X
j
Î»j = 1, Pj âˆˆH},
where speciï¬cally for our coin-toss example becomes:
Q = {Î»

p
1 âˆ’p
âŠ—3
+ (1 âˆ’Î»)

q
1 âˆ’q
âŠ—3
: Î», p, q âˆˆ[0, 1]}
We see therefore that the eight entries of P âˆ—âˆˆQ which minimizes D( Ë†P||P)
over the set Q is determined by three parameters Î», p, q. For the coin-toss
3.1 The EM Algorithm: General
21
example this looks like:
argmin
0â‰¤Î»,p,qâ‰¤1
D
 
Ë†P || Î»

p
1 âˆ’p
âŠ—3
+ (1 âˆ’Î»)

q
1 âˆ’q
âŠ—3!
= argmax
0â‰¤Î»,p,qâ‰¤1
1
X
i1=0
1
X
i2=0
1
X
i3=0
Ë†Pi1i2i3 log

Î»pni123(1 âˆ’p)(3âˆ’ni123) + (1 âˆ’Î»)qni123(1 âˆ’q)(3âˆ’ni123)
where ni123 = i1 + i2 + i3. Trying to work out an algorithm for minimizing
the unknown parameters Î», p, q would be somewhat â€unpleasantâ€ (and even
more so for other families of distributions H) because of the log-over-a-sum
present in the optimization function â€” if we could somehow turn this into
a sum-over-log our task would be much easier. We would then be able to
turn the problem into a succession of problems over H rather than a single
problem over Q = P
j Î»jH.
Another point worth attention is the non-
negativity of the output variables â€” simply minimizing the relative-entropy
measure under the constraints of the class model Q would not guarantee a
non-negative solution. As we shall see, breaking down the problem into a
successions of problems over H would give us the â€non-negativity for freeâ€
feature.
The technique for turning the log-over-sum into a sum-over-log as part of
ï¬nding the ML solution for a mixture model is known as the Expectation-
Maximization (EM) algorithm introduced by Dempster, Laird and Rubin in
1977. It is based on two ideas: (i) introduce auxiliary variables, and (ii) use
of Jensenâ€™s inequality.
3.1 The EM Algorithm: General
Let D = {x1, ..., xm} represent the training data where xi âˆˆX is taken from
some instance space X which we leave unspeciï¬ed. For now, we leave matters
to be as general as possible and speciï¬cally we do not make independence
assumptions on the data generation process.
The ML problem is to ï¬nd a setting of parameters Î¸ which maximizes
the likelihood P(x1, ..., xm | Î¸), namely, we wish to maximize P(D | Î¸) over
parameters Î¸, which is equivalent to maximizing the log-likelihood:
Î¸âˆ—= argmax
Î¸
log P(D | Î¸) = log
ï£«
ï£­X
y
P(D, y | Î¸)
ï£¶
ï£¸,
where y represents the hidden variables. We will denote L(Î¸) = log P(D | Î¸).
22
EM Algorithm: ML over Mixture of Distributions
Let q(y | D, Î¸) be some (arbitrary) distribution of the hidden variables y con-
ditioned on the parameters Î¸ and the input sample D, i.e., P
y q(y | D, Î¸) =
1. We deï¬ne a lower bound on L(Î¸) as follows:
L(Î¸)
=
log
ï£«
ï£­X
y
P(D, y | Î¸)
ï£¶
ï£¸
(3.2)
=
log
ï£«
ï£­X
y
q(y | D, Î¸)P(D, y | Î¸)
q(y | D, Î¸)
ï£¶
ï£¸
(3.3)
â‰¥
X
y
q(y | D, Î¸) log P(D, y | Î¸)
q(y | D, Î¸)
(3.4)
=
Q(q, Î¸).
(3.5)
The inequality comes from Jensenâ€™s inequality log P
j Î±jaj â‰¥P
j Î±j log aj
when P
j Î±j = 1. What we have obtained is an â€auxiliaryâ€ function Q(q, Î¸)
satisfying
L(Î¸) â‰¥Q(q, Î¸),
for all distributions q(y | D, Î¸). The maximization of Q(q, Î¸) proceeds by
interleaving the variables q and Î¸ as we separately ascend on each set of
variables. At the (t + 1) iteration we ï¬x the current value of Î¸ to be Î¸(t)
of the tâ€™th iteration and maximize Q(q, Î¸(t)) over q, and then maximize
Q(q(t+1), Î¸) over Î¸:
q(t+1)
=
argmax
q
Q(q, Î¸(t))
(3.6)
Î¸(t+1)
=
argmax
Î¸
Q(q(t+1), Î¸).
(3.7)
The strategy of the EM algorithm is to maximize the lower bound Q(q, Î¸)
with the hope that if we ascend on the lower bound function we will also
ascend with respect to L(Î¸). The claim below guarantees that an ascend on
Q will also generate an ascend on L:
Claim 3 (Jordan-Bishop) The optimal q(y | D, Î¸(t)) at each step is
P(y | D, Î¸(t)).
Proof: We will show that Q(P(y | D, Î¸(t)), Î¸(t)) = L(Î¸(t)) which proves the
claim since L(Î¸) â‰¥Q(q, Î¸) for all q, Î¸, thus the best q-distribution we can
3.1 The EM Algorithm: General
23
hope to ï¬nd is one that makes the lower-bound meet L(Î¸) at Î¸ = Î¸(t).
Q(P(y | D, Î¸(t)), Î¸(t))
=
X
y
P(y | D, Î¸(t)) log P(D, y | Î¸(t))
P(y | D, Î¸(t))
=
X
y
P(y | D, Î¸(t)) log P(y | D, Î¸(t))P(D | Î¸(t))
P(y | D, Î¸(t))
=
log P(D | Î¸(t))
X
y
P(y | D, Î¸(t))
=
L(Î¸(t))
The proof provides also the validity for the approach of ascending along
the lower bound Q(q, Î¸) because at the point Î¸(t) the two functions coincide,
i.e., the lower bound function at Î¸ = Î¸(t) is equal to L(Î¸(t)) therefore if
we continue and ascend along Q(Â·) we are guaranteed to ascend along L(Î¸)
as wellâ€  â€” therefore, convergence is guaranteed. It can also be shown (but
omitted here) that the point of convergence is a stationary point of L(Î¸) (was
shown originally by C.F. Jeï¬€Wu in 1983 years after EM was introduced in
1977) under fairly general conditions. The second step of maximizing over
Î¸ then becomes:
Î¸(t+1) = argmax
Î¸
X
y
P(y | D, Î¸(t)) log P(D, y | Î¸).
(3.8)
This deï¬nes the EM algorithm. Often the â€Expectationâ€ step is described
as taking the expectation of:
Eyâˆ¼P(y | D,Î¸(t)) [log P(D, y | Î¸)] ,
followed by a Maximization step of ï¬nding Î¸ that maximizes the expectation
â€” hence the term EM for this algorithm.
Eqn. 3.8 describes a principle but not an algorithm because in general,
without making assumptions on the statistical relationship between the data
points and the hidden variable the problem presented in eqn. 3.8 is unwieldy.
We will reduce eqn. 3.8 to something more manageable by making the i.i.d.
assumption. This is detailed in the following section.
â€  this manner of deriving EM was adapted from Jordan and Bishopâ€™s book notes, 2001.
24
EM Algorithm: ML over Mixture of Distributions
3.2 EM with i.i.d. Data
The EM optimization presented in eqn. 3.8 can be simpliï¬ed if we assume
the data points (and the hidden variable values) are i.i.d.
P(D | Î¸) =
n
Y
i=1
P(xi | Î¸),
P(D, y | Î¸) =
n
Y
i=1
P(xi, yi | Î¸),
and
P(y | D, Î¸) =
n
Y
i=1
P(yi | xi, Î¸).
For any Î±(yi) we have:
X
y
Î±(yi)P(y | D, Î¸)
=
X
y1
Â· Â· Â·
X
yn
Î±(yi)P(y1 | x1, Î¸) Â· Â· Â· P(yn | xn, Î¸)
=
X
yi
Î±(yi)P(yi | xi, Î¸)
this is because P
yj P(yj | xj, Î¸) = 1. Substituting the simpliï¬cations above
into eqn. 3.8 we obtain:
Î¸(t+1) = argmax
Î¸
k
X
j=1
m
X
i=1
P(yi = Î±j | xi, Î¸(t)) log P(xi, yi = Î±j | Î¸)
(3.9)
where yi âˆˆ{Î±1, ..., Î±k}.
3.3 Back to the Coins Example
We will apply the EM scheme to our running example of mixture of Bernoulli
distributions. We wish to compute
Q(Î¸, Î¸(t))
=
X
y
P(y | D, Î¸(t)) log P(D, y | Î¸)
=
n
X
i=1
2
X
j=1
P(yi = j | xi, Î¸(t)) log P(xi, yi = j | Î¸),
3.3 Back to the Coins Example
25
and then maximize Q() with respect to p, q, Î».
Q(Î¸, Î¸â€²)
=
n
X
i=1

P(yi = 1 | xi, Î¸â€²) log P(xi | yi = 1, Î¸)P(yi = 1 | Î¸)

+
n
X
i=1

P(yi = 2 | xi, Î¸â€²) log P(xi | yi = 2, Î¸)P(yi = 2 | Î¸)

=
X
i
h
Âµi log(Î»pni(1 âˆ’p)(3âˆ’ni)) + (1 âˆ’Âµi) log((1 âˆ’Î»)qni(1 âˆ’q)(3âˆ’ni))
i
where Î¸â€² stands for Î¸(t) and Âµi = P(yi = 1 | xi, Î¸â€²). The values of Âµi are
known since Î¸â€² = (Î»o, po, qo) are given from the previous iteration.
The
Bayes formula is used to compute Âµi:
Âµi
=
P(yi = 1 | xi, Î¸â€²) = P(xi | yi = 1, Î¸â€²)P(yi = 1 | Î¸â€²)
P(xi | Î¸â€²)
=
Î»opni
o (1 âˆ’po)(3âˆ’ni)
Î»opni
o (1 âˆ’po)(3âˆ’ni) + (1 âˆ’Î»o)qni
o (1 âˆ’qo)(3âˆ’ni)
We wish to compute: maxp,q,Î» Q(Î¸, Î¸â€²). The partial derivative with respect
to Î» is:
âˆ‚Q
âˆ‚Î» =
X
i
Âµi
1
Î» âˆ’
X
i
(1 âˆ’Âµi)
1
1 âˆ’Î» = 0,
from which we obtain the update formula of Î» given Âµi:
Î» = 1
k
n
X
i=1
Âµi.
The partial derivative with respect to p is:
âˆ‚Q
âˆ‚p =
X
i
Âµini
p
âˆ’
X
i
Âµi(3 âˆ’ni)
1 âˆ’p
= 0,
from which we obtain the update formula:
p =
1
P
i Âµi
X
i
ni
3 Âµi.
Likewise the update rule for q is:
q =
1
P
i(1 âˆ’Âµi)
X
i
ni
3 (1 âˆ’Âµi).
To conclude, we start with some initial â€guessâ€ of the values of p, q, Î», com-
pute the values of Âµi and update iteratively the values of p, q, Î» where at the
end of each iteration the new values of Âµi are computed.
26
EM Algorithm: ML over Mixture of Distributions
3.4 Gaussian Mixture
The Gaussian mixture model assumes that P(x) where x âˆˆRd is a linear
combination of Gaussian distributions
P(x) =
k
X
j=1
P(x | y = j)P(y = j)
where
P(x | y = j) =
1
(2Ï€)d/2Ïƒd
j
exp
âˆ’
âˆ¥xâˆ’cjâˆ¥2
2Ïƒ2
j
,
is Normally distributed with mean cj and covariance matrix Ïƒ2
j I. Let D =
{x1, ..., xm} be the i.i.d sample data and we wish to solve for the mean
and covariances of the individual Gaussians (the â€factorsâ€) and the mixing
coeï¬ƒcients Î»j = P(y = j). In order to make clear where the parameters are
located we will write P(x | Ï†j) instead of P(x | y = j) where Ï†j = (cj, Ïƒ2
j )
are the mean and variance of the jâ€™th factor. We denote by Î¸ the collection
of mixing coeï¬ƒcients Î»j and Ï†j, j = 1, ..., k. Let wj
i be auxiliary variables
per point xi and per factor y = j standing for:
wj
i = P(yi = j | xi, Î¸).
The EM step (eqn. 3.9) is:
Î¸(t+1) = argmax
Î¸={Î»,Ï†}
k
X
j=1
m
X
i=1
wj
i
(t) log (Î»jP(xi | Ï†j))
s.t.
X
j
Î»j = 1.
(3.10)
Note the constraint P
j Î»j = 1. The update formula for wj
i is done through
the use of Bayes formula:
wj
i
(t) = P(yi = j | Î¸(t))P(xi | yi = j, Î¸(t))
P(xi | Î¸(t))
= 1
Zi
Î»(t)
j P(xi | Ï†(t)),
where Zi is a scaling factor so that P
j wj
i = 1.
The update formula for Î»j, cj, Ïƒj follow by taking partial derivatives of
eqn. (3.10) and setting them to zero. Taking partial derivatives with respect
3.5 Application Examples
27
to Î»j, cj and Ïƒj we obtain the update rules:
Î»j
=
1
m
m
X
i=1
wj
i
cj
=
1
P
i wj
i
m
X
i=1
wj
i xi,
Ïƒ2
j
=
1
d P
i wj
i
m
X
i=1
wj
i âˆ¥xi âˆ’cjâˆ¥2.
In other words, the observations xi are weighted by wj
i before a Gaussian is
ï¬tted (k times, one for each factor).
3.5 Application Examples
3.5.1 Gaussian Mixture and Clustering
The Gaussian mixture model is classically used for clustering applications.
In a clustering application one receives a sample of points x1, ..., xm where
each point resides in Rd. The task of the learner (in this case â€unsupervisedâ€
learning) is to group the m points into k sets. Let yi âˆˆ{1, ..., k} where
i = 1, ..., m stands for the required labeling. The clustering solution is an
assignment of values to y1, ..., ym according to some clustering criteria.
In the Gaussian mixture model points are clustered together if they arise
from the same Gaussian distribution. The EM algorithm provides a proba-
bilistic assignment P(yi = j | xi) which we denoted above as wj
i .
3.5.2 Multinomial Mixture and â€bag of wordsâ€ Application
The multinomial mixture (the coins example we toyed with) is typically used
for representing â€countâ€ data, such as when representing text documents as
high-dimensional vectors. A vector representation of a text document asso-
ciates a word from a ï¬xed vocabulary to a coordinate entry of the vector.
The value of the entry represents the number of times that particular word
appeared in the document. If we ignore the order in which the words ap-
peared and count only their frequency, a set of documents d1, ..., dm and a
set of words w1, ...., wn could be jointly represented by a co-occurence nÃ—m
matrix G where Gij contains the number of times word wi appeared in doc-
ument dj. If we scale G such that P
ij Gij = 1 then we have a distribution
P(w, d). This kind of representation of a set of documents is called â€bag of
wordsâ€.
28
EM Algorithm: ML over Mixture of Distributions
For purposes of search and ï¬ltering it is desired to reveal additional infor-
mation about words and documents such as to which â€topicâ€ a document
belongs to or to which topics a word is associated with. This is similar to
a clustering task where documents associated with the same topic are to be
clustered together. This can be achieved by considering the topics as the
value of a latent variable y:
P(w, d) =
X
y
P(w, d | y)P(y) =
X
y
P(w | y)P(d | y)P(y),
where we made the assumption that wâŠ¥d | y (i.e., words and documents are
conditionally independent given the topic).
The conditional independent
assumption gives rise to the multinomial mixture model. To be more speciï¬c,
ley y âˆˆ{1, ..., k} denote the k possible topics and let Î»j = P(y = j) (note
that P
j Î»j = 1), then the latent class model becomes:
P(w, d) =
k
X
j=1
Î»jP(w | y = j)P(d | y = j).
Note that P(w | y = j) is a vector which we denote as uj âˆˆRn and P(d | y =
j) is also a vector we denote by vj âˆˆRm. The term P(w | y = j)P(d | y = j)
stands for the outer-product ujvâŠ¤
j of the two vectors, i.e., is a rank-1 n Ã— m
matrix. The Maximum-Likelihood estimation problem is therefore to ï¬nd
vectors u1, ..., uk and v1, ..., vk and scalars Î»1, ..., Î»k such that the empirical
distribution represented by the unit scaled matrix G is as close as possible
(in relative-entropy measure) to the low-rank matrix P
j Î»jujvâŠ¤
j subject to
the constraints of non-negativity and P
j Î»j = 1, uj and vj are unit-scaled
as well (1âŠ¤uj = 1âŠ¤vj = 1).
Let xi = (w(i), d(i)) stand for the iâ€™th example i = 1, ..., q where an
example is a pair of word and document where w(i) âˆˆ{1, ..., n} is the index
to the word alphabet and d(i) âˆˆ{1, ..., m} is the index to the document.
The EM algorithm involves the following optimization step:
Î¸(t+1)
=
argmax
Î¸
q
X
i=1
k
X
j=1
P(yi = j | xi, Î¸(t)) log P(xi, yi = j | Î¸)
=
argmax
Î¸
q
X
i=1
k
X
j=1
w(t)
ij log

Î»juj,w(i)vj,d(i)

s.t.
1âŠ¤Î» = 1âŠ¤uj = 1âŠ¤vj = 1
An update rule for ujr (the râ€™th entry of uj) is derived below: the derivative
3.5 Application Examples
29
of the Lagrangian is:
âˆ‚
âˆ‚ujr
" q
X
i=1
w(t)
ij log uj,w(i) âˆ’Âµujr
#
=
âˆ‚
âˆ‚ujr
ï£®
ï£°N(r) log ujr
X
w(i)=r
w(t)
ij âˆ’Âµujr
ï£¹
ï£»
=
N(r) P
w(i)=r w(t)
ij
ujr
âˆ’Âµ = 0
where N(r) stands for the frequency of the word wr in all the documents
d1, ..., dm. Note that N(r) is the result of summing-up the râ€™th row of G and
that the vector N(1), ..., N(n) is the marginal P(w) = P
d P(w, d). Given
the constraint 1âŠ¤uj = 1 we obtain the update rule:
ujr â†
N(r) P
w(i)=r w(t)
ij
Pn
s=1 N(s) P
w(i)=s w(t)
ij
.
Update rules for the remaining unknowns are similarly derived. Once EM
has converged, then P
w(i)=r wâˆ—
ij is the probability of the word wr to belong
to the jâ€™th topic and P
d(i)=s wâˆ—
ij is the probability that the sâ€™th document
comes from the jâ€™th topic.
4
Support Vector Machines and Kernel Functions
In this lecture we begin the exploration of the 2-class hyperplane separation
problem. We are given a training set of instances xi âˆˆRn, i = 1, ..., m,
and class labels yi = Â±1 (i.e., the training set is made up of â€œpositiveâ€
and â€œnegativeâ€ examples). We wish to ï¬nd a hyperplane direction w âˆˆRn
and an oï¬€set scalar b such that w Â· xi âˆ’b > 0 for positive examples and
w Â· xi âˆ’b < 0 for negative examples â€” which together means that the
margins yi(w Â· xi âˆ’b) > 0 are positive.
Assuming that such a hyperplane exists, clearly it is not unique.
We
therefore need to introduce another constraint so that we could ï¬nd the
most â€œsensibleâ€ solution among all (inï¬nitley many) possible hyperplanes
which separate the training data. Another issue is that the framework is
very limited in the sense that for most real-world classiï¬cation problems
it is somewhat unlikely that there would exist a linear separating function
to begin with. We therefore need to ï¬nd a way to extend the framework
to include non-linear decision boundaries at a reasonable cost. These two
issues will be the focus of this lecture.
Regarding the ï¬rst issue, since there is more than one separating hyper-
plane (assuming the training data is linearly separable) then the question
we need to ask ourselves is among all those solutions which of them has the
best â€œgeneralizationâ€ properties? In other words, our goal in constructing
a learning machine is not necessarily to do very well (or perfect) on the
training data, because the training data is merely a sample of the instance
space, and not necessarily a â€œrepresentativeâ€ sample â€” it is simply a sample.
Therefore, doing well on the sample (the training data) does not necessarily
guarantee (or even imply) that we will do well on the entire instance space.
The goal of constructing a learning machine is to maximize the performance
on the test data (the instances we havenâ€™t seen), which in turn means that
30
4.1 Large Margin Classiï¬er as a Quadratic Linear Programming
31
we wish to generalize â€œgoodâ€ classiï¬cation performance on the training set
onto the entire instance space.
A related issue to generalization is that the distribution used to generate
the training data is unknown. Unlike the statistical inference material we
had so far, this time we will not attempt to estimate the distribution. The
reason one can derive optimal learning algorithms yet bypass the need for
estimating distributions would be explained later in the course when PAC-
learning will be introduced. For now we will focus only on the algorithmic
aspect of the learning problem.
The idea is to consider a subset CÎ³ of all hyperplanes which have a ï¬xed
margin Î³ where the margin is deï¬ned as the distance of the closest training
point to the hyperplane:
Î³ = min
i
yi(wâŠ¤xi âˆ’b)
âˆ¥wâˆ¥

.
The Support Vector Machine (SVM), ï¬rst introduced by Vapnik and his
colleagues in 1992, seeks a separating hyperplane which simultaneously min-
imizes the empirical error and maximizes the margin. The idea of maximiz-
ing the margin is intuitively appealing because a decision boundary which
lies close to some of the training instances is less likely to generalize well
because the learning machine will be susceptible to small perturbations of
those instance vectors. A formal motivation for this approach is deferred to
the PAC-learning material we will introduce later in the course.
4.1 Large Margin Classiï¬er as a Quadratic Linear Programming
We would ï¬rst like to set up the linear separating hyperplane as an optimiza-
tion problem which is both consistent with the training data and maximizes
the margin induce by the separating hyperplane over all possible consistent
hyperplanes.
Formally speaking, the distance between a point x and the hyperplane is
deï¬ned by
| w Â· x âˆ’b |
âˆšw Â· w
.
Since we are allowed to scale the parameters w, b at will (note that if w Â·
x âˆ’b > 0 so is (Î»w) Â· x âˆ’(Î»b) > 0 for all Î» > 0) we can set the distance
between the boundary points to the hyperplane to be 1/âˆšw Â· w by scaling
w, b such the point(s) with smallest margin (closest to the hyperplane) will
be normalized: | wÂ·xâˆ’b |= 1, therefore the margin is simply 2/âˆšw Â· w (see
Fig. 5.1). Note that argmaxw2/âˆšw Â· w is equivalent to argmaxw2/(w Â· w)
32
Support Vector Machines and Kernel Functions
which in turn is equivalent to argminw
1
2w Â· w.
Since all positive points
and negative points should be farther away from the boundary points we
also have the separability constraints w Â· x âˆ’b â‰¥1 when x is a positive
instance and wÂ·xâˆ’b â‰¤âˆ’1 when x is a negative instance. Both separability
constraints can be combined: y(w Â· x âˆ’b) â‰¥1. Taken together, we have
deï¬ned the following optimization problem:
min
w,b
1
2w Â· w
(4.1)
subject to
yi(w Â· xi âˆ’b) âˆ’1 â‰¥0
i = 1, ..., m
(4.2)
This type of optimization problem has a quadratic criteria function and
linear inequalities and is known in the literature as a Quadratic Linear Pro-
gramming (QP) type of problem.
This particular QP, however, requires that the training data are linearly
separable â€” a condition which may be unrealistic. We can relax this condi-
tion by introducing the concept of a â€œsoft marginâ€ in which the separability
holds approximately with some error:
min
w,b,Ïµi
1
2w Â· w + Î½
l
X
i=1
Ïµi
(4.3)
subject to
yi(w Â· xi âˆ’b) â‰¥1 âˆ’Ïµi
i = 1, ..., m
Ïµi â‰¥0
Where Î½ is some pre-deï¬ned weighting factor. The (non-negative) vari-
ables Ïµi allow data points to be miss-classiï¬ed thereby creating an approx-
imate separation. Speciï¬cally, if xi is a positive instance (yi = 1) then the
â€œsoftâ€ constraint becomes:
w Â· xi âˆ’b â‰¥1 âˆ’Ïµi,
where if Ïµi = 0 we are back to the original constraint where xi is either a
boundary point or laying further away in the half space assigned to positive
instances. When Ïµi > 0 the point xi can reside inside the margin or even in
the half space assigned to negative instances. Likewise, if xi is a negative
instance (yi = âˆ’1) then the soft constraint becomes:
w Â· xi âˆ’b â‰¤âˆ’1 + Ïµi.
4.1 Large Margin Classiï¬er as a Quadratic Linear Programming
33
)
,
(
b
w
maximize the margin
|
|
2
w
0
>
i!
0
>
i
Âµ
0
=
j
Âµ
Fig. 4.1. Separating hyperplane w, b with maximal margin. The boundary points
are associated with non-vanishing Lagrange multipliers Âµi > 0 and margin errors
are associated with Ïµi > 0 where the criteria function encourages a small number
of margin errors.
The criterion function penalizes (the L1-norm) for non-vanishing Ïµi thus the
overall system will seek a solution with few as possible â€œmargin errorsâ€ (see
Fig. 5.1). Typically, when possible, an L1 norm is preferable as the L2 norm
overly weighs high magnitude outliers which in some cases can dominate
the energy function. Another note to make here is that strictly speaking
the â€right thingâ€ to do is to penalize the margin errors based on the L0
norm âˆ¥Ïµâˆ¥0
0 = |{i : Ïµi > 0}|, i.e., the number of non-zero entries, and drop
the balancing parameter Î½. This is because it does not matter how far away
a point is from the hyperplane â€” all what matters is whether a point is
classiï¬ed correctly or not (see the deï¬nition of empirical error in Lecture 4).
The problem with that is that the optimization problem would no longer be
convex and non-convex problems are notoriously diï¬ƒcult to solve. Moreover,
the class of convex optimization problems (as the one described in Eqn. 4.3)
can be solved in polynomial time complexity.
So far we have described the problem formulation which when solved
would provide a solution with â€œsensibleâ€ generalization properties. Although
we can proceed using an oï¬€-the-shelf QLP solver, we will ï¬rst pursue the
â€dualâ€ problem. The dual form will highlight some key properties of the
approach and will enable us to extend the framework to handle non-linear
34
Support Vector Machines and Kernel Functions
decision surfaces at a very little cost. In the appendix we take a brief tour on
the basic principles associated with constrained optimization, the Karush-
Kuhn-Tucker (KKT) theorem and the dual form. Those are recommended
to read before moving to the next section.
4.2 The Support Vector Machine
We return now to the primal problem (eqn. 6.3) representing the maximal
margin separating hyperplane with margin errors:
min
w,b,Ïµi
1
2w Â· w + Î½
l
X
i=1
Ïµi
subject to
yi(w Â· xi âˆ’b) â‰¥1 âˆ’Ïµi
i = 1, ..., m
Ïµi â‰¥0
We will now derive the Lagrangian Dual of this problem. By doing so a
new key property will emerge facilitated by the fact that the criteria func-
tion Î¸(Âµ) (note there are no equality constraints thus there is no need for Î»)
involves only inner-products of the training instance vectors xi. This prop-
erty will form the key of mapping the original input space of dimension n to
a higher dimensional space thereby allowing for non-linear decision surfaces
for separating the training data.
Note that with this particular problem the strong duality conditions are
satisï¬ed because the criteria function and the inequality constraints form a
convex set. The Lagrangian takes the following form:
L(w, b, Ïµi, Âµ) = 1
2w Â· w + Î½
m
X
i=1
Ïµi âˆ’
m
X
i=1
Âµi [yi(w Â· xi âˆ’b) âˆ’1 + Ïµi] âˆ’
m
X
i=1
Î´iÏµi
Recall that
Î¸(Âµ) = min
w,b,Ïµ L(w, b, Ïµ, Âµ, Î´).
Since the minimum is obtained at the vanishing partial derivatives of the
Lagrangian with respect to w, b, the next step would be to evaluate those
4.2 The Support Vector Machine
35
constraints and substitute them back into L() to obtain Î¸(Âµ):
âˆ‚L
âˆ‚w
=
w âˆ’
X
i
Âµiyixi = 0
(4.4)
âˆ‚L
âˆ‚b
=
X
i
Âµiyi = 0
(4.5)
âˆ‚L
âˆ‚Ïµi
=
Î½ âˆ’Âµi âˆ’Î´i = 0
(4.6)
From the ï¬rst constraint (4.4) we obtain w = P
i Âµiyixi, that is, w is de-
scribed by a linear combination of a subset of the training instances. The
reason that not all instances participate in the linear superposition is due
to the KKT conditions: Âµi = 0 when yi(w Â· xi âˆ’b) > 1, i.e., the instance xi
is classiï¬ed correctly and is not a boundary point, and conversely, Âµi > 0
when yi(w Â· xi âˆ’b) = 1 âˆ’Ïµi, i.e., when xi is a boundary point or when
xi is a margin error (Ïµi > 0) â€” note that for a margin error instance the
value of Ïµi would be the smallest possible required to reach an equality in
the constraint because the criteria function penalizes large values of Ïµi. The
boundary points (and the margin errors) are called support vectors thus w is
deï¬ned by the support vectors only. The third constraint (4.6) is equivalent
to the constraint:
0 â‰¤Âµi â‰¤Î½
i = 1, ..., l,
since Î´i â‰¥0. Also note that if Ïµi > 0, i.e., point xi is a margin-error point,
then by KKT conditions we must have Î´i = 0. As a result Âµi = Î½. Therefore
based on the values of Âµi alone we can make the following classiï¬cations:
â€¢ 0 < Âµi < Î½: point xi is on the margin and is not a margin-error.
â€¢ Âµi = Î½: points xi is a margin-error point.
â€¢ Âµi = 0: point xi is not on the margin.
Substituting these results/constraints back into the Lagrangian L() we
obtain the dual problem:
max
Âµ1,...,Âµm
Î¸(Âµ) =
m
X
i=1
Âµi âˆ’1
2
X
i,j
ÂµiÂµjyiyjxi Â· xj
(4.7)
subject to
0 â‰¤Âµi â‰¤Î½
i = 1, ..., m
m
X
i=1
yiÂµi = 0
The criterion function Î¸(Âµ) can be written in a more compact manner as
36
Support Vector Machines and Kernel Functions
follows: Let M be a l Ã— l matrix whose entries are Mij = yiyjxi Â· xj then
Î¸(Âµ) = ÂµâŠ¤1 âˆ’1
2ÂµâŠ¤MÂµ where 1 is the vector of (1, ..., 1) and Âµ is the vector
(Âµ1, ..., Âµm) and ÂµâŠ¤is the transpose (row vector). Note that M is positive
deï¬nite, i.e., xâŠ¤Mx > 0 for all vectors x Ì¸= 0 â€” a property which will be
important later.
The key feature of the dual problem is not so much that it is simpler
than the primal (in fact it isnâ€™t since the primal has no equality constraints)
or that it has a more â€œelegantâ€ feel, the key feature is that the problem
is completely described by the inner products of the training instances xi,
i = 1, ..., m. This fact will be shown to be a crucial ingredient in the so called
â€œkernel trickâ€ for the computation of inner-products in high dimensional
spaces using simple functions deï¬ned on pairs of training instances.
4.3 The Kernel Trick
We ended with the dual formulation of the SVM problem and noticed that
the input data vectors xi are represented by the Gram matrix M. In other
words, only inner-products of the input vectors play a role in the dual for-
mulation â€” there is no explicit use of xi or any other function of xi besides
inner-products. This observation suggests the use of what is known as the
â€kernel trickâ€ to replace the inner-products by non-linear functions.
The common principle of kernel methods is to construct nonlinear vari-
ants of linear algorithms by substituting inner-products by nonlinear kernel
functions. Under certain conditions this process can be interpreted as map-
ping of the original measurement vectors (so called â€input spaceâ€) onto
some higher dimensional space (possibly inï¬nitely high) commonly referred
to as the â€feature spaceâ€. Mathematically, the kernel approach is deï¬ned
as follows: let x1, ..., xl be vectors in the input space, say Rn, and con-
sider a mapping Ï†(x) : Rn â†’F where F is an inner-product space. The
kernel-trick is to calculate the inner-product in F using a kernel function
k : Rn Ã—Rn â†’R, k(xi, xj) = Ï†(xi)âŠ¤Ï†(xj), while avoiding explicit mappings
(evaluation of) Ï†().
Common choices of kernel selection include the dâ€™th order polynomial
kernels k(xi, xj) = (xâŠ¤
i xj + Î¸)d and the Gaussian RBF kernels k(xi, xj) =
exp(âˆ’1
2Ïƒ2 âˆ¥xi âˆ’xjâˆ¥2). If an algorithm can be restated such that the input
vectors appear in terms of inner-products only, one can substitute the inner-
products by such a kernel function. The resulting kernel algorithm can be
interpreted as running the original algorithm on the space F of mapped
objects Ï†(x).
We know that M of the dual form is positive semi-deï¬nite because M
4.3 The Kernel Trick
37
can be written is M = QâŠ¤Q where Q = [y1x1, ..., ylxl]. Therefore xâŠ¤Mx =
âˆ¥Qxâˆ¥2 â‰¥0 for all choices of x (which means that the eigenvalues of M are
non-negative). If the entries of M are to be replaced with yiyjk(xi, xj) then
the condition we must enforce on the function k() is that it is a positive
deï¬nite kernel function. A positive deï¬nite function is deï¬ned such that
for any set of vectors x1, ..., xq and for any values of q the matrix K whose
entries are Kij = k(xi, xj) is positive semi-deï¬nite. Formally, the conditions
for admissible kernels k() are known as Mercerâ€™s conditions summarized
below:
Theorem 4 (Mercerâ€™s Conditions) Let k(x, y) be symmetric and contin-
uous. The following conditions are equivalent:
(i) k(x, y) = Pâˆ
i=1 Î±iÏ†i(x)Ï†i(y) = Ï†(x)âŠ¤Ï†(y) for any uniformly converg-
ing series Î±i > 0.
(ii) for all Ïˆ() satisfying
R
x Ïˆ2(x)dx < âˆ, then
Z
x
Z
y
k(x, y)Ïˆ(x)Ïˆ(y)dxdy â‰¥0
(iii) for all {xi}q
i=1 and for all q, the matrix Kij = k(xi, xj) is positive
semi-deï¬nite.
Perhaps the non-obvious condition is No. 1 which allows for the feature
map Ï†() to have inï¬nitely many coordinates (a vector in Hilbert space). For
example, as we shall see below, the kernel exp(âˆ’1
2Ïƒ2 âˆ¥xi âˆ’xjâˆ¥2) is an inner-
product of two vectors with inï¬nitely many coordinates. We will consider
next a number of popular kernels.
4.3.1 The Homogeneous Polynomial Kernel
Let x, y âˆˆRk and deï¬ne k(x, y) = (xâŠ¤y)d where d > 0 is a natural number.
Then, the corresponding feature map Ï†(x) has
 k+dâˆ’1
d

= O(kd) coordinates
which take the value:
Ï†(x) =
 s
d
n1, ..., nk

xn1
1 Â· Â· Â· xnk
k
!
niâ‰¥0,P
i ni=d
where
 d
n1,...,nk

= d!/(n1! Â· Â· Â· nk!) is the multinomial coeï¬ƒcient (number of
ways to distribute d balls into k bins where the jâ€™th bin hold exactly nj â‰¥0
balls):
(x1 + ... + xk)d =
X
niâ‰¥0,P
i ni=d

d
n1, ..., nk

xn1
1 Â· Â· Â· xnk
k .
38
Support Vector Machines and Kernel Functions
The dimension of the vector space Ï†(x) where x âˆˆRk can be measured
using the following combinatorial problem: how many arrangements of kâˆ’1
partitions to be placed among d items? the answer is
 k+dâˆ’1
kâˆ’1

=
 k+dâˆ’1
d

=
O(kd). For example, k = d = 2 gives us :
(xâŠ¤y)2 = x2
1y2
1 + 2x1x2y1y2 + x2
2y2
2 = Ï†(x)âŠ¤Ï†(y),
where Ï†(x) = (x2
1, x2
2,
âˆš
2x1x2).
4.3.2 The non-homogeneous Polynomial Kernel
The feature map Ï†(x) contains all monomials whose power is lesser or equal
to d, i.e., P
i ni â‰¤d. This can be acheived by increasing the dimension
to k + 1 where nk+1 is used to ï¬ll the gap between Pk
i=1 ni < d and d.
Therefore the dimension of Ï†(x) where x âˆˆRk would be
 k+d
d

. We have:
(xâŠ¤y + Î¸)d
=
(x1y1 + ... + xkyk +
âˆš
Î¸
âˆš
Î¸)d
=
X
niâ‰¥0,Pk+1
i=1 ni=d

d
n1, ..., nk+1

xn1
1 yn1
1 Â· Â· Â· xnk
1 ynk
1
Â· Î¸nk+1/2Î¸nk+1/2
Therefore, the entries of the vector Ï†(x) take the values:
Ï†(x) =
 s
d
n1, ..., nk+1

xn1
1 Â· Â· Â· xnk
k Â· Î¸nk+1/2
!
niâ‰¥0,Pk+1
i=1 ni=d
For example, k = d = 2 gives us :
(xâŠ¤y + Î¸)2 = x2
1y2
1 + 2x1x2y1y2 + x2
2y2
2 + 2Î¸x1y1 + 2Î¸x2y2 + Î¸ = Ï†(x)âŠ¤Ï†(y),
where Ï†(x) = (x2
1, x2
2,
âˆš
2x1x2,
âˆš
2Î¸x1,
âˆš
2Î¸x2,
âˆš
Î¸). In this example, Ï†() is a
mapping from R2 to R6 and hyperplanes Ï†(w)âŠ¤Ï†(x)âˆ’b = 0 in R6 correspond
to conics in R2:
(w2
1)x2
1 + (w2
2)x2 + (2w1w2)x1x2 + (2Î¸w1)x1 + (2Î¸w2)x2 + (Î¸ âˆ’b) = 0
Assume we would like to ï¬nd a separating conic (Parabola, Hyperbola,
Ellipse) function rather than a line in R2. The discussion so far suggests we
construct the Gram matrix M in the dual form with the d = 2 polynomial
kernel k(x, y) = (xâŠ¤y+Î¸)2 for some parameter Î¸ of our choosing. The extra
eï¬€ort we will need to invest is negligible â€” simply replace every occurrence
xâŠ¤
i xj with (xâŠ¤
i xj + Î¸)2.
4.3 The Kernel Trick
39
4.3.3 The RBF Kernel
The function k(x, y) = eâˆ’âˆ¥xâˆ’yâˆ¥2/2Ïƒ2 known as a Radial Basis Function
(RBF) is a kernel function but with an inï¬nite expansion. Without loss of
generality let Ïƒ = 1, then we have:
eâˆ’âˆ¥xâˆ’yâˆ¥2/2
=
eâˆ’âˆ¥xâˆ¥2/2eâˆ’âˆ¥yâˆ¥2/2exâŠ¤y
=
âˆ
X
j=0
(xâŠ¤y)j
j!
eâˆ’âˆ¥xâˆ¥2/2eâˆ’âˆ¥yâˆ¥2/2
=
âˆ
X
j=0
ï£«
ï£­eâˆ’âˆ¥xâˆ¥2
2j
âˆšj!1/j
eâˆ’âˆ¥yâˆ¥2
2j
âˆšj!1/j xâŠ¤y
ï£¶
ï£¸
j
=
âˆ
X
j=0
X
P
i ni=j
eâˆ’âˆ¥xâˆ¥2
2j
âˆšj!1/j

j
n1, ..., nk
1/2
xn1
1 Â· Â· Â· xnk
k
eâˆ’âˆ¥yâˆ¥2
2j
âˆšj!1/j

j
n1, ..., nk
1/2
yn1
1 Â· Â· Â· ynk
k
From which we can see that the entries of the feature map Ï†(x) are:
Ï†(x) =
ï£«
ï£­eâˆ’âˆ¥xâˆ¥2
2j
âˆšj!1/j

j
n1, ..., nk
1/2
xn1
1 Â· Â· Â· xnk
k
ï£¶
ï£¸
j=0,..,âˆ,Pk
i=1 ni=j
4.3.4 Classifying New Instances
By adopting some kernel k() we are in fact mapping x â†’Ï†(x), thus we
then proceed to solve for Ï†(w) and b using some QLP solver. The QLP
solution of the dual form will yield the solution for the Lagrange multipliers
Âµ1, ..., Âµm. We saw from eqn. (4.4) that we can express Ï†(w) as a function
of the (mapped) examples:
Ï†(w) =
X
i
ÂµiyiÏ†(xi).
Rather than explicitly representing Ï†(w) â€” a task which may be prohibitly
expensive since in general the dimension of the feature space of a polynomial
mapping is
 k+d
d

â€” we store all the support vectors (those input vectors
with corresponding Âµi > 0) and use them for the evaluation of test examples:
f(x)
=
sign(Ï†(w)âŠ¤Ï†(x) âˆ’b) = sign(
X
i
ÂµiyiÏ†(xi)âŠ¤Ï†(x) âˆ’b)
=
sign(
X
i
Âµiyik(xi, x) âˆ’b).
40
Support Vector Machines and Kernel Functions
We see that the kernel trick enabled us to look for a non-linear separating
surface by making an implicit mapping of the input space onto a higher di-
mensional feature space using the same dual form of the SVM formulation â€”
the only change required was in the way the Gram matrix was constructed.
The price paid for this convenience is to carry all the support vectors at the
time of classiï¬cation f(x).
A couple of notes may be worthwhile at this point. The constant b can
be recovered from any of the support vectors. Say, x+ is a positive support
vector (but not a margin error, i.e., Âµi < Î½). Then Ï†(w)âŠ¤Ï†(x+) âˆ’b = 1
from which b can be recovered.
The second note is that the number of
support vectors is typically around 10% of the number of training examples
(empirically). Thus the computational load during evaluation of f(x) may
be relatively high. Approximations have been proposed in the literature by
looking for a reduced number of support vectors (not necessarily aligned
with the training set) â€” but this is beyond the scope of this course.
The kernel trick gained its popularity with the introduction of the SVM
but since then has taken a life of its own and has been applied to principal
component analysis (PCA), ridge regression, canonical correlation analysis
(CCA), QR factorization and the list goes on. We will meet again with the
kernel trick later on.
5
Spectral Analysis I: PCA, LDA, CCA
In this lecture (and the following one) we will focus on spectral methods for
learning. Today we will focus on dimensionality reduction using Principle
Component Analysis (PCA), multi-class learning using Linear Discriminant
Analysis (LDA) and Canonical Correlation Analysis (CCA). In the next
lecture we will focus on spectral clustering methods.
Dimensionality reduction appears when the dimension of the input vector
is very large (imagine pixels in an image, for example) while the coordi-
nate measurements are highly inter-dependent (again, imagine the redun-
dancy present among neighboring pixels in an image). High dimensional
data impose computational eï¬ƒciency challenges and often translate to poor
generalization abilities of the learning engine (see lectures on PAC). A di-
mensionality reduction can also be viewed as a feature extraction process
where one takes as input a large feature set (the original measurements)
and creates from them a much smaller number of new features which are
then fed into the learning engine.
In this lecture we will focus on feature extraction from a very speciï¬c (and
constrained) stanpoint. We would be looking for a mixing (linear combina-
tion) of the input coordinates such that we obtain a linear projection from
Rn to Rq for some q < n. In doing so we wish to reduce the redundancy
while preserving as much as possible the variance of the data. From a sta-
tistical standpoint this is achieved by transforming to a new set of variables,
called principal components, which are uncorrelated so that the ï¬rst few
retain most of the variation present in all of the original coordinates. For
example, in an image processing application the input images are highly re-
dundant where neighboring pixel values are highly correlated. The purpose
of feature extraction would be to transform the input image into a vector of
output components with the least redundancy possible. Form a geometric
standpoint, this is achieved by ï¬nding the â€closestâ€ (in least squares sense)
41
42
Spectral Analysis I: PCA, LDA, CCA
linear q-dimensional susbspace to the m sample points S. The new sub-
space is a lower dimensional â€best approximationâ€ to the sample S. These
two, equivalent, perspectives on data compression (dimensionality reduc-
tion) form the central idea of principal component analysis (PCA) which
probably the oldest (going back to Pearson 1901) and best known of the
techniques of multivariate analysis in statistics. The computation of PCA
is very simple and the deï¬nition is straightforward, but has a wide variety
of diï¬€erent applications, a number of diï¬€erent derivations, quite a number
of diï¬€erent terminologies (especially outside the statistical literature) and is
the basis for quite a number of variations on the basic technique.
We then extend the variance preserving approach for data representation
for labeled data sets. We will describe the linear classiï¬er approach (sepa-
rating hyperplane) form the point of view of looking for a hyperplane such
that when the data is projected onto it the separation is maximized (the dis-
tance between the class means is maximal) and the data within each class is
compact (the variance/spread is minimized). The solution is also produced,
just like PCA, by a spectral analysis of the data. This approach goes under
the name of Fisherâ€™s Linear Discriminant Analysis (LDA).
What is common between PCA and LDA is (i) the use of spectral ma-
trix analysis â€” i.e., what can you do with eigenvalues and eigenvectors of
matrices representing subspaces of the data? (ii) these techniques produce
optimal results for normally distributed data and are very easy to imple-
ment. There is a large variety of uses of spectral analysis in statistical and
learning literature including spectral clustering, Multi Dimensional Scaling
(MDS) and data modeling in general. Another point to note is that this is
the ï¬rst time in the course where the type of data distribution plays a role
in the analysis â€” the two techniques are deï¬ned for any distribution but
are optimal only under the Gaussian distribution.
We will also describe a non-linear extension of PCA known as Kernel-
PCA, but the focus would be mostly on PCA itself and its analysis from
a couple of vantage points: (i) PCA as an optimal reconstruction after a
dimension reduction, i.e., data compression, and (ii) PCA for redundancy
reduction (decorrelation) of the output components.
5.1 PCA: Statistical Perspective
Let x1, ..., xm âˆˆRn be our sample data S of vectors in Rn, arranged as
columns of a matrix A. It will be convenient to assume that the data is
centered, i.e., P xi = 0. If the data is not centered we can always center it
by computing the mean vector Âµ = (1/m) P
i xi and replace the original data
5.1 PCA: Statistical Perspective
43
sample with the new sample xi âˆ’Âµ. In a statistical sense, the coordinates
of the vector x âˆˆRn are considered as random variables, thus a row in the
matrix A is the sample of values of a particular random variable, drawn from
some unknown probability distribution, associated with the row position.
We wish to ï¬nd vectors u1, ..., uq (arranged as columns of a matrix U),
where q â‰¤min(n, m), such that the new feature measurements y = UâŠ¤x
(who are the result of linear combinations uâŠ¤
1 x, ..., uâŠ¤
q x of the original feature
measurements x) have certain desirable properties.
The idea property to seek from the new coordinates y is statistical inde-
pendence, i.e., P(y1, .., yq) = P(y1)Â·Â·Â·P(yq) which would mean that we have
removed the redundancy of the original data x in the best possible manner.
This goal, however, is too much to ask from a linear transformation and
instead we would ask for a weaker property to hold: that the pairwise co-
variance cov(yi, yj) = 0 vanishes, i.e., that the covariance matrix on the new
coordinates is diagonal. A diagonal covariance insures some redundancy re-
moval, but not as good as statistical independence. However, when the data
is Normally distributed P(x) âˆ¼N(Âµ, Î£) with mean Âµ and covariance Î£, then
the transformation which diagonalizes the covariance matrix also guarantees
statistical independence. Among all transformations that de-correlate the
data we will seek the one that maximizes the spread (variance) of the sample
data after being projected onto the new axes vectors.
5.1.1 Maximizing the Variance of Output Coordinates
The property we would like to maximize is that the projection of the sample
data on the new axes is as spread as possible. To start this analysis, assume
q = 1, i.e., the n components of the input vector x are reduced to a single
output component y = uâŠ¤x. We are looking for a single vector u âˆˆRn
whose direction maximizes the variance of the output component y.
Formally, we are looking for a unit vector u which maximizes P
i(uâŠ¤xi)2
(see Appendix A for basic statistical deï¬nitions and note that E[y] = 0
because P
i uâŠ¤xi = uâŠ¤
i (P
i xi) = 0). In other words, the projected points
onto the axis represented by the vector u are as spread as possible (in a
least squares sense). In vector notation, the optimization problem takes the
following form:
max
u
1
2âˆ¥uâŠ¤Aâˆ¥2
subject to
1
2uâŠ¤u = 1
The Lagrangian of the problem is:
L(u, Î») = 1
2uâŠ¤AAâŠ¤u âˆ’Î»(1
2uâŠ¤u âˆ’1)
44
Spectral Analysis I: PCA, LDA, CCA
By taking the partial derivative âˆ‚L/âˆ‚u = 0 we obtain the following necessary
condition (see Appendix B):
AAâŠ¤u = Î»u,
which tells us that u is an eigenvector of the n Ã— n (symmetric and positive
deï¬nite) matrix AAâŠ¤. There are n eigenvectors associated with AAâŠ¤and
we can easily convince ourselves that we are looking for the one associated
with the maximal eigenvalue: substitute Î»u instead of AAâŠ¤u in the criterion
function uâŠ¤AAâŠ¤u to obtain Î»(uâŠ¤u) = Î» and since the eigenvalues must be
positive (since AAâŠ¤is positive deï¬nite), then the optimum is obtained for
the maximal eigenvalue. The leading eigenvector u of AAâŠ¤is called the ï¬rst
principal axis of the data sample represented by the columns of the matrix
A, and y = uâŠ¤x is called the ï¬rst principal component of the data sample.
For convenience, we denote u1 = u and Î»1 = Î» as the leading eigenvector
and eigenvalue of AAâŠ¤. Next, we look for y2 = uâŠ¤
2 x which is uncorrelated
with y1 = uâŠ¤
1 x and which has maximum variance (and so on for u3, ..., uq).
Two random variables are uncorrelated if their covariance vanishes.
By
deï¬nition of covariance (see Appendix A) we obtain:
Cov(y1y2)
=
X
i
(uâŠ¤
1 xi)(uâŠ¤
2 xi) = uâŠ¤
1 (
X
i
xixâŠ¤
i )u2
=
uâŠ¤
1 AAâŠ¤u2 = uâŠ¤
2 AAâŠ¤u1 = Î»1uâŠ¤
1 u2 = 0
We can therefore use the condition uâŠ¤
1 u2 = 0 to specify zero correlation
between y1, y2. The functional to be optimized becomes:
max
u2
1
2âˆ¥uâŠ¤
2 Aâˆ¥2
subject to
1
2uâŠ¤
2 u2 = 1,
uâŠ¤
1 u2 = 0,
with the Lagrangian being:
L(u2, Î», Î´) = 1
2uâŠ¤
2 AAâŠ¤u2 âˆ’Î»(1
2uâŠ¤
2 u2 âˆ’1) âˆ’Î´uâŠ¤
1 u2.
By taking the partial derivative with respect to u2 we obtain the necessary
condition:
AAâŠ¤u2 âˆ’Î»u2 âˆ’Î´u1 = 0.
Multiply the equation by u1 from the left:
uâŠ¤
1 AAâŠ¤u2 âˆ’Î»uâŠ¤
1 u2 âˆ’Î´uâŠ¤
1 u1 = 0,
and noting from above that uâŠ¤
1 AAâŠ¤u2 = uâŠ¤
1 u2 = 0 we obtain Î´ = 0. As a
result we obtain:
AAâŠ¤u2 = Î»u2,
5.1 PCA: Statistical Perspective
45
so once more we have that Î», u2 form an eigenvalue/eigenvector pair of AAâŠ¤.
As before, Î» should be as large as possible from the remaining spectral de-
composition. By induction, it can be shown that the remaining principal
vectors u3, ..., uq are the decreasing order eigenvactors of AAâŠ¤and the vari-
ance of the iâ€™th principal component yi = uâŠ¤
i x is Î»i.
Taken together, the PCA is the solution of the following optimization
problem:
max
u1,...,uq
1
2
X
i
âˆ¥uâŠ¤
i Aâˆ¥2
subject to
uâŠ¤
i ui = 1,
uâŠ¤
i uj = 0,
i Ì¸= j = 1, ..., q.
It will be useful for later to write the optimization function in a more concise
manner as follows. Let U be the n Ã— q matrix whose columns are ui and
D = diag(Î»1, ..., Î»q) is an qÃ—q diagonal matrix and Î»1 â‰¥Î»2 â‰¥... â‰¥Î»q. Then
from above we have that UâŠ¤U = I and AAâŠ¤U = UD. Using the fact that
trace(xyâŠ¤) = xâŠ¤y, trace(AB) = trace(BA) and trace(A+B) = trace(A)+
trace(B) we can convert P
i âˆ¥uâŠ¤
i Aâˆ¥2 to trace(UâŠ¤AAâŠ¤U) as follows:
X
i
uâŠ¤
i AAâŠ¤ui
=
X
i
trace(AâŠ¤uiuâŠ¤
i A) = trace(AâŠ¤(
X
i
uiuâŠ¤
i )A)
=
trace(AâŠ¤UUâŠ¤A) = trace(UâŠ¤AAâŠ¤U)
Thus, PCA becomes the solution of the following optimization function:
max
UâˆˆRnÃ—q trace(UâŠ¤AAâŠ¤U)
subject to
UâŠ¤U = I.
(5.1)
The solution, as saw above, is that U = [u1, ..., uq] consists of the decreasing
order eigenvectors of AAâŠ¤. At the optimum, trace(UâŠ¤AAâŠ¤U) is equal to
trace(D) which is equal to the sum of eigenvalues Î»1 + ... + Î»q.
It is worthwhile noting that when q = n, UUâŠ¤= UâŠ¤U = I, and the PCA
transform is a change of basis in Rn known as Karhunen-Loeve transform.
To conclude, the PCA transform looks for q orthogonal direction vectors
(called the principal axes) such that the projection of input sample vectors
onto the principal directions has the maximal spread, or equivalently that
the variance of the output coordinates y = UâŠ¤x is maximal. The principal
directions are the leading (with respect to descending eigenvalues) q eigen-
vectors of the matrix AAâŠ¤. When q = n, the principal directions form a
basis of Rn with the property of de-correlating the data and maximizing the
variance of the coordinates of the sample input vectors.
46
Spectral Analysis I: PCA, LDA, CCA
5.1.2 Decorrelation: Diagonalization of the Covariance Matrix
In the previous section we saw that PCA generates a new coordinate system
y = UâŠ¤x where the coordinates y1, ..., yq of x in the new system are uncorre-
lated. This means that the covariance matrix over the principle components
should be diagonal. In this section we will explore this perspective in more
detail.
The covariance matrix Î£x of the sample data x1, ..., xm with zero mean
is
(1/m)
X
i
xixâŠ¤
i = (1/m)AAâŠ¤,
therefore the matrix AAâŠ¤we derived above is a scaled version of the co-
variance of the sample data (see Appendix A). The scale factor 1/m was
unimportant in the process above because the eigenvectors are of unit norm,
thus any scale of AAâŠ¤would produce the same set of eigenvectors.
The oï¬€-diagonal entries of the covariance matrix Î£x represent the corre-
lation (a measure of statistical dependence) between the iâ€™th and jâ€™th com-
ponent vectors, i.e., the entries of the input vectors x. The existence of
correlations among the components (features) of the input signal is a sign
of redundancy, therefore from the point of view of transforming the input
representation into one which is less redundant, we would like to ï¬nd a
transformation y = UâŠ¤x with an output representation y which is associ-
ated with a diagonal covariance matrix Î£y, i.e., the components of y are
uncorrelated.
Formally, Î£y = (1/m) P
i yiyâŠ¤
i = (1/m)UâŠ¤AAâŠ¤U, therefore we wish to
ï¬nd an n Ã— q matrix for which UâŠ¤AAâŠ¤U is diagonal. If in addition, we
would require that the variance of the output coordinates is maximized,
i.e., trace(UâŠ¤AAâŠ¤U) is maximal (but then we need to constrain the length
of the column vectors of U, i.e., set âˆ¥uiâˆ¥= 1) then we would get a unique
solution for U where the columns are orthonormal and are deï¬ned as the ï¬rst
q eigenvectors of the covariance matrix Î£x. This is exactly the optimization
problem deï¬ned by eqn. (5.1).
We see therefore that PCA â€œdecorrelatesâ€ the input data. Decorrelation
and statistical independence are not the same thing. If the coordinates are
statistically independent then the covariance matrix is diagonalâ€ , but it does
not follow that uncorrelated variables must be statistically independent â€”
covariance is just one measure of dependence. In fact, the covariance is a
measure of pairwise dependency only. However, it is a fact that uncorrelated
â€  Ïƒxy = P
x
P
y(x âˆ’Âµx)(y âˆ’Âµy)p(x, y) = P
x
P
y(x âˆ’Âµx)(y âˆ’Âµy)p(x)(p(y) = (P
x(x âˆ’
Âµx)p(x))(P
y(y âˆ’Âµy)p(y)) = 0
5.2 PCA: Optimal Reconstruction
47
variables are statistically independent if they have a multivariate normal
distribution (a Gaussian). In other words, if the sample data x are drawn
from a probability distribution p(x) which has Gaussian form, the PCA
transforms the sample data into a statistically independent set of variables
y = UâŠ¤x. The details are explained below.
Recall that a multivariate normal distribution of the random variables
x = (x1, ..., xn)âŠ¤is deï¬ned as p(x) â‰ˆN(Âµ, Î£):
p(x) =
1
(2Ï€)n/2|Î£|1/2 eâˆ’1
2 (xâˆ’Âµ)âŠ¤Î£âˆ’1(xâˆ’Âµ).
Also recall that a linear combination of the variables produces also a normal
distribution N(UâŠ¤Âµ, UâŠ¤Î£U):
Î£y =
X
y
(yâˆ’Âµy)(yâˆ’Âµy)âŠ¤=
X
x
(UâŠ¤xâˆ’UâŠ¤Âµx)(UâŠ¤xâˆ’UâŠ¤Âµx)âŠ¤= UâŠ¤Î£xU,
therefore choose U such that Î£y = UâŠ¤Î£U is a diagonal matrix Î£y =
diag(Ïƒ2
1, ..., Ïƒ2
n). We have in that case:
p(x) =
1
(2Ï€)n/2 Q
i Ïƒi
eâˆ’1
2
P
i
â€œ xiâˆ’Âµi
Ïƒi
â€2
which can be written as a product of univariate normal distributions pxi(xi):
p(x) =
n
Y
i=1
1
(2Ï€)1/2Ïƒi
eâˆ’1
2
â€œ xiâˆ’Âµi
Ïƒi
â€2
=
n
Y
i=1
pxi(xi),
which proves the assertion that decorrelated normally distributed variables
are statistically independent.
5.2 PCA: Optimal Reconstruction
A diï¬€erent, yet equivalent, perspective on the PCA transformation is as an
optimal reconstruction (in a least squares sense) after a dimension reduction.
We are given a sample data as before x1, ..., xm and we are looking for a small
number of orthonormal principal vectors u1, ..., uq where q < min(n, k)
which deï¬ne a q-dimensional linear subspace of Rn which best approximate
the original input vectors in a least squares sense.
In other words, the
projection Ë†xi of the sample points xi onto the q-dimensional subspace should
minimize P
i âˆ¥xi âˆ’Ë†xiâˆ¥2 over all possible q-dimensional subspaces of Rn.
Let U be the subspace spanned by the principal vectors (columns of U)
and let P be the n Ã— n projection matrix mapping a point x âˆˆRn onto its
projection Ë†x âˆˆU. From the deï¬nition of projection, the vector x âˆ’Ë†x must
48
Spectral Analysis I: PCA, LDA, CCA
be orthogonal to the subspace U. Let y = (y1, ..., yq) be the coordinates of Ë†x
with respect to the principal vectors, i.e., Uy = Ë†x. Then, from orthogonality
we have that (x âˆ’Uy)âŠ¤Uw = 0 for all vectors w âˆˆRn. Since this is true
for all w then UâŠ¤Uy âˆ’UâŠ¤x = 0. Therefore, y = (UâŠ¤U)âˆ’1UâŠ¤x and as a
result the projection matrix P becomes:
P = U(UâŠ¤U)âˆ’1UâŠ¤,
satisfying Px = Ë†x. In the case the columns of U are orthonormal, UâŠ¤U = I,
we have P = UUâŠ¤. We are ready now to describe the optimization problem
on U: we wish to ï¬nd an orthonormal set of principal vectors, UâŠ¤U = I,
such that P
i âˆ¥xi âˆ’UUâŠ¤xiâˆ¥2 is minimized.
Note that P
i âˆ¥xi âˆ’UUâŠ¤xiâˆ¥2 = âˆ¥A âˆ’UUâŠ¤Aâˆ¥2
F where âˆ¥Bâˆ¥2
F = P
i,j b2
ij
is the square Frobenious norm of a matrix.
The optimal reconstruction
problem therefore becomes:
min
U âˆ¥A âˆ’UUâŠ¤Aâˆ¥2
F
subject to
UâŠ¤U = I.
We will show now that:
argmin
U
âˆ¥A âˆ’UUâŠ¤Aâˆ¥2
F = argmax
U
trace(UâŠ¤AAâŠ¤U),
which shows that the optimal reconstruction problem is solved by PCA
(recall Eqn. 5.1).
From the identity âˆ¥Bâˆ¥2
F = trace(BBâŠ¤), we have:
âˆ¥A âˆ’UUâŠ¤Aâˆ¥2
F = trace((A âˆ’UUâŠ¤A)(A âˆ’UUâŠ¤A)âŠ¤).
Expanding the right hand side gives us:
trace((A âˆ’UUâŠ¤A)(A âˆ’UUâŠ¤A)âŠ¤)
=
trace(AAâŠ¤) âˆ’trace(AAâŠ¤UUâŠ¤)
âˆ’
trace(UUâŠ¤AAâŠ¤) + trace(UUâŠ¤AAâŠ¤UUâŠ¤)
The second and third term are equal (commutativity of trace) and is also
equal to the 4th term due to commutativity of the trace and UâŠ¤U = I.
Taken together:
âˆ¥A âˆ’UUâŠ¤Aâˆ¥2
F = trace(AAâŠ¤) âˆ’trace(UâŠ¤AAâŠ¤U).
To conclude, we have proven that by taking the ï¬rst q eigenvectors of AAâŠ¤
we obtain a linear subspace which is as close as possible (in a least squares
sense) to the original sample data. Hence, PCA can be viewed as a vehi-
cle for optimal reconstruction after dimension reduction. The optimization
5.3 The Case n >> m
49
problem whose solution is the leading q eigenvectors of AAâŠ¤is described in
eqn. 5.1:
max
UâˆˆRnÃ—q trace(UâŠ¤AAâŠ¤U)
subject to
UâŠ¤U = I.
5.3 The Case n >> m
Consider the situation where n, the dimension of the input vectors, is rela-
tively large compared to the number of sample vectors m. For example, con-
sider input vectors representing 50 Ã— 50 sized images of faces, i.e., n = 2500,
where m = 100. In other words, we are looking for a small number of â€œface
templatesâ€ (known as â€œeigenfacesâ€) which approximate well the original set
of 100 face images. In this case, AAâŠ¤is very large, 2500Ã—2500, yet the num-
ber of non-vanishing eigenvalues cannot be higher than 100. Given that the
eigendecomposition process is O(25003), the computational burden would
be very high. However, it is possible to perform an eigendecomposition on
AâŠ¤A (a 100 Ã— 100 matrix) instead, as shown next.
Let the columns of Q be the ï¬rst q < m eigenvectors of AâŠ¤A, i.e., AâŠ¤AQ =
QD where D is diagonal containing the corresponding eigenvalues. After
pre-multiplying both sides by A we obtain:
AAâŠ¤(AQ) = (AQ)D,
from which we conclude that AQ contains the ï¬rst q eigenvectors (but un-
normalized) of AAâŠ¤. We have therefore that U = AQDâˆ’1
2 because:
UâŠ¤U = Dâˆ’1
2 QâŠ¤AâŠ¤AQDâˆ’1
2 = Dâˆ’1
2 DDâˆ’1
2 = I,
where we used the fact that QâŠ¤AâŠ¤AQ = D. Note that eigenvalues of AâŠ¤A
and AAâŠ¤are the same (because AAâŠ¤(AQDâˆ’1
2 ) = (AQDâˆ’1
2 )D).
5.4 Kernel PCA
We can take the case n >> m described in the previous section one step fur-
ther and consider such large values of n which are practically uncomputable
â€” a situation which results when mapping the original input vectors to a
high dimensional space: Ï†(x) where Ï† : Rn â†’F for which dim(F) >> n.
For example, Ï†(x) representing the dâ€™th order monomials of the coordinates
of x, i.e., dim(F) =
 n+dâˆ’1
d

which is exponential in d.
The mappings
of interest are those which are paired with a non-linear kernel function:
k(x, xâ€²) = Ï†(x)âŠ¤Ï†(xâ€²).
Performing PCA on A = [Ï†(x1), ..., Ï†(xm)] is equivalent to ï¬nding the
50
Spectral Analysis I: PCA, LDA, CCA
non-linear surface in Rn (the nature of the non-linearity depends on the
choice of Ï†()) which best approximates the original sample data x1, ..., xm.
The problem is that AAâŠ¤is not computable â€” however AâŠ¤A is computable
because (AâŠ¤A)ij = k(xi, xj).
From the previous section, U = AQDâˆ’1
2 = AV contains the ï¬rst q eigen-
vectors of AAâŠ¤(where Q and D are computable).
Since A itself is not
computable we cannot represent U explicitly, but we can project a new
vector Ï†(x) onto the principal directions u1, ..., uq and obtain the principal
components, i.e., the output vector y = UâŠ¤Ï†(x), as follows.
y = UâŠ¤Ï†(x) = V âŠ¤AâŠ¤Ï†(x) = V âŠ¤
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
k(x1, x)
.
.
.
k(xm, x)
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.
Given the principal components (entries of y = UâŠ¤Ï†(x) of Ï†(x)) we can
measure, for example, the distance between Ï†(x) and the projection
Ë†
Ï†(x) =
UUâŠ¤Ï†(x) = Uy onto the linear subspace spanned by u1, ..., uq (without the
need to explicitly compute the principal axes ui), as follows.
âˆ¥Ï†(x) âˆ’
Ë†
Ï†(x)âˆ¥2
=
Ï†(x)âŠ¤Ï†(x) +
Ë†
Ï†(x)
âŠ¤Ë†
Ï†(x) âˆ’2Ï†(x)âŠ¤Ë†
Ï†(x)
=
k(x, x) + yâŠ¤UâŠ¤Uy âˆ’2Ï†(x)âŠ¤(UUâŠ¤Ï†(x))
=
k(x, x) âˆ’yâŠ¤y âˆ’2yâŠ¤y
=
k(x, x) âˆ’âˆ¥yâˆ¥2
5.5 Fisherâ€™s LDA: Basic Idea
We now extend the variance preserving approach for data representation for
labeled data sets. We will focus on 2-class sets and look for a separating
hyperplane:
f(x) = wâŠ¤x + b,
such that x belongs to the ï¬rst class if f(x) > 0 and x belongs to the second
class if f(x) < 0. In the statistical literature this type of function is called
a linear discriminant function. The decision boundary is given by the set
of points satisfying f(x) = 0 which is a hyperplane. Fisherâ€™s (1936) Linear
Discriminant Analysis (LDA) is a variance preserving approach for ï¬nding
a linear discriminant function.
5.5 Fisherâ€™s LDA: Basic Idea
51
Fig. 5.1. Linear discriminant analysis based on class centers alone is not suï¬ƒcient.
Seeking a projection which maximizes the distance between the projected centers
will prefer the horizontal axis over the vertical, yet the two classes overlap on the
horizontal axis. The projected distance along the vertical axis is smaller yet the
classes are better separated. The conclusion is that the sample variance of the two
classes must be taken into consideration as well.
We will then introduce another popular statistical technique called Canon-
ical Correlation Analysis (CCA) for learning the mapping between input and
output vectors using the notion â€angleâ€ between subspaces.
What is common in the three techniques PCA, LDA and CCA is the use
of spectral matrix analysis â€” i.e., what can you do with eigenvalues and
eigenvectors of matrices representing subspaces of the data? These tech-
niques produce optimal results for normally distributed data and are very
easy to implement. There is a large variety of uses of spectral analysis in
statistical and learning literature including spectral clustering, Multi Di-
mensional Scaling (MDS) and data modeling in general.
To appreciate the general idea behind Fisherâ€™s LDA consider Fig. 5.1. Let
the centers of classes one and two be denoted by Âµ1 and Âµ2 respectively. A
linear discriminant function is a projection onto a 1D subspace such that
the classes would be separated the most in the 1D subspace. The obvious
ï¬rst step in this kind of analysis is to make sure that the projected centers
Ë†Âµ1, Ë†Âµ2 would be separated as much as possible. We can easily see that the
direction of the 1D subspace should be proportional to Âµ1 âˆ’Âµ2 as follows:
(Ë†Âµ1 âˆ’Ë†Âµ2)2 =
wâŠ¤Âµ1
âˆ¥wâˆ¥âˆ’wâŠ¤Âµ2
âˆ¥wâˆ¥
2
=
 wâŠ¤
âˆ¥wâˆ¥(Âµ1 âˆ’Âµ2)
2
.
The right-hand term is maximized when w â‰ˆÂµ1 âˆ’Âµ2. As illustrated in
52
Spectral Analysis I: PCA, LDA, CCA
Fig. 5.1, this type of consideration is not suï¬ƒcient to capture separability
in the projected subspace because the spread (variance) of the data points
around their centers also play an important role. For example, the horizontal
axis in the ï¬gure separates the centers better than the vertical axis but on
the other hand does a worse job in separating the classes themselves because
of the way the data points are spread around their centers. The argument
in favor of separating the centers would work if the data points were living
in a hyper-sphere around the centers, but will not be suï¬ƒcient otherwise.
The basic idea behind Fisherâ€™s LDA is to consider the sample covariance
matrix of the individual classes as well as their centers, in the following way.
The optimal 1D projection would that which maximizes the variance of the
projected centers while minimizes the variance of the projected data points
of each class separately. Mathematically, this idea can be implemented by
maximizes the following ratio:
max
w
(Ë†Âµ1 âˆ’Ë†Âµ2)2
s2
1 + s2
2
,
where s2
1 is the scaled variance of the projected points of the ï¬rst class:
s2
1 =
X
xiâˆˆC1
( Ë†xi âˆ’Ë†Âµ1)2,
and likewise,
s2
2 =
X
xiâˆˆC2
( Ë†xi âˆ’Ë†Âµ2)2,
where Ë†x = wâŠ¤
âˆ¥wâˆ¥xi + b.
We will now formalize this approach and derive its solution. We will begin
with a general description of a multiclass problem where the sample data
points belong to q diï¬€erent classes, and later focus on the case of q = 2.
5.6 Fisherâ€™s LDA: General Derivation
Let the sample data points S be members of q classes C1, ..., Cq where the
number of points belonging to class Ci is denoted by li and the total number
of the training set is l = P
i li. Let Âµj denote the center of class Ci and Âµ
denote the center of the complete training set S:
Âµj
=
1
lj
X
bfxiâˆˆCj
xi
Âµ
=
1
l
X
xiâˆˆS
xi
5.6 Fisherâ€™s LDA: General Derivation
53
Let Aj be the matrix associated with class Cj whose columns consists of the
mean shifted data points:
Aj = [x1 âˆ’Âµj, ..., xlj âˆ’Âµj]
xi âˆˆCj.
Then,
1
lj AjAâŠ¤
j is the covariance matrix associated with class Cj. Let Sw
(where â€wâ€ stands for â€withinâ€) be the sum of the class covariance matrices:
Sw =
q
X
i
1
lj
AjAâŠ¤
j .
From the discussion in the previous section, it is
1
âˆ¥wâˆ¥2 wâŠ¤Sww which we
wish to minimize. To see why this is so, note
X
xiâˆˆCj
( Ë†xi âˆ’Ë†Âµj)2 =
X
xiâˆˆCj
wâŠ¤(xi âˆ’Âµj)2
âˆ¥wâˆ¥2
=
1
âˆ¥wâˆ¥2 wâŠ¤AjAâŠ¤
j w.
Let B be the matrix holding the class centers:
B = [Âµ1 âˆ’Âµ, ..., Âµq âˆ’Âµ],
and let Sb = 1
qBBâŠ¤(where â€bâ€ stands for â€betweenâ€). From the discussion
above it is
1
âˆ¥wâˆ¥2 wâŠ¤Sbw = P
i(Ë†Âµi âˆ’Ë†Âµ)2 which we wish to maximize. Taken
together, we wish to maximize the ratio (called â€Rayleighâ€™s quotientâ€):
max
w J(w) = wâŠ¤Sbw
wâŠ¤Sww.
The necessary condition for optimality is:
âˆ‚J
âˆ‚w = Sbw(wâŠ¤Sww) âˆ’Sww(wâŠ¤Sbw)
(wâŠ¤Sww)2
= 0,
From which we obtain the generalized eigensystem:
Sbw = J(w)Sww.
(5.2)
That is, w is the leading eigenvector of Sâˆ’1
w Sb (assuming Sw is invertible).
The general case of ï¬nding q such axes involves ï¬nding the leading general-
ized eigenvectors of (Sb, Sw) â€” the derivation is out of scope of this lecture.
Note that since Sâˆ’1
w Sb is not symmetric there may be no real-value solution,
which is a complication will not pursue further in this course. Instead we
will focus now on the 2-class (q = 2) setting below.
54
Spectral Analysis I: PCA, LDA, CCA
5.7 Fisherâ€™s LDA: 2-class
The general derivation is simpliï¬ed when there are only two classes. The
covariance matrix BBâŠ¤becomes a rank-1 matrix:
BBâŠ¤= (Âµ1 âˆ’Âµ)(Âµ1 âˆ’Âµ)âŠ¤+ (Âµ2 âˆ’Âµ)(Âµ2 âˆ’Âµ)âŠ¤= (Âµ1 âˆ’Âµ2)(Âµ1 âˆ’Âµ2)âŠ¤.
As a result, BBâŠ¤w is a vector in direction Âµ1 âˆ’Âµ2. Therefore, the solution
for w from eqn. 5.2 is:
w âˆ¼= Sâˆ’1
w (Âµ1 âˆ’Âµ2).
The decision boundary wâŠ¤(x âˆ’Âµ) = 0 becomes:
xâŠ¤Sâˆ’1
w (Âµ1 âˆ’Âµ2) âˆ’1
2(Âµ1 + Âµ2)âŠ¤Sâˆ’1
w (Âµ1 âˆ’Âµ2) = 0.
(5.3)
This decision boundary will surface again in the course when we consider
Bayseian inference.
It will be shown that this decision boundary is the
Maximum Likelihood solution in the case where the two classes are normally
distributed with means Âµ1, Âµ2 and with the same covariance matrix Sw.
5.8 LDA versus SVM
Both LDA and SVM search for a so called â€optimalâ€ linear discriminant
function, what is the diï¬€erence? The heart of the matter lies in the deï¬nition
of what constitutes a suï¬ƒcient compact representation of the data. In LDA
the assumption is that each class can be represented by its mean vector and
its spread (i.e., covariance matrix). This is true for normally distributed
data â€” but not true in general. This means that we should expect that
LDA will produce the optimal discriminant linear function when each of the
classes are normally distributed.
With SVM, on the other hand, there is no assumption on how the data
is distributed. Instead, the emerging result is that the data is represented
by the subset of data points which lie on the boundary between the two
classes (the so called support vectors). Rather than making a parametric
assumption on how the data can be captured (i.e., mean and covariance) the
theory shows that the data can be captured by a special subset of points. The
tools, as a result, are naturally more complex (quadratic linear programming
versus spectral matrix analysis) â€” but the advantage is that optimality is
guaranteed without making assumptions on the distribution of the data (i.e.,
distribution free). It can be shown that SVM and LDA would produce the
same result if the class data is normally distributed.
5.9 Canonical Correlation Analysis
55
5.9 Canonical Correlation Analysis
CCA is a technique for learning a mapping f(x) = y where x âˆˆRk and
y âˆˆRs using the notion of subspace similarity (an extension of the inner
product between two vectors) from a training set of (xi, yi), i = 1, ..., n. Such
a mapping, where y can be any point in Rk as opposed to a discrete set of
labels, is often referred to as a â€regressionâ€ (as opposed to â€classiï¬cationâ€).
Like in PCA and LDA, the approach would be to look for projection axes
such that the projection of the input and output vectors on those axes satisfy
certain requirements â€” and like PCA and LDA the tools we would be using
is matrix spectral analysis.
It will be convenient to stack our vectors as rows of an input matrix A and
output matrix B. Let A be an n Ã— k matrix whose rows are xâŠ¤
1 , ..., xâŠ¤
n and
B is the n Ã— s matrix whose rows are yâŠ¤
1 , ..., yâŠ¤
n . Consider vectors u âˆˆRk
and v âˆˆRs and project the input and output data onto them producing
Au = (xâŠ¤
1 u, ..., xâŠ¤
n u) and Bv. The requirement we would like to place on
the projection axes is that Au â‰ˆBv, or in other words that (Au)âŠ¤(Bv)
is maximal. The requirement therefore is that the projection of the input
points onto the u axis is similar to the projection of the output points
onto the v axis. If we extend this notion to multiple axes u1, ..., uq (not
necessarily orthogonal) and v1, ..., vq where q â‰¤min(k, s) our requirement
becomes that the new coordinates of the input points projected onto the
subspace spanned by the u vectors are similar to the new coordinates of
the output points projected onto the subspace spanned by the v vectors.
In other words, we wish to ï¬nd two q-dimensional subspaces one of Rk and
the other of Rs such that the two sets of projected points are as aligned as
possible.
CCA goes a step further and makes the assumption that the input/output
relationship is solely determined by the relation (angles) between the column
spaces of A, B. In other words, the particular columns of A are not really
important, what is important is the space UA spanned by the columns.
Since g = Au is a point in UA (a linear combination of the columns of
A) and h = Bv is a point in UB, then gâŠ¤h is the cosine angle, cos(Ï†)
between the two axes provided that we normalize the vectors g and h. If
we continue this line of reasoning recursively, we obtain a set of angles
0 â‰¤Î¸1 â‰¤... â‰¤Î¸q â‰¤(Ï€/2), called â€principal anglesâ€, between the two
subspaces uniquely deï¬ned as:
cos(Î¸j) = max
gâˆˆUA max
hâˆˆUB
gâŠ¤h
(5.4)
56
Spectral Analysis I: PCA, LDA, CCA
subject to:
gâŠ¤g = hâŠ¤h = 1,
hâŠ¤hi = 0, gâŠ¤gi = 0,
i = 1, ..., j âˆ’1
As a result, we obtain the following optimization function over axes u, v:
max
u,v uâŠ¤AâŠ¤Bv
s.t.
âˆ¥Auâˆ¥2 = 1,
âˆ¥Bvâˆ¥2 = 1.
To solve this problem we ï¬rst perform a â€QRâ€ factorization of A and B. A
â€QRâ€ factorization of a matrix A is a Grahm-Schmidt process resulting in
an orthonormal set of vectors arranged as the columns of a matrix QA whose
column space is equal to the column space of A, and a matrix RA which
contains the coeï¬ƒcients of the linear combination of the columns of QA such
that A = QARA. Since orthoganilzation is not unique, the Grahm-Schmidt
process perfroms the orthogonalization such that RA is an upper-diagonal
matrix. Likewise let B = QBRB. Because the column spaces of A and QA
are the same, then for every u there exists a Ë†u such that Au = QAË†u. Our
optimization problem now becomes:
max
Ë†u,Ë†v
Ë†uâŠ¤QâŠ¤
AQBË†v
s.t.
âˆ¥Ë†uâˆ¥2 = 1,
âˆ¥Ë†vâˆ¥2 = 1.
The solution of this problem is when Ë†u and Ë†v are the leading singular vectors
of QâŠ¤
AQB. The singular value decomposition (SVD) of any matrix E is a
decomposition E = UDV âŠ¤where the columns of U are the leading eigen-
vectors of EEâŠ¤, the rows of V âŠ¤are the leading eigenvectors of EâŠ¤E and
D is a diagonal matrix whose entries are the corresponding square eigen-
values (note that the eigenvalues of EEâŠ¤and EâŠ¤E are the same). The
SVD decomposition has the property that if we keep only the ï¬rst q leading
eigenvectors then UDV âŠ¤is the closest (in least squares sense) rank q matrix
to E.
Therefore, let Ë†UD Ë†V âŠ¤be the SVD of QâŠ¤
AQB using the ï¬rst q eigenvectors.
Then, our sought after axes U = [u1, ..., uq] is simply Râˆ’1
A Ë†U and likewise
and the axes V = [v1, ..., vq] is equal to Râˆ’1
B Ë†V . The axes are called â€canon-
ical vectorsâ€, and the vectors gi = Aui (mutually orthogonal) are called
â€variatesâ€. The concept of principal angles is due to Jordan in 1875, where
Hotelling in 1936 is the ï¬rst to introduce the recursive deï¬nition above.
Given a new vector x âˆˆRk the resulting vector y can be found by solving
the linear system UâŠ¤x = V âŠ¤y (since our assumption is that in the new basis
the coordinates of x and y are similar).
To conclude, the relationship between A and B is captured by creating
similar variates, i.e., creating subspaces of dimension q such that the projec-
tions of the input vectors and the output vectors have similar coordinates.
5.9 Canonical Correlation Analysis
57
The process for obtaining the two q-dimensional subspaces is by performing
a QR factorization of A and B followed by an SVD. Here again the spectral
analysis of the input and output data matrices plays a pivoting role in the
input/output association.
6
Spectral Analysis II: Clustering
In the previous lecture we ended up with the formulation:
max
GmÃ—k trace(GâŠ¤KG)
s.t. GâŠ¤G = I
(6.1)
and showed the solution G is the leading eigenvectors of the symmetric pos-
itive semi deï¬nite matrix K. When K = AAâŠ¤(sample covariance matrix)
with A = [x1, ..., xm], xi âˆˆRn, those eigenvectors form a basis to a k-
dimensional subspace of Rn which is the closest (in L2 norm sense) to the
sample points xi. The axes (called principal axes) g1, ..., gk preserve the
variance of the original data in the sense that the projection of the data
points on the g1 has maximum variance, projection on g2 has the maximum
variance over all vectors orthogonal to g1, etc. The spectral decomposition
of the sample covariance matrix is a way to â€compressâ€ the data by means
of linear super-position of the original coordinates y = GâŠ¤x.
We also ended with a ratio formulation:
max
w
wâŠ¤S1w
wâŠ¤S2w
where S1, S2 where scatter matrices deï¬ned such that wâŠ¤S1w is the variance
of class centers (which we wish to maximize) and wâŠ¤S2w is the sum of
within class variance (which we want to minimize). The solution w is the
generalized eigenvector S1w = Î»S2w with maximal Î».
In this lecture we will show additional applications where the search for
leading eigenvectors plays a pivotal part of the solution. So far we have
seen how spectral analysis relates to PCA and LDA and today we will fo-
cus on the classic Data Clustering problem of partitioning a set of points
x1, ..., xm into k â‰¥2 classes, i.e., generating as output indicator variables
y1, ..., ym where yi âˆˆ{1, ..., k}. We will begin with â€K-meansâ€ algorithm for
clustering and then move on to show how the optimization criteria relates
58
6.1 K-means Algorithm for Clustering
59
to grapth-theoretic approaches (like Min-Cut, Ratio-Cut, Normalized Cuts)
and spectral decomposition.
6.1 K-means Algorithm for Clustering
The K-means formulation (originally introduced by [4]) assumes that the
clusters are deï¬ned by the distance of the points to their class centers only.
In other words, the goal of clustering is to ï¬nd those k mean vectors c1, ..., ck
and provide the cluster assignment yi âˆˆ{1, ..., k} of each point xi in the set.
The K-means algorithm is based on an interleaving approach where the
cluster assignments yi are established given the centers and the centers are
computed given the assignments. The optimization criterion is as follows:
min
y1,...,ym,c1,...,ck
k
X
j=1
X
yi=j
âˆ¥xi âˆ’cjâˆ¥2
(6.2)
Assume that c1, ..., ck are given from the previous iteration, then
yi = argmin
j
âˆ¥xi âˆ’cjâˆ¥2,
and next assume that y1, .., ym (cluster assignments) are given, then for any
set S âŠ†{1, ..., m} we have that
1
|S|
X
jâˆˆS
xj = argmin
c
X
jâˆˆS
âˆ¥xj âˆ’câˆ¥2.
In other words, given the estimated centers in the current round, the new
assignments are computed by the closest center to each point xi, and then
given the updated assignments the new centers are estimated by taking the
mean of each cluster. Since each step is guaranteed to reduce the optimiza-
tion energy the process must converge â€” to some local optimum.
The drawback of the K-means algorithm is that the quality of the local
optimum strongly depends on the initial guess (either the centers or the
assignments). If we start with a wild guess for the centers it would be fairly
unlikely that the process would converge to a good local minimum (i.e. one
that is close to the global optimum). An alternative approach would be to
deï¬ne an approximate but simpler problem which has a closed form solution
(such as obtained by computing eigenvectors of some matrix). The global
optimum of the K-means is an NP-Complete problem (mentioned brieï¬‚y in
the next section).
Next, we will rewrite the K-means optimization criterion in matrix form
and see that it relates to the spectral formulation (eqn. 6.1).
60
Spectral Analysis II: Clustering
6.1.1 Matrix Formulation of K-means
We rewrite eqn. 6.2 as follows [7]. Instead of carrying the class variables yi
we deï¬ne class sets Ïˆ1, ..., Ïˆk where Ïˆi âŠ‚{1, ..., n} with S Ïˆj = {1, ..., n}
and Ïˆi
T Ïˆj = âˆ…. The K-means optimization criterion seeks for the centers
and the class sets:
min
Ïˆ1,...,Ïˆk,c1,...,ck
k
X
j=1
X
iâˆˆÏˆj
âˆ¥xi âˆ’cjâˆ¥2.
Let lj = |Ïˆj| and following the expansion of the squared norm and dropping
xâŠ¤
i xi we end up with an equivalent problem:
min
Ïˆ1,...,Ïˆk,c1,...,ck
k
X
j=1
ljcâŠ¤
j cj âˆ’2
k
X
j=1
X
iâˆˆÏˆj
xâŠ¤
i cj.
Next we substitute cj with its deï¬nition: (1/lj) P
iâˆˆÏˆj xj and obtain a new
equivalent formulation where the centers cj are eliminated form considera-
tion:
min
Ïˆ1,...,Ïˆk
âˆ’
k
X
j=1
1
lj
X
r,sâˆˆÏˆj
xâŠ¤
r xs
which is more conveniently written as a maximization problem:
max
Ïˆ1,...,Ïˆk
k
X
j=1
1
lj
X
r,sâˆˆÏˆj
xâŠ¤
r xs.
(6.3)
Since the resulting formulation involves only inner-products we could have
replaced xi with Ï†(xi) in eqn. 6.2 where the mapping Ï†(Â·) is chosen such
that Ï†(xi)âŠ¤Ï†(xj) can be replaced by some non-linear function Îº(xi, xj) â€”
known as the â€kernel trickâ€ (discussed in previous lectures). Having the
ability to map the input vectors onto some high-dimensional space before
K-means is applied provides more ï¬‚exibility and increases our chances of
getting out a â€goodâ€ clustering from the global K-means solution (again,
the local optimum depends on the initial conditions so it could be â€badâ€).
The RBF kernel is quite popular in this context Îº(xi, xj) = eâˆ’âˆ¥xiâˆ’xjâˆ¥2/Ïƒ2
with Ïƒ some pre-determined parameter. Note that Îº(xi, xj) âˆˆ(0, 1] which
can be interpreted loosely as the probability of xi and xj to be clustered
together.
Let Kij = Îº(xi, xj) making K a m Ã— m symmetric positive-semi-deï¬nite
matrix often referred to as the â€aï¬ƒnityâ€ matrix. Let F be an n Ã— n matrix
whose entries are Fij = 1/lr if (i, j) âˆˆÏˆr for some class Ïˆr and Fij = 0
6.1 K-means Algorithm for Clustering
61
otherwise.
In other words, if we sort the points xi according to cluster
membership, then F is a block diagonal matrix with blocks F1, ..., Fk where
Fr = (1/lr)11âŠ¤is an lr Ã— lr block of 1â€™s scaled by 1/lr. Then, Eqn. 6.3 can
be written in terms of K as follows:
max
F
n
X
i,j=1
KijFij = trace(KF)
(6.4)
In order to form this as an optimization problem we need to represent the
structure of F in terms of constraints. Let G be an n Ã— k column-scaled
indicator matrix: Gij = (1/
p
lj) if i âˆˆÏˆj (i.e., xi belongs to the jâ€™th class)
and Gij = 0 otherwise. Let g1, ..., gk be the columns of G and it can be easily
veriï¬ed that grgr
âŠ¤= diag(0, .., Fr, 0, .., 0) therefore F = P
j gjgâŠ¤
j = GGâŠ¤.
Since trace(AB) = trace(BA) we can now write eqn. 6.4 in terms of G:
max
G trace(GâŠ¤KG)
under conditions on G which we need to further spell out.
We will start with the necessary conditions. Clearly G â‰¥0 (has non-
negative entries). Because each point belongs to exactly one cluster we must
have GâŠ¤Gij = 0 when i Ì¸= j and GâŠ¤Gii = (1/li)1âŠ¤1 = 1, thus GâŠ¤G = I.
Furthermore we have that the rows and columns of F = GGâŠ¤sum up to
1, i.e., F1 = 1, F âŠ¤1 = 1 which means that F is doubly stochastic which
translates to the constraint GGâŠ¤1 = 1 on G.
We have therefore three
necessary conditions on G: (i) G â‰¥0, (ii) GâŠ¤G = I, and (iii) GGâŠ¤1 = 1.
The claim below asserts that these are also suï¬ƒcient conditions:
Claim 4 The feasibility set of matrices G which satisfy the three conditions
G â‰¥0, GGâŠ¤1 = 1 and GâŠ¤G = I are of the form:
Gij =
(
1
âˆš
lj
xi âˆˆÏˆj
0
otherwise
)
Proof:
From G â‰¥0 and gâŠ¤
r gs = 0 we have that GirGis = 0, i.e., G
has a single non-vanishing element in each row. It will be convenient to
assume that the points are sorted according to the class membership, thus
the columns of G have the non-vanishing entries in consecutive order and
let lj be the number of non-vanishing entries in column gj.
Let uj the
vector of lj entries holding only the non-vanishing entries of gj.
Then,
the doubly stochastic constraint GGâŠ¤1 = 1 results that (1âŠ¤uj)uj = 1 for
j = 1, ..., k.
Multiplying 1 from both sides yields (1âŠ¤uj)2 = 1âŠ¤1 = lj,
therefore uj = (1/
p
lj)1.
62
Spectral Analysis II: Clustering
This completes the equivalence between the matrix formulation:
max
GâˆˆRmÃ—k trace(GâŠ¤KG)
s.t. G â‰¥0, GâŠ¤G = I, GGâŠ¤1 = 1
(6.5)
and the original K-means formulation of eqn. 6.2.
We have obtained the same optimization criteria as eqn. 6.1 with addi-
tional two constraints: G should be non-negative and GGâŠ¤should be doubly
stochastic. The constraint GâŠ¤G = I comes from the requirement that each
point is assigned to one class only. The doubly stochastic constraint comes
from a â€class balancingâ€ requirement which we will expand on below.
6.2 Min-Cut
We will arrive to eqn. 6.5 from a graph-theoretic perspective.
We start
with representing the graph Min-Cut problem in matrix form, as follows.
A convenient way to represent the data to be clustered is by an undirected
graph with edge-weights where V = {1, ..., m} is the vertex set, E âŠ‚V Ã— V
is the edge set and Îº : E â†’R+ is the positive weight function. Vertices
of the graph correspond to data points xi, edges represent neighborhood
relationships, and edge-weights represent the similarity (aï¬ƒnity) between
pairs of linked vertices. The weight adjacency matrix K holds the weights
where Kij = Îº(i, j) for (i, j) âˆˆE and Kij = 0 otherwise.
A cut in the graph is deï¬ned between two disjoint sets A, B âŠ‚V , A âˆª
B = V , is the sum of edge-weights connecting the two sets: cut(A, B) =
P
iâˆˆA,jâˆˆB Kij which is a measure of dissimilarity between the two sets. The
Min-Cut problem is to ï¬nd a minimal weight cut in the graph (can be solved
in polynomial time through Max Network Flow solution). The following
claim associates algebraic conditions on G with an indicator matrix:
Claim 5 The feasibility set of matrices G which satisfy the three conditions
G â‰¥0, G1 = 1 and GâŠ¤G = D for some diagonal matrix D are of the form:
Gij =
 1
xi âˆˆÏˆj
0
otherwise

Proof:
Let G = [g1, ..., gk]. From G â‰¥0 and gâŠ¤
r gs = 0 we have that
GirGis = 0, i.e., G has a single non-vanishing element in each row. From
G1 = 1 the single non-vanishing entry of each row must have the value of
1.
In the case of two classes (k = 2), the function tr(GâŠ¤KG) is equal to
P
(i,j)âˆˆÏˆ1 Kij + P
(i,j)âˆˆÏˆ2 Kij. Therefore maxG tr(GâŠ¤KG) is equivalent to
6.3 Spectral Clustering: Ratio-Cuts and Normalized-Cuts
63
minimizing the cut: P
iâˆˆÏˆ1,jâˆˆÏˆ2 Kij. As a result, the Min-Cut problem is
equivalent to solving the optimization problem:
max
GâˆˆRmÃ—2 tr(GâŠ¤KG) s.t
G â‰¥0, G1 = 1, GâŠ¤G = diag
(6.6)
We seem to be close to eqn. 6.5 with the diï¬€erence that G is orthogonal
(instead of orthonormal) and the doubly-stochasitc constraint is replaced
by G1 = 1. The diï¬€erence can be bridged by considering a â€balancingâ€
requirement. Min-Cut can produce an unbalanced partition where one set
of vertices is very large and the other contains a spurious set of vertices
having a small number of edges to the larger set. This is an undesirable
outcome in the context of clustering.
Consider a â€balancingâ€ constraint
GâŠ¤1 = (m/k)1 which makes a strict requirement that all the k clusters have
an equal number of points. We can relax the balancing constraint slightly by
combining the balancing constraint with G1 = 1 into one single constraint
GGâŠ¤1 = (m/k)1, i.e., GGâŠ¤is scaled doubly stochastic. Note that the two
conditions GGâŠ¤1 = (m/k)1 and GâŠ¤G = D result in D = (m/k)I. Thus we
propose the relaxed-balanced hard clustering scheme:
max
G tr(GâŠ¤KG) s.t
G â‰¥0, GGâŠ¤1 = m
k 1, GâŠ¤G = m
k I
The scale m/k is a global scale that can be dropped without aï¬€ecting the
resulting solution, thus the Min-Cut with a relaxed balancing requirement
becomes eqn. 6.5 which we saw is equivalent to K-means:
max
G tr(GâŠ¤KG) s.t
G â‰¥0, GGâŠ¤1 = 1, GâŠ¤G = I.
6.3 Spectral Clustering: Ratio-Cuts and Normalized-Cuts
We saw above that the doubly-stochastic constraint has to do with a â€bal-
ancingâ€ desire. A further relaxation of the balancing desire is to perform the
optimization in two steps: (i) replace the aï¬ƒnity matrix K with the closest
(under some chosen error measure) doubly-stochastic matrix Kâ€², (ii) ï¬nd a
solution to the problem:
max
GâˆˆRmÃ—k tr(GâŠ¤Kâ€²G) s.t
G â‰¥0, GâŠ¤G = I
(6.7)
because GGâŠ¤should come out close to Kâ€² (tr(GâŠ¤Kâ€²G) = tr(Kâ€²GGâŠ¤))
and Kâ€² is doubly-stochastic, then GGâŠ¤should come out close to satisfying
a doubly-stochastic constraint â€” this is the motivation behind the 2-step
approach. Moreover, we drop the non-negativity constraint G â‰¥0. Note
that the non-negativity constraint is crucial for the physical interpretation of
64
Spectral Analysis II: Clustering
G; nevertheless, for k = 2 clusters it is possible to make an interpretation, as
we shall next. As a result we are left with a spectral decomposition problem
of eqn. 6.1:
max
GâˆˆRmÃ—k tr(GâŠ¤Kâ€²G) s.t
GâŠ¤G = I,
where the columns of G are the leading eigenvectors of Kâ€². We will refer
to the ï¬rst step as a â€normalizationâ€ process and there are two popular
normalizations in the literature â€” one leading to Ratio-Cuts and the other
to Normalized-Cuts.
6.3.1 Ratio-Cuts
Let D = diag(K1) which is a diagonal matrix containing the row sums of
K. The Ratio-Cuts normalization is to look for Kâ€² as the closest doubly-
stochastic matrix to K by minimizing the L1 norm â€” this turns out to be
Kâ€² = K âˆ’D + I.
Claim 6 (ratio-cut) Let K be a symmetric positive-semi-deï¬nite whose
values are in the range [0, 1]. The closest doubly stochastic matrix Kâ€² under
the L1 error norm is
Kâ€² = K âˆ’D + I
Proof: Let r = minF âˆ¥K âˆ’Fâˆ¥1 s.t. F1 = 1, F = F âŠ¤. Since âˆ¥K âˆ’Fâˆ¥1 â‰¥
âˆ¥(K âˆ’F)1âˆ¥1 for any matrix F, we must have:
r â‰¥âˆ¥(K âˆ’F)1âˆ¥1 = âˆ¥D1 âˆ’1âˆ¥1 = âˆ¥D âˆ’Iâˆ¥1.
Let F = K âˆ’D + I, then
âˆ¥K âˆ’(K âˆ’D + I)âˆ¥1 = âˆ¥D âˆ’Iâˆ¥1.
The Laplacian matrix of a graph is D âˆ’K. If v is an eigenvector of the
Laplacian D âˆ’K with eigenvalue Î», then v is also an eigenvector of Kâ€² =
K âˆ’D + I with eigenvalue 1 âˆ’Î» and since (D âˆ’K)1 = 0 then the smallest
eigenvector v = 1 of the Laplacian is the largest of Kâ€², and the second
smallest eigenvector of the Laplacian (the ratio-cut result) corresponds to the
second largest eigenvector of Kâ€². Because the eigenvectors are orthogonal,
the second eigenvector must have positive and negative entries (because the
inner-product with 1 is zero) â€” thus the sign of the entries of the second
eigenvector determines the class membership.
6.3 Spectral Clustering: Ratio-Cuts and Normalized-Cuts
65
Ratio-Cuts, the second smallest eigenvector of the Laplacian D âˆ’K, is
an approximation due to Hall in the 70s [2] to the Min-Cut formulation.
Let z âˆˆRm determine the class membership such that xi and xj would be
clustered together if zi and zj have similar values. This leads to the following
optimization problem:
min
z
1
2
X
i,j
(zi âˆ’zj)2Kij
s.t.
zâŠ¤z = 1
The criterion function is equal to (1/2)zâŠ¤(Dâˆ’K)z and the derivative of the
Lagrangian (1/2)zâŠ¤(D âˆ’K)z âˆ’Î»(zâŠ¤z âˆ’1) with respect to z gives rise to
the necessary condition (D âˆ’K)z = Î»z and the Ratio-Cut scheme follows.
6.3.2 Normalized-Cuts
Normalized-Cuts looks for the closest doubly-stochastic matrix Kâ€² in relative
entropy error measure deï¬ned as:
RE(x || y) =
X
i
xi ln xi
yi
+
X
i
yi âˆ’
X
i
xi.
We will encounter the relative entropy measure in more detail later in the
course. We can show that Kâ€² must have the form Î›KÎ› for some diagonal
matrix Î›:
Claim 7 The closest doubly-stochastic matrix F under the relative-entropy
error measure to a given non-negative symmetric matrix K, i.e., which min-
imizes:
min
F
RE(F||K) s.t. F â‰¥0, F = F âŠ¤, F1 = 1, F âŠ¤1 = 1
has the form F = Î›KÎ› for some (unique) diagonal matrix Î›.
Proof: The Lagrangian of the problem is:
L() =
X
ij
fij ln fij
kij
+
X
ij
kij âˆ’
X
ij
fij âˆ’
X
i
Î»i(
X
j
fij âˆ’1)âˆ’
X
j
Âµj(
X
i
fij âˆ’1)
The derivative with respect to fij is:
âˆ‚L
âˆ‚fij
= ln fij + 1 âˆ’ln kij âˆ’1 âˆ’Î»i âˆ’Âµj = 0
from which we obtain:
fij = eÎ»ieÂµjkij
66
Spectral Analysis II: Clustering
Let D1 = diag(eÎ»1, ..., eÎ»n) and D2 = diag(eÂµ1, ..., eÂµn), then we have:
F = D1KD2
Since F = F âŠ¤and K is symmetric we must have D1 = D2.
Next, we can show that the diagonal matrix Î› can found by an iterative
process where K is replaced by Dâˆ’1/2KDâˆ’1/2 where D was deï¬ned above
as diag(K1):
Claim 8 For any non-negative symmetric matrix K(0), iterating the process
K(t+1) â†Dâˆ’1/2K(t)Dâˆ’1/2 with D = diag(K(t)1) converges to a doubly
stochastic matrix.
The proof is based on showing that the permanent increases monotonically,
i.e. perm(K(t+1)) â‰¥perm(K(t)). Because the permanent is bounded the
process must converge and if the permanent does not change (at the conver-
gence point) the resulting matrix must be doubly stochastic. The resulting
doubly stochastic matrix is the closest to K in relative-entropy.
Normalized-Cuts takes the result of the ï¬rst iteration by replacing K
with Kâ€² = Dâˆ’1/2KDâˆ’1/2 followed by the spectral decomposition (in case
of k = 2 classes the partitioning information is found in the second leading
eigenvector of Kâ€² â€” just like Ratio-Cuts but with a diï¬€erent Kâ€²). Thus, Kâ€²
in this manner is not the closest doubly-stochastic matrix to K but is fairly
close (the ï¬rst iteration is the dominant one in the process).
Normalized-Cuts, as the second leading eigenvector of Kâ€² = Dâˆ’1/2KDâˆ’1/2,
is an approximation to a â€balancedâ€ Min-Cut described ï¬rst in [6]. Deriving
it from ï¬rst principles proceeds as follows:
Let sum(V1, V2) = sumiâˆˆV1,jâˆˆV2Kij be deï¬ned for any two subsets (not
necessarily disjoint) of vertices. The normalized-cuts measures the cut cost
as a fraction of the total edge connections to all the nodes in the graph:
Ncuts(A, B) = cut(A, B)
sum(A, V ) + cut(A, B)
sum(B, V ).
A minimal Ncut partition will no longer favor small isolated points since the
cut value would most likely be a large percentage of the total connections
from that small set to all the other vertices. A related measure Nassoc(A, B)
deï¬ned as:
Nassoc(A, B) = sum(A, A)
sum(A, V ) + sum(B, B)
sum(B, V ),
reï¬‚ects how tightly on average nodes within the group are connected to each
other. Given that cut(A, B) = sum(A, V )âˆ’sum(A, A) one can easily verify
6.3 Spectral Clustering: Ratio-Cuts and Normalized-Cuts
67
that:
Ncuts(A, B) = 2 âˆ’Nassoc(A, B),
therefore the optimal bi-partition can be represented as maximizing Nassoc(A, V âˆ’
A). The Nassoc naturally extends to k > 2 classes (partitions) as follows:
Let Ïˆ1, ..., Ïˆk be disjoint sets âˆªjÏˆj = V , then:
Nassoc(Ïˆ1, ..., Ïˆk) =
k
X
j=1
sum(Ïˆj, Ïˆj)
sum(Ïˆj, V ) .
We will now rewrite Nassoc in matrix form and establish equivalence to
eqn. 6.7. Let Â¯G = [g1, ..., gk] with gj = 1/
p
sum(Ïˆj, V )(0, ..., 0, 1, ...1, 0., , , 0)
with the 1s indicating membership to the jâ€™th class. Note that
gâŠ¤
j Kgj = sum(Ïˆj, Ïˆj)
sum(Ïˆj, V ) ,
therefore trace( Â¯GâŠ¤K Â¯G) = Nassoc(Ïˆ1, ..., Ïˆk).
Note also that gâŠ¤
i Dgi =
(1/sum(Ïˆi, V )) P
râˆˆÏˆi dr = 1, therefore Â¯GâŠ¤D Â¯G = I. Let G = D1/2 Â¯G so we
have that GâŠ¤G = I and trace(GâŠ¤Dâˆ’1/2KDâˆ’1/2G) = Nassoc(Ïˆ1, ..., Ïˆk).
Taken together we have that maximizing Nassoc is equivalent to:
max
GâˆˆRmÃ—k trace(GâŠ¤Kâ€²G)
s.t. G â‰¥0, GâŠ¤G = I,
(6.8)
where Kâ€² = Dâˆ’1/2KDâˆ’1/2. Note that this is exactly the K-means matrix
setup of eqn. 6.5 where the doubly-stochastic constraint is relaxed into the
replacement of K by Kâ€². The constraint G â‰¥0 is then dropped and the
resulting solution for G is the k leading eigenvectors of Kâ€².
We have arrived via seemingly diï¬€erent paths to eqn. 6.8 which after we
drop the constraint G â‰¥0 we end up with a closed form solution consisting
of the k leading eigenvectors of Kâ€².
When k = 2 (two classes) one can
easily verify that the partitioning information is fully contained in the second
eigenvector.
Let v1, v2 be the ï¬rst leading eigenvectors of Kâ€².
Clearly
v = D1/21 is an eigenvector with eigenvalue Î» = 1:
Dâˆ’1/2KDâˆ’1/2(D1/21) = Dâˆ’1/2K1 = D1/21.
In fact Î» = 1 is the largest eigenvalue (left as an exercise) thus v1 = D1/21 >
0.
Since Kâ€² is symmetric the vâŠ¤
2 v1 = 0 thus v2 contains positive and
negative entries â€” those are interpreted as indicating class membership
(positive to one class and negative to the other).
The case k > 2 is treated as an embedding (also known as Multi-Dimensional
Scaling) by re-coordinating the points xi using the rows of G.
In other
68
Spectral Analysis II: Clustering
words, the iâ€™th row of G is a representation of xi in Rk. Under ideal con-
ditions where K is block diagonal (the distance between clusters is inï¬nity)
the rows associated with points clustered together are identical (i.e., the n
original points are mapped to k points in Rk) [5]. In practice, one performs
the iterative K-means in the embedded space.
7
The Formal (PAC) Learning Model
We have see so far algorithms that explicitly estimate the underlying dis-
tribution of the data (Bayesian methods and EM) and algorithms that are
in some sense optimal when the underlying distribution is Gaussian (PCA,
LDA). We have also encountered an algorithm (SVM) that made no as-
sumptions on the underlying distribution and instead tied the accuracy to
the margin of the training data.
In this lecture and in the remainder of the course we will address the
issue of â€accuracyâ€ and â€generalizationâ€ in a more formal manner. Because
the learner receives only a ï¬nite training sample, the learning function can
do very well on the training set yet perform badly on new input instances.
What we would like to establish are certain guarantees on the accuracy of
the learner measured over all the instance space and not only on the training
set. We will then use those guarantees to better understand what the large-
margin principle of SVM is doing in the context of generalization.
In the remainder of this lecture we will refer to the following notations:
the class of learning functions is denoted by C. A learning functions is often
referred to as a â€conceptâ€ or â€hypothesisâ€. A target function ct âˆˆC is a
function that has zero error on all input instances (such a function may not
always exist).
7.1 The Formal Model
In many learning situations of interest, we would like to assume that the
learner receives m examples sampled by some ï¬xed (yet unknown) distri-
bution D and the learner must do its best with the training set in order to
achieve the accuracy and conï¬dence objectives. The Probably Approximate
Correct (PAC) model, also known as the â€formal modelâ€, ï¬rst introduced by
69
70
The Formal (PAC) Learning Model
Valient in 1984, provides a probabilistic setting which formalizes the notions
of accuracy and conï¬dence.
The PAC model makes the following statistical assumption. We assume
the learner receives a set S of m instances x1, ..., xm âˆˆX which are sampled
randomly and independently according to a distribution D over X. In other
words, a random training set S of length m is distributed according to the
product probability distribution Dm. The distribution D is unknown, but
we will see that one can obtain useful results by simply assuming that D is
ï¬xed â€” there is no need to attempt to recover D during the learning process.
To recap, we make the following three assumptions: (i) D is unkown, (ii)
D is ï¬xed throughout the learning process, and (iii) the example instances
are sampled independently of each other (are Identically and Independently
Distributed â€” i.i.d.).
We distinguish between the â€realizableâ€ case where a target concept ct(x)
is known to exist, and the unrealizable case, where there is no such guar-
antee. In the realizable case our training examples are Z = {(xi, ct(xi)},
i = 1, ..., m and D is deï¬ned over X (since yi âˆˆY are given by ct(xi)). In
the unrealizable case, Z = {(xi, yi)} and D is the distribution over X Ã— Y
(each element is a pair, one from X and the other from Y ).
We next deï¬ne what is meant by the error induced by a concept function
h(x). In the realizable case, given a function h âˆˆC, the error of h is deï¬ned
with respect to the distribution D:
err(h) = probD[x : ct(x) Ì¸= h(x)] =
Z
xâˆˆX
ind(ct(x) Ì¸= h(x))D(x)dx
where ind(F) is an indication function which returns â€™1â€™ if the proposition
F is true and â€™0â€™ otherwise.
The function err(h) is the probability that
an instance x sampled according to D will be labeled incorrectly by h(x).
Let Ïµ > 0 be a parameter given to the learner specifying the â€accuracyâ€ of
the learning process, i.e. we would like to achieve err(h) â‰¤Ïµ. Note that
err(ct) = 0.
In addition, we deï¬ne a â€conï¬denceâ€ parameter Î´ > 0, also given to the
learner, which deï¬nes the probability that err(h) > Ïµ, namely,
prob[err(h) > Ïµ] < Î´,
or equivalently:
prob[err(h) â‰¤Ïµ] â‰¥1 âˆ’Î´.
In other words, the learner is supposed to meet some accuracy criteria but
is allowed to deviate from it by some small probability. Finally, the learning
7.1 The Formal Model
71
algorithm is supposed to be â€eï¬ƒcientâ€ if the running time is polynomial in
1/Ïµ, ln(1/Î´), n and the size of the concept target function ct() (measured by
the number of bits necessary for describing it, for example).
We will say that an algorithm L learns a concept family C in the formal
sense (PAC learnable) if for any ct âˆˆC and for every distribution D on the
instance space X, the algorithm L generates eï¬ƒciently a concept function
h âˆˆC such that the probability that err(h) â‰¤Ïµ is at least 1 âˆ’Î´.
The inclusion of the conï¬dence value Î´ could seem at ï¬rst unnatural.
What we desire from the learner is to demonstrate a consistent performance
regardless of the training sample Z. In other words, it is not enough that
the learner produces a hypothesis h whose accuracy is above threshold, i.e.,
err(h) â‰¤Ïµ, for some training sample Z. We would like the accuracy per-
formance to hold under all training samples (sampled from the distribution
Dm) â€” since this requirement could be too diï¬ƒcult to satisfy, the formal
model allows for some â€failuresâ€, i.e, situations where err(h) > Ïµ, for some
training samples Z, as long as those failures are rare and the frequency of
their occurrence is controlled (the parameter Î´) and can be as small as we
like.
In the unrealizable case, there may be no function h âˆˆC for which
err(h) = 0, thus we need to deï¬ne what we mean by the best a learning
algorithm can achieve:
Opt(C) = min
hâˆˆC err(h),
which is the best that can be done on the concept class C using functions
that map between X and Y . Given the desired accuracy Ïµ and conï¬dence Î´
values the learner seeks a hypothesis h âˆˆC such that:
prob[err(h) â‰¤Opt(C) + Ïµ] â‰¥1 âˆ’Î´.
We are ready now to formalize the discussion above and introduce the deï¬-
nition of the formal learning model (Anthony & Bartlett [1], pp. 16):
Deï¬nition 1 (Formal Model) Let C be the concept class of functions that
map from a set X to Y . A learning algorithm L is a function:
L :
âˆ
[
m=1
{(xi, yi)}m
i=1 â†’C
from the set of all training examples to C with the following property: given
any Ïµ, Î´ âˆˆ(0, 1) there is an integer m0(Ïµ, Î´) such that if m â‰¥m0 then, for
any probability distribution D on X Ã— Y , if Z is a training set of length m
72
The Formal (PAC) Learning Model
drawn randomly according to the product probability distribution Dm, then
with probability of at least 1 âˆ’Î´ the hypothesis h = L(Z) âˆˆC output by
L is such that err(h) â‰¤Opt(C) + Ïµ. We say that C is learnable (or PAC
learnable) if there is a learning algorithm for C.
There are few points to emphasize. The sample size m0(Ïµ, Î´) is a suf-
ï¬cient sample size for PAC learning C by L and is allowed to vary with
Ïµ, Î´. Decreasing the value of either Ïµ or Î´ makes the learning problem more
diï¬ƒcult and in turn a larger sample size is required. Note however that
m0(Ïµ, Î´) does not depend on the distribution D! that is, a suï¬ƒcient sample
size can be given that will work for any distribution D â€” provided that
D is ï¬xed throughout the learning experience (both training and later for
testing). This point is a crucial property of the formal model because if the
suï¬ƒcient sample size is allowed to vary with the distribution D then not
only we would need to have some information about the distribution in or-
der to set the sample complexity bounds, but also an adversary (supplying
the training set) could control the rate of convergence of L to a solution
(even if that solution can be proven to be optimal) and make it arbitrarily
slow by suitable choice of D.
What makes the formal model work in a distribution-invariant manner
is that it critically depends on the fact that in many interesting learning
scenarios the concept class C is not too complex. For example, we will show
later in the lecture that any ï¬nite concept class |C| < âˆis learnable, and
the sample complexity (in the realizable case) is
m â‰¥1
Ïµ ln |C|
Î´ .
In the next lecture we will consider concept classes of inï¬nite size and show
that despite the fact that the class is inï¬nite it still can be of low complexity!
Before we illustrate the concepts above with an example, there is another
useful measure which is the empirical error (also known as the sample error)
Ë†
err(h) which is deï¬ned as the proportion of examples from Z on which h
made a mistake:
Ë†
err(h) = 1
m|{i : h(xi) Ì¸= ct(xi)}|
(replace ct(xi) with yi for the unrealizable case). The situation of bounding
the true error err(h) by minimizing the sample error
Ë†
err(h) is very conve-
nient â€” we will get to that later.
7.2 The Rectangle Learning Problem
73
7.2 The Rectangle Learning Problem
As an illustration of learnability we will consider the problem (introduced
in Kearns & Vazirani [3]) of learning an axes-aligned rectangle from positive
and negative examples. We will show that the problem is PAC-learnable
and ï¬nd out m0(Ïµ, Î´).
In the rectangle learning game we are given a training set consisting of
points in the 2D plane with a positive â€™+â€™ or negative â€™-â€™ label. The positive
examples are sampled inside the target rectangle (parallel to the main axes)
R and the negative examples are sampled outside of R. Given m examples
sampled i.i.d according to some distribution D the learner is supposed to
generate an approximate rectangle Râ€² which is consistent with the training
set (we are assuming that R exists) and which satisï¬es the accuracy and
conï¬dence constraints.
We ï¬rst need to decide on a learning strategy.
Since the solution Râ€²
is not uniquely deï¬ned given any training set Z, we need to add further
constraints to guarantee a unique solution. We will choose Râ€² as the axes-
aligned concept which gives the tightest ï¬t to the positive examples, i.e., the
smallest area axes-aligned rectangle which contains the positive examples.
If no positive examples are given then Râ€² = âˆ…. We can also assume that
Z contains at least three non-collinear positive examples in order to avoid
complications associated with inï¬nitesimal area rectangles. Note that we
could have chosen other strategies, such as the middle ground between the
tightest ï¬t to the positive examples and the tightest ï¬t (from below) to the
negative examples, and so forth. Deï¬ning a strategy is necessary for the
analysis below â€” the type of strategy is not critical though.
We next deï¬ne the error err(Râ€²) on the concept Râ€² generated by our
learning strategy. We ï¬rst note that with the strategy deï¬ned above we
always have Râ€² âŠ‚R since Râ€² is the tightest ï¬t solution which is consistent
with the sample data (there could be a positive example outside of Râ€² which
is not in the training set). We will deï¬ne the â€weightâ€ w(E) of a region E
in the plane as
w(E) =
Z
xâˆˆE
D(x)dx,
i.e., the probability that a random point sampled according to the distri-
bution D will fall into the region. Therefore, the error associated with the
concept Râ€² is
err(Râ€²) = w(R âˆ’Râ€²)
74
The Formal (PAC) Learning Model
Fig. 7.1. Given the tightest-ï¬t to positive examples strategy we have that Râ€² âŠ‚R.
The strip T1 has weight Ïµ/4 and the strip T â€²
1 is deï¬ned as the upper strip covering
the area between R and Râ€².
and we wish to bound the error w(R âˆ’Râ€²) â‰¤Ïµ with probability of at least
1 âˆ’Î´ after seeing m examples.
We will divide the region R âˆ’Râ€² into four strips T â€²
1, ..., T â€²
4 (see Fig.7.1)
which overlap at the corners. We will estimate prob(w(T â€²
i) â‰¥Ïµ
4) noting that
the overlaps between the regions makes our estimates more pessimistic than
they truly are (since we are counting the overlapping regions twice) thus
making us lean towards the conservative side in our estimations.
Consider the upper strip T â€²
1. If w(T â€²
1 â‰¤
Ïµ
4) then we are done. We are
however interested in quantifying the probability that this is not the case.
Assume w(T â€²
1) > Ïµ
4 and deï¬ne a strip T1 which starts from the upper axis
of R and stretches to the extent such that w(T1) = Ïµ
4. Clearly T1 âŠ‚T â€²
1. We
have that w(T â€²
1) > Ïµ
4 iï¬€T1 âŠ‚T â€²
1. Furthermore:
Claim 9 T1 âŠ‚T â€²
1 iï¬€x1, ..., xm Ì¸âˆˆT1.
Proof:
If xi âˆˆT1 the the label must be positive since T1 âŠ‚R. But if
the label is positive then given our learning strategy of ï¬tting the tightest
rectangle over the positive examples, then xi âˆˆRâ€². Since T1 Ì¸âŠ‚Râ€² it follows
that xi Ì¸âˆˆT1.
We have therefore that w(T â€²
1 > Ïµ
4) iï¬€no point in T1 appears in the sample
S = {x1, ..., xm} (otherwise T1 intersects with Râ€² and thus T â€²
1 âŠ‚T1). The
probability that a point sampled according to the distribution D will fall
outside of T1 is 1 âˆ’Ïµ
4. Given the independence assumption (examples are
7.3 Learnability of Finite Concept Classes
75
drawn i.i.d.), we have:
prob(x1, ..., xm Ì¸âˆˆT1) = prob(w(T â€²
1 > Ïµ
4)) = (1 âˆ’Ïµ
4)m.
Repeating the same analysis to regions T â€²
2, T â€²
3, T â€²
4 and using the union bound
P(A âˆªB) â‰¤P(A) + P(B) we come to the conclusion that the probability
that any of the four strips of R âˆ’Râ€² has weight greater that Ïµ/4 is at most
4(1 âˆ’Ïµ
4)m. In other words,
prob(err(Lâ€²) â‰¥Ïµ) â‰¤4((1 âˆ’Ïµ
4)m â‰¤Î´.
We can make the expression more convenient for manipulation by using the
inequality eâˆ’x â‰¥1 âˆ’x (recall that 1 + (1/n))n < e from which it follows
that (1 + z)1/z < e and by taking the power of rz where r â‰¥0 we obtain
(1 + z)r < erz then set r = 1, z = âˆ’x):
4(1 âˆ’Ïµ
4)m â‰¤4eâˆ’Ïµm
4 â‰¤Î´,
from which we obtain the bound:
m â‰¥4
Ïµ ln 4
Î´ .
To conclude, assuming that the learner adopts the tightest-ï¬t to positive ex-
amples strategy and is given at least m0 = 4
Ïµ ln 4
Î´ training examples in order
to ï¬nd the axes-aligned rectangle Râ€², we can assert that with probability
1 âˆ’Î´ the error associated with Râ€² (i.e., the probability that an (m + 1)â€™th
point will be classiï¬ed incorrectly) is at most Ïµ.
We can see form the analysis above that indeed it applies to any distri-
bution D where the only assumption we had to make is the independence
of the draw. Also, the sample size m behaves well in the sense that if one
desires a higher level of accuracy (smaller Ïµ) or a higher level of conï¬dence
(smaller Î´) then the sample size grows accordingly.
The growth of m is
linear in 1/Ïµ and linear in ln(1/Î´).
7.3 Learnability of Finite Concept Classes
In the previous section we illustrated the concept of learnability with a
particular simple example. We will now focus on applying the learnability
model to a more general family of learning examples. We will consider the
family of all learning problems over ï¬nite concept classes |C| < âˆ. For
example, the conjunction learning problem (over boolean formulas) with n
literals contains only 3n hypotheses because each variable can appear in
the conjunction or not and if appears it could be negated or not. We have
76
The Formal (PAC) Learning Model
shown that n is the lower bound on the number of mistakes on the worst
case analysis any on-line algorithm can achieve. With the deï¬nitions we
have above on the formal model of learnability we can perform accuracy
and sample complexity analysis that will apply to any learning problem
over ï¬nite concept classes. This was ï¬rst introduced by Valiant in 1984.
In the realizable case over |C| < âˆ, we will show that any algorithm L
which returns a hypothesis h âˆˆC which is consistent with the training set
Z is a learning algorithm for C. In other words, any ï¬nite concept class
is learnable and the learning algorithms simply need to generate consistent
hypotheses. The sample complexity m0 associated with the choice of Ïµ and
Î´ can be shown as equal to: 1
Ïµ ln |C|
Î´ .
In the unrealizable case, any algorithm L that generates a hypothesis
h âˆˆC that minimizes the empirical error (the error obtained on Z) is a
learning algorithm for C. The sample complexity can be shown as equal to:
2
Ïµ2 ln 2|C|
Î´ . We will derive these two cases below.
7.3.1 The Realizable Case
Let h âˆˆC be some consistent hypothesis with the training set Z (we know
that such a hypothesis exists, in particular h = ct the target concept used
for generating Z) and suppose that
err(h) = prob[x âˆ¼D : h(x) Ì¸= ct(x)] > Ïµ.
Then, the probability (with respect to the product distribution Dm) that h
agrees with ct on a random sample of length m is at most (1 âˆ’Ïµ)m. Using
the inequality we saw before eâˆ’x â‰¥1 âˆ’x we have:
prob[err(h) > Ïµ && h(xi) = ct(xi),
i = 1, ..., m] â‰¤(1 âˆ’Ïµ)m < eâˆ’Ïµm.
We wish to bound the error uniformly, i.e., that err(h) â‰¤Ïµ for all concepts
h âˆˆC. This requires the evaluation of:
prob[max
hâˆˆC {err(h) > Ïµ} && h(xi) = ct(xi),
i = 1, ..., m].
There at most |C| such functions h, therefore using the Union-Bound the
probability that some function in C has error larger than Ïµ and is consistent
7.3 Learnability of Finite Concept Classes
77
with ct on a random sample of length m is at most |C|eâˆ’Ïµm:
prob[âˆƒh : err(h) > Ïµ && h(xi) = ct(xi),
i = 1, ..., m]
â‰¤
X
h:err(h)>Ïµ
prob[h(xi) = ct(xi),
i = 1, ..., m]
â‰¤|h : err(h) > Ïµ|eâˆ’Ïµm
â‰¤|C|eâˆ’Ïµm
For any positive Î´, this probability is less than Î´ provided:
m â‰¥1
Ïµ ln |C|
Î´ .
This derivation can be summarized in the following theorem (Anthony &
Bartlett [1], pp. 25):
Theorem 5 Let C be a ï¬nite set of functions from X to Y .
Let L be
an algorithm such that for any m and for any ct âˆˆC, if Z is a training
sample {(xi, ct(xi))}, i = 1, ..., m, then the hypothesis h = L(Z) satisï¬es
h(xi) = ct(xi). Then L is a learning algorithm for C in the realizable case
with sample complexity
m0 = 1
Ïµ ln |C|
Î´ .
7.3.2 The Unrealizable Case
In the realizable case an algorithm simply needs to generate a consistent
hypothesize to be considered a learning algorithm in the formal sense. In
the unrealizable situation (a target function ct might not exist) an algorithm
which minimizes the empirical error, i.e., an algorithm L generates h = L(Z)
having minimal sample error:
Ë†
err(L(Z)) = min
hâˆˆC Ë†
err(h)
is a learning algorithm for C (assuming ï¬nite |C|). This is a particularly
useful property given that the true errors of the functions in C are un-
known. It seems natural to use the sample errors Ë†
err(h) as estimates to the
performance of L.
The fact that given a large enough sample (training set Z) then the sam-
ple error
Ë†
err(h) becomes close to the true error err(h) is somewhat of a
restatement of the â€law of large numbersâ€ of probability theory. For ex-
ample, if we toss a coin many times then the relative frequency of â€™headsâ€™
approaches the true probability of â€™headâ€™ at a rate determined by the law of
78
The Formal (PAC) Learning Model
large numbers. We can bound the probability that the diï¬€erence between
the empirical error and the true error of some h exceeds Ïµ using Hoeï¬€dingâ€™s
inequality:
Claim 10 Let h be some function from X to Y = {0, 1}. Then
prob[| Ë†
err(h) âˆ’err(h)| â‰¥Ïµ] â‰¤2e(âˆ’2Ïµ2m),
for any probability distribution D, any Ïµ > 0 and any positive integer m.
Proof:
This is a straightforward application of Hoeï¬€dingâ€™s inequality to
Bernoulli variables. Hoeï¬€dingâ€™s inequality says: Let X be a set, D a proba-
bility distribution on X, and f1, ..., fm real-valued functions fi : X â†’[ai, bi]
from X to an interval on the real line (ai < bi). Then,
prob
"
| 1
m
m
X
i=1
fi(xi) âˆ’Exâˆ¼D[f(x)]| â‰¥Ïµ
#
â‰¤2e
âˆ’
2Ïµ2m2
P
i(biâˆ’ai)2
(7.1)
where
Exâˆ¼D[f(x)] = 1
m
m
X
i=1
Z
fi(x)D(x)dx.
In our case fi(xi) = 1 iï¬€h(xi) Ì¸= yi and ai = 0, bi = 1.
Therefore
(1/m) P
i fi(xi) = Ë†
err(h) and err(h) = Exâˆ¼D[f(x)].
The Hoeï¬€ding bound almost does what we need, but not quite so. What
we have is that for any given hypothesis h âˆˆC, the empirical error is
close to the true error with high probability.
Recall that our goal is to
minimize err(h) over all possible h âˆˆC but we can access only
Ë†
err(h). If
we can guarantee that the two are close to each other for every h âˆˆC, then
minimizing
Ë†
err(h) over all h âˆˆC will approximately minimize err(h). Put
formally, in order to ensure that L learns the class C, we must show that
prob

max
hâˆˆC | Ë†
err(h) âˆ’err(h)| < Ïµ

> 1 âˆ’Î´
In other words, we need to show that the empirical errors converge (at high
probability) to the true errors uniformly over C as m â†’âˆ. If that can be
guaranteed, then with (high) probability 1 âˆ’Î´, for every h âˆˆC,
err(h) âˆ’Ïµ < Ë†
err(h) < err(h) + Ïµ.
So, since the algorithm L running on training set Z returns h = L(Z) which
minimizes the empirical error, we have:
err(L(Z)) â‰¤Ë†
err(L(Z)) + Ïµ = min
h
Ë†
err(h) + Ïµ â‰¤Opt(C) + 2Ïµ,
7.3 Learnability of Finite Concept Classes
79
which is what is needed in order that L learns C. Thus, what is left is to
prove the following claim:
Claim 11
prob

max
hâˆˆC | Ë†
err(h) âˆ’err(h)| â‰¥Ïµ

â‰¤2|C|eâˆ’2Ïµ2m
Proof:
We will use the union bound. Finding the maximum over C is
equivalent to taking the union of all the events:
prob

max
hâˆˆC | Ë†
err(h) âˆ’err(h)| â‰¥Ïµ

= prob
" [
hâˆˆC
{Z : | Ë†
err(h) âˆ’err(h)| â‰¥Ïµ}
#
,
using the union-bound and Claim 2, we have:
â‰¤
X
hâˆˆC
prob [| Ë†
err(h) âˆ’err(h)| â‰¥Ïµ] â‰¤|C|2e(âˆ’2Ïµ2m).
Finally, given that 2|C|eâˆ’2Ïµ2m â‰¤Î´ we obtain the sample complexity:
m0 = 2
Ïµ2 ln 2|C|
Î´
.
This discussion is summarized with the following theorem (Anthony & Bartlett
[1], pp. 21):
Theorem 6 Let C be a ï¬nite set of functions from X to Y = {0, 1}. Let L
be an algorithm such that for any m and for any training set Z = {(xi, yi)},
i = 1, ..., m, then the hypothesis L(Z) satisï¬es:
Ë†
err(L(Z)) = min
hâˆˆC Ë†
err(h).
Then L is a learning algorithm for C with sample complexity m0 = 2
Ïµ2 ln 2|C|
Î´ .
Note that the main diï¬€erence with the realizable case (Theorem 1) is the
larger 1/Ïµ2 rather than 1/Ïµ. The realizable case requires a smaller training
set since we are estimating a random quantity so the smaller the variance
the less data we need.
8
The VC Dimension
The result of the PAC model (also known as the â€formalâ€ learning model)
is that if the concept class C is PAC-learnable then the learning strategy
must simply consist of gathering a suï¬ƒciently large training sample S of
size m > mo(Ïµ, Î´), for given accuracy Ïµ > 0 and conï¬dence 0 < Î´ < 1
parameters, and ï¬nds a hypothesis h âˆˆC which is consistent with S. The
learning algorithm is then guaranteed to have a bounded error err(h) < Ïµ
with probability 1 âˆ’Î´. The error measurement includes data not seen by
the training phase.
This state of aï¬€air also holds (with some slight modiï¬cations on the sam-
ple complexity bounds) when there is no consistent hypothesis (the unreal-
izable case). In this case the learner simply needs to minimize the empirical
error
Ë†
err(h) on the sample training data S, and if m is suï¬ƒciently large
then the learner is guaranteed to have err(h) < Opt(C)+Ïµ with probability
1 âˆ’Î´. The measure Opt(C) is deï¬ned as the minimal err(g) over all g âˆˆC.
Note that in the realizable case Opt(C) = 0.
The property of bounding the true error err(h) by minimizing the sample
error
Ë†
err(h) is very convenient. The fundamental question is under what
conditions this type of generalization property applies? We saw in the previ-
ous lecture that a satisfactorily answer can be provided when the cardinality
of the concept space is bounded, i.e. |C| < âˆ, which happens for Boolean
concept space for example. In that lecture we have proven that:
mo(Ïµ, Î´) = O(1
Ïµ ln |C|
Î´ ),
is suï¬ƒcient for guaranteeing a learning model in the formal sense, i.e., which
has the generalization property described above.
In this lecture and the one that follows we have two goals in mind. First
is to generalize the result of ï¬nite concept class cardinality to inï¬nite car-
80
8.1 The VC Dimension
81
dinality â€” note that the bound above is not meaningful when |C| = âˆ.
Can we learn in the formal sense any non-trivial inï¬nite concept class? (we
already saw an example of a PAC-learnable inï¬nite concept class which is
the class of axes aligned rectangles). In order to answer this question we will
need to a general measure of concept class complexity which will replace the
cardinality term |C| in the sample complexity bound mo(Ïµ, Î´). It is tempting
to assume that the number of parameters which fully describe the concepts
of C can serve as such a measure, but we will show that in fact one needs
a more powerful measure called the Vapnik-Chervonenkis (VC) dimension.
Our second goal is to pave the way and provide the theoretical foundation
for the large margin principle algorithm (SVM) we derived in Lecture 4.
8.1 The VC Dimension
The basic principle behind the VC dimension measure is that although C
may have inï¬nite cardinality, the restriction of the application of concepts
in C to a ï¬nite sample S has a ï¬nite outcome. This outcome is typically
governed by an exponential growth with the size m of the sample S â€” but
not always. The point at which the growth stops being exponential is when
the â€complexityâ€ of the concept class C has exhausted itself, in a manner
of speaking.
We will assume C is a concept class over the instance space X â€” both
of which can be inï¬nite. We also assume that the concept class maps in-
stances in X to {0, 1}, i.e., the input instances are mapped to â€positiveâ€
or â€negativeâ€ labels. A training sample S is drawn i.i.d according to some
ï¬xed but unknown distribution D and S consists of m instances x1, ..., xm.
In our notations we will try to reserve c âˆˆC to denote the target concept
and h âˆˆC to denote some concept. We begin with the following deï¬nition:
Deï¬nition 2
Î C(S) = {(h(x1), ..., h(xm) : h âˆˆC}
which is a set of vectors in {0, 1}m.
Î C(S) is set whose members are m-dimensional Boolean vectors induced by
functions of C. These members are often called dichotomies or behaviors on
S induced or realized by C. If C makes a full realization then Î C(S) will
have 2m members. An equivalent description is a collection of subsets of S:
Î C(S) = {h âˆ©S : h âˆˆC}
where each h âˆˆC makes a partition of S into two sets â€” the positive and
82
The VC Dimension
negative points. The set Î C(S) contains therefore subsets of S (the positive
points of S under h). A full realization will provide Pm
i=0
 m
i

= 2m. We
will use both descriptions of Î C(S) as a collection of subsets of S and as a
set of vectors interchangeably.
Deï¬nition 3 If |Î C(S)| = 2m then S is considered shattered by C. In
other words, S is shattered by C if C realizes all possible dichotomies of S.
Consider as an example a ï¬nite concept class C = {c1, ..., c4} applied to
three instance vectors with the results:
x1
x2
x3
c1
1
1
1
c2
0
1
1
c3
1
0
0
c4
0
0
0
Then,
Î C({x1}) = {(0), (1)}
shattered
Î C({x1, x3}) = {(0, 0), (0, 1), (1, 0), (1, 1)}
shattered
Î C({x2, x3}) = {(0, 0), (1, 1)}
not shattered
With these deï¬nitions we are ready to describe the measure of concept class
complexity.
Deï¬nition 4 (VC dimension) The VC dimension of C, noted as V Cdim(C),
is the cardinality d of the largest set S shattered by C. If all sets S (arbi-
trarily large) can be shattered by C, then V Cdim(C) = âˆ.
V Cdim(C) = max{d | âˆƒ|S| = d, and |Î C(S)| = 2d}
The VC dimension of a class of functions C is the point d at which all
samples S with cardinality |S| > d are no longer shattered by C. As long as
C shatters S it manifests its full â€richnessâ€ in the sense that one can obtain
from S all possible results (dichotomies).
Once that ceases to hold, i.e.,
when |S| > d, it means that C has â€exhaustedâ€ its richness (complexity).
An inï¬nite VC dimension means that C maintains full richness for all sample
sizes. Therefore, the VC dimension is a combinatorial measure of a function
class complexity.
Before we consider a number of examples of geometric concept classes
and their VC dimension, it is important clarify the lower and upper bounds
(existential and universal quantiï¬ers) in the deï¬nition of VC dimension.
The VC dimension is at least d if there exists some sample |S| = d which is
8.1 The VC Dimension
83
shattered by C â€” this does not mean that all samples of size d are shattered
by C. Conversely, in order to show that the VC dimension is at most d,
one must show that no sample of size d + 1 is shattered. Naturally, proving
an upper bound is more diï¬ƒcult than proving the lower bound on the VC
dimension. The following examples are shown in a â€hand waivingâ€ style
and are not meant to form rigorous proofs of the stated bounds â€” they are
shown for illustrative purposes only.
Intervals of the real line: The concept class C is governed by two param-
eters Î±1, Î±2 in the closed interval [0, 1]. A concept from this class will tag an
input instance 0 < x < 1 as positive if Î±1 â‰¤x â‰¤Î±2 and negative otherwise.
The VC dimension is at least 2: select a sample of 2 points x1, x2 positioned
in the open interval (0, 1). We need to show that there are values of Î±1, Î±2
which realize all the possible four dichotomies (+, +), (âˆ’, âˆ’), (+, âˆ’), (âˆ’, +).
This is clearly possible as one can place the interval [Î±1, Î±2] such the inter-
section with the interval [x1, x2] is null, (thus producing (âˆ’, âˆ’)), or to fully
include [x1, x2] (thus producing (+, +)) or to partially intersect [x1, x2] such
that x1 or x2 are excluded (thus producing the remaining two dichotomies).
To show that the VC dimension is at most 2, we need to show that any
sample of three points x1, x2, x3 on the line (0, 1) cannot be shattered. It is
suï¬ƒcient to show that one of the dichotomies is not realizable: the labeling
(+, âˆ’, +) cannot be realizable by any interval [Î±1, Î±2] â€” this is because if
x1, x3 are labeled positive then by deï¬nition the interval [Î±1, Î±2] must fully
include the interval [x1, x3] and since x1 < x2 < x3 then x2 must be labeled
positive as well. Thus V Cdim(C) = 2.
Axes-aligned rectangles in the plane: We have seen this concept class
in the previous lecture â€” a point in the plane is labeled positive if it lies
in an axes-aligned rectangle. The concept class C is thus governed by 4
parameters. The VC dimension is at least 4: consider a conï¬guration of
4 input points arranged in a cross pattern (recall that we need only to
show some sample S that can be shattered). We can place the rectangles
(concepts of the class C) such that all 16 dichotomies can be realized (for
example, placing the rectangle to include the vertical pair of points and
exclude the horizontal pair of points would induce the labeling (+, âˆ’, +, âˆ’)).
It is important to note that in this case, not all conï¬gurations of 4 points
can be shattered â€” but to prove a lower bound it is suï¬ƒcient to show
the existence of a single shattered set of 4 points. To show that the VC
dimension is at most 4, we need to prove that any set of 5 points cannot
84
The VC Dimension
be shattered.
For any set of 5 points there must be some point that is
â€internalâ€, i.e., is neither the extreme left, right, top or bottom point of the
ï¬ve. If we label this internal point as negative and the remaining 4 points as
positive then there is no axes-aligned rectangle (concept) which cold realize
this labeling (because if the external 4 points are labeled positive then they
must be fully within the concept rectangle, but then the internal point must
also be included in the rectangle and thus labeled positive as well).
Separating hyperplanes: Consider ï¬rst linear half spaces in the plane.
The lower bound on the VC dimension is 3 since any three (non-collinear)
points in R2 can be shattered, i.e., all 8 possible labelings of the three points
can be realized by placing a separating line appropriately. By having one
of the points on one side of the line and the other two on the other side we
can realize 3 dichotomies and by placing the line such that all three points
are on the same side will realize the 4th. The remaining 4 dichotomies are
realized by a sign ï¬‚ip of the four previous cases. To show that the upper
bound is also 3, we need to show that no set of 4 points can be shattered.
We consider two cases: (i) the four points form a convex region, i.e., lie on
the convex hull deï¬ned by the 4 points, (ii) three of the 4 points deï¬ne the
convex hull and the 4th point is internal. In the ï¬rst case, the labeling which
is positive for one diagonal pair and negative to the other pair cannot be
realized by a separating line. In the second case, a labeling which is positive
for the three hull points and negative for the interior point cannot be realize.
Thus, the VC dimension is 3 and in general the VC dimension for separating
hyperplanes in Rn is n + 1.
Union of a ï¬nite number of intervals on the line: This is an example
of a concept class with an inï¬nite VC dimension. For any sample of points
on the line, one can place a suï¬ƒcient number of intervals to realize any
labeling.
The examples so far were simple enough that one might get the wrong
impression that there is a correlation between the number of parameters
required to describe concepts of the class and the VC dimension.
As a
counter example, consider the two parameter concept class:
C = {sign(sin(Ï‰x + Î¸) : Ï‰}
which has an inï¬nite VC dimension as one can show that for every set
of m points on the line one can realize all possible labelings by choosing
8.2 The Relation between VC dimension and PAC Learning
85
a suï¬ƒciently large value of Ï‰ (which serves as the frequency of the sync
function) and appropriate phase.
We conclude this section with the following claim:
Theorem 7 The VC dimension of a ï¬nite concept class |C| < âˆis bounded
from above:
V Cdim(C) â‰¤log2 |C|.
Proof:
if V Cdim(C) = d then there exists at least 2d functions in C
because every function induces a labeling and there are at least 2d labelings.
Thus, from |C| â‰¥2d follows that d â‰¤log2 |C|.
8.2 The Relation between VC dimension and PAC Learning
We saw that the VC dimension is a combinatorial measure of concept class
complexity and we would like to have it replace the cardinality term in the
sample complexity bound.
The ï¬rst result of interest is to show that if
the VC dimension of the concept class is inï¬nite then the class is not PAC
learnable.
Theorem 8 Concept class C with V Cdim(C) = âˆis not learnable in the
formal sense.
Proof: Assume the contrary that C is PAC learnable. Let L be the learning
algorithm and m be the number of training examples required to learn the
concept class with accuracy Ïµ = 0.1 and 1 âˆ’Î´ = 0.9. That is, after seeing
at least m(Ïµ, Î´) training examples, the learner generates a concept h which
satisï¬es p(err(h) â‰¤0.1) â‰¥0.9.
Since the VC dimension is inï¬nite there exist a sample set S with 2m
instances which is shattered by C. Since the formal model (PAC) applies
to any training sample we will use the set S as follows. We will deï¬ne a
probability distribution on the instance space X which is uniform on S (with
probability
1
2m) and zero everywhere else.
Because S is shattered, then any target concept is possible so we will
choose our target concept c in the following manner:
prob(ct(xi) = 0) = 1
2
âˆ€xi âˆˆS,
in other words, the labels ct(xi) are determined by a coin ï¬‚ip. The learner
L selects an i.i.d. sample of m instances Â¯S â€” which due to the structure of
86
The VC Dimension
D means that the Â¯S âŠ‚S and outputs a consistent hypothesis h âˆˆC. The
probability of error for each xi Ì¸âˆˆÂ¯S is:
prob(ct(xi) Ì¸= h(xi)) = 1
2.
The reason for that is because S is shattered by C, i.e., we can select any
target concept for any labeling of S (the 2m examples) therefore we could
select the labels of the m points not seen by the learner arbitrarily (by
ï¬‚ipping a coin). Regardless of h, the probability of mistake is 0.5. The
expectation on the error of h is:
E[err(h)] = m Â· 0 Â· 1
2m + m Â· 1
2 Â· 1
2m = 1
4.
This is because we have 2m points to sample (according to D as all other
points have zero probability) from which the error on half of them is zero
(as h is consistent on the training set Â¯S) and the error on the remaining half
is 0.5. Thus, the average error is 0.25. Note that E[err(h)] = 0.25 for any
choice of Ïµ, Î´ as it is based on the sample size m. For any sample size m we
can follow the construction above and generate the learning problem such
that if the learner produces a consistent hypothesis the expectation of the
error will be 0.25.
The result that E[err(h)] = 0.25 is not possible for the accuracy and
conï¬dence values we have set: with probability of at least 0.9 we have that
err(h) â‰¤0.1 and with probability 0.1 then err(h) = Î² where 0.1 < Î² â‰¤1.
Taking the worst case of Î² = 1 we come up with the average error:
E[err(h)] â‰¤0.9 Â· 0.1 + 0.1 Â· 1 = 0.19 < 0.25.
We have therefore arrived to a contradiction that C is PAC learnable.
We next obtain a bound on the growth of |Î S(C)| when the sample size
|S| = m is much larger than the VC dimension V Cdim(C) = d of the
concept class. We will need few more deï¬nitions:
Deï¬nition 5 (Growth function)
Î C(m) = max{|Î S(C)| : |S| = m}
The measure Î C(m) is the maximum number of dichotomies induced by C
for samples of size m. As long as m â‰¤d then Î C(m) = 2m. The question
is what happens to the growth pattern of Î C(m) when m > d. We will
see that the growth becomes polynomial â€” a fact which is crucial for the
learnability of C.
8.2 The Relation between VC dimension and PAC Learning
87
Deï¬nition 6 For any natural numbers m, d we have the following deï¬nition:
Î¦d(m)
=
Î¦d(m âˆ’1) + Î¦dâˆ’1(m âˆ’1)
Î¦d(0)
=
Î¦0(m) = 1
By induction on m, d it is possible to prove the following:
Theorem 9
Î¦d(m) =
d
X
i=0
m
i

Proof:
by induction on m, d. For details see [[3], pp. 56].
For m â‰¤d we have that Î¦d(m) = 2m.
For m > d we can derive a
polynomial upper bound as follows.
 d
m
d
d
X
i=0
m
i

â‰¤
d
X
i=0
 d
m
i m
i

â‰¤
m
X
i=0
 d
m
i m
i

= (1 + d
m)m â‰¤ed
From which we obtain:
 d
m
d
Î¦d(m) â‰¤ed.
Dividing both sides by
  d
m
d yields:
Î¦d(m) â‰¤ed m
d
d
=
em
d
d
= O(md).
We need one more result before we are ready to present the main result of
this lecture:
Theorem 10 (Sauerâ€™s lemma) If V Cdim(C) = d, then for any m,
Î C(m) â‰¤Î¦d(m).
Proof:
By induction on both d, m. For details see [[3], pp. 55â€“56].
Taken together, we have now a fairly interesting characterization on how
the combinatorial measure of complexity of the concept class C scales up
with the sample size m. When the VC dimension of C is inï¬nite the growth
is exponential, i.e., Î C(m) = 2m for all values of m. On the other hand,
when the concept class has a bounded VC dimension V Cdim(C) = d < âˆ
then the growth pattern undergoes a discontinuity from an exponential to
a polynomial growth:
Î C(m) =
(
2m
m â‰¤d
â‰¤
  em
d
d
m > d
)
88
The VC Dimension
As a direct result of this observation, when m >> d is much larger than d
the entropy becomes much smaller than m. Recall than from an informa-
tion theoretic perspective, the entropy of a random variable Z with discrete
values z1, ..., zn with probabilities pi, i = 1, ..., n is deï¬ned as:
H(Z) =
n
X
i=0
pi log2
1
pi
,
where I(pi) = log2
1
pi is a measure of â€informationâ€, i.e., is large when
pi is small (meaning that there is much information in the occurrence of
an unlikely event) and vanishes when the event is certain pi = 1.
The
entropy is therefore the expectation of information. Entropy is maximal for
a uniform distribution H(Z) = log2 n. The entropy in information theory
context can be viewed as the number of bits required for coding z1, ..., zn. In
coding theory it can be shown that the entropy of a distribution provides the
lower bound on the average length of any possible encoding of a uniquely
decodable code fro which one symbol goes into one symbol.
When the
distribution is uniform we will need the maximal number of bits, i.e., one
cannot compress the data. In the case of concept class C with VC dimension
d, we see that one when m â‰¤d all possible dichotomies are realized and
thus one will need m bits (as there are 2m dichotomies) for representing
all the outcomes of the sample.
However, when m >> d only a small
fraction of the 2m dichotomies can be realized, therefore the distribution
of outcomes is highly non-uniform and thus one would need much less bits
for coding the outcomes of the sample. The technical results which follow
are therefore a formal way of expressing in a rigorous manner this simple
truth â€” If it is possible to compress, then it is possible to learn. The crucial
point is that learnability is a direct consequence of the â€phase transitionâ€
(from exponential to polynomial) in the growth of the number of dichotomies
realized by the concept class.
In the next lecture we will continue to prove the â€double samplingâ€ the-
orem which derives the sample size complexity as a function of the VC
dimension.
9
The Double-Sampling Theorem
In this lecture will use the measure of VC dimension, which is a combina-
torial measure of concept class complexity, to bound the sample size com-
plexity.
9.1 A Polynomial Bound on the Sample Size m for PAC Learning
In this section we will follow the material presented in Kearns & Vazirani
[3] pp. 57â€“61 and prove the following:
Theorem 11 (Double Sampling) Let C be any concept class of VC di-
mension d. Let L be any algorithm that when given a set S of m labeled
examples {xi, c(xi)}i, sampled i.i.d according to some ï¬xed but unknown
distribution D over the instance space X, of some concept c âˆˆC, produces
as output a concept h âˆˆC that is consistent with S. Then L is a learning
algorithm in the formal sense provided that the sample size obeys:
m â‰¥c0
1
Ïµ log 1
Î´ + d
Ïµ log 1
Ïµ

for some constant c0 > 0.
The idea behind the proof is to build an â€approximateâ€ concept space which
includes concepts arranged such that the distance between the approximate
concepts h and the target concept c is at least Ïµ â€” where distance is de-
ï¬ned as the weight of the region in X which is in conï¬‚ict with the target
concept. To formalize this story we will need few more deï¬nitions. Unless
speciï¬ed otherwise, c âˆˆC denotes the target concept and h âˆˆC denotes
some concept.
89
90
The Double-Sampling Theorem
Deï¬nition 7
câˆ†h = hâˆ†c = {x : c(x) Ì¸= h(x)}
câˆ†h is the region in instance space where both concepts do not agree â€”
the error region. The probability that x âˆˆcâˆ†h is equal to (by deï¬nition)
err(h).
Deï¬nition 8
âˆ†(c)
=
{hâˆ†c : h âˆˆC}
âˆ†Ïµ(c)
=
{hâˆ†c : h âˆˆC and err(h) â‰¥Ïµ}
âˆ†(c) is a set of error regions, one per concept h âˆˆC over all concepts. The
error regions are with respect to the target concept. The set âˆ†Ïµ(c) âŠ‚âˆ†(c)
is the set of all error regions whose weight exceeds Ïµ. Recall that weight is
deï¬ned as the probability that a point sampled according to D will hit the
region.
It will be important for later to evaluate the VC dimension of âˆ†(c). Unlike
C, we are not looking for the VC dimension of a class of function but the
VC dimension of a set of regions in space. Recall the deï¬nition of Î C(S)
from the previous lecture: there were two equivalent deï¬nitions one based
on a set of vectors each representing a labeling of the instances of S induced
by some concept. The second, yet equivalent, deï¬nition is based on a set
of subsets of S each induced by some concept (where the concept divides
the sample points of S into positive and negative labeled points). So far it
was convenient to work with the ï¬rst deï¬nition, but for evaluating the VC
dimension of âˆ†(c) it will be useful to consider the second deï¬nition:
Î âˆ†(c)(S) = {r âˆ©S : r âˆˆâˆ†(c)},
that is, the collection of subsets of S induced by intersections with regions
of âˆ†(c). An intersection between S and a region r is deï¬ned as the subset of
points from S that fall into r. We can easily show that the VC dimensions
of C and âˆ†(c) are equal:
Lemma 1
V Cdim(C) = V Cdim(âˆ†(c)).
Proof:
we have that the elements of Î C(S) and Î âˆ†(c)(S) are susbsets of
S, thus we need to show that for every S the cardinality of both sets is
equal |Î C(S)| = |Î âˆ†(c)(S)|. To do that it is suï¬ƒcient to show that for every
element s âˆˆÎ C(S) there is a unique corresponding element in Î âˆ†(c)(S). Let
9.1 A Polynomial Bound on the Sample Size m for PAC Learning
91
câˆ©S be the subset of S induced by the target concept c. The set s (a subset
of S) is realized by some concept h (those points in S which were labeled
positive by h). Therefore, the set s âˆ©(c âˆ©S) is the subset of S containing
the points that hit the region hâˆ†c which is an element of Î âˆ†(c)(S). Since
this is a one-to-one mapping we have that |Î C(S)| = |Î âˆ†(c)(S)|.
Deï¬nition 9 (Ïµ-net) For every Ïµ > 0, a sample set S is an Ïµ-net for âˆ†(c)
if every region in âˆ†Ïµ(c) is hit by at least one point of S:
âˆ€r âˆˆâˆ†Ïµ(c),
S âˆ©r Ì¸= âˆ….
In other words, if S hits all the error regions in âˆ†(c) whose weight exceeds
Ïµ, then S is an Ïµ-net. Consider as an example the concept class of intervals
on the line [0, 1]. A concept is deï¬ned by an interval [Î±1, Î±2] such that all
points inside the interval are positive and all those outside are negative.
Given c âˆˆC is the target concept and h âˆˆC is some concept, then the
error region hâˆ†c is the union of two intervals: I1 consists of all points x âˆˆh
which are not in c, and I2 the interval of all points x âˆˆc but which are not
in h. Assume that the distribution D is uniform (just for the sake of this
example) then, prob(x âˆˆI) = |I| which is the length of the interval I. As a
result, err(h) > Ïµ if either |I1| > Ïµ/2 or |I2| > Ïµ/2. The sample set
S = {x = kÏµ
2
: k = 0, 1, ..., 2/Ïµ}
contains sample points from 0 to 1 with increments of Ïµ/2. Therefore, every
interval larger than Ïµ must be hit by at least one point from S and by
deï¬nition S is an Ïµ-net.
It is important to note that if S forms an Ïµ-net then we are guaranteed
that err(h) â‰¤Ïµ. Let h âˆˆC be the consistent hypothesis with S (returned
by the learning algorithm L).
Becuase h is consistent, hâˆ†c âˆˆâˆ†(c) has
not been hit by S (recall that hâˆ†c is the error region with respect to the
target concept c, thus if h is consistent then it agrees with c over S and
therefore S does not hit hâˆ†c). Since S forms an Ïµ-net for âˆ†(c) we must
have hâˆ†c Ì¸âˆˆâˆ†Ïµ(c) (recall that by deï¬nition S hits all error regions with
weight larger than Ïµ). As a result, the error region hâˆ†c must have a weight
smaller than Ïµ which means that err(h) â‰¤Ïµ.
The conclusion is that if we can bound the probability that a random sam-
ple S does not form an Ïµ-net for âˆ†(c), then we have bounded the probability
that a concept h consistent with S has err(h) > Ïµ. This is the goal of the
proof of the double-sampling theorem which we are about to prove below:
92
The Double-Sampling Theorem
Proof (following Kearns & Vazirani [3] pp. 59â€“61): Let S1 be a ran-
dom sample of size m (sampled i.i.d. according to the unknown distribution
D) and let A be the event that S1 does not form an Ïµ-net for âˆ†(c). From
the preceding discussion our goal is to upper bound the probability for A to
occur, i.e., prob(A) â‰¤Î´.
If A occurs, i.e., S1 is not an Ïµ-net, then by deï¬nition there must be some
region r âˆˆâˆ†Ïµ(c) which is not hit by S1, that is S1 âˆ©r = âˆ…. Note that
r = hâˆ†(c) for some concept h which is consistent with S1. At this point the
space of possibilities is inï¬nite, because the probability that we fail to hit
hâˆ†(c) in m random examples is at most (1âˆ’Ïµ)m. Thus the probability that
we fail to hit some hâˆ†c âˆˆâˆ†Ïµ(c) is bounded from above by |âˆ†(c)|(1 âˆ’Ïµ)m
â€” which does not help us due to the fact that |âˆ†(c)| is inï¬nite. The idea
of the proof is to turn this into a ï¬nite space by using another sample, as
follows.
Let S2 be another random sample of size m. We will select m (for both
S1 and S2) to guarantee a high probability that S2 will hit r many times.
In fact we wish that S2 will hit r at least Ïµm
2 with probability of at least 0.5:
prob(|S2 âˆ©r| > Ïµm
2 ) = 1 âˆ’prob(|S2 âˆ©r| â‰¤Ïµm
2 ).
We will use the Chernoï¬€bound (lower tail) to obtain a bound on the right-
hand side term.
Recall that if we have m Bernoulli trials (coin tosses)
Z1, ..., Zm with expectation E(Zi) = p and we consider the random variable
Z = Z1 + ... + Zm with expectation E(Z) = Âµ (note that Âµ = pm) then for
all 0 < Ïˆ < 1 we have:
prob(Z < (1 âˆ’Ïˆ)Âµ) â‰¤eâˆ’ÂµÏˆ2
2 .
Considering the sampling of m examples that form S2 as Bernoulli trials,
we have that Âµ â‰¥Ïµm (since the probability that an example will hit r is at
least Ïµ) and Ïˆ = 0.5. We obtain therefore:
prob(|S2 âˆ©r| â‰¤(1 âˆ’1
2)Ïµm) â‰¤eâˆ’Ïµm
8 = 1
2
which happens when m =
8
Ïµ ln 2 = O(1
Ïµ).
To summarize what we have
obtained so far, we have calculated the probability that S2 will hit r many
times given that r was ï¬xed using the previous sampling, i.e., given that S1
does not form an Ïµ-net. To formalize this, let B denote the combined event
that S1 does not form an Ïµ-event and S2 hits r at least Ïµm/2 times. Then,
we have shown that for m = O(1/Ïµ) we have:
prob(B/A) â‰¥1
2.
9.1 A Polynomial Bound on the Sample Size m for PAC Learning
93
From this we can calculate prob(B):
prob(B) = prob(B/A)prob(A) â‰¥1
2prob(A),
which means that our original goal of bounding prob(A) is equivalent to
ï¬nding a bound prob(B) â‰¤Î´/2 because prob(A) â‰¤2 Â· prob(B) â‰¤Î´. The
crucial point with the new goal is that to analyze the probability of the
event B, we need only to consider a ï¬nite number of possibilities, namely to
consider the regions of
Î âˆ†Ïµ(c)(S1 âˆªS2) = {r âˆ©{S1 âˆªS2} : r âˆˆâˆ†Ïµ(c)} .
This is because the occurrence of the event B is equivalent to saying that
there is some r âˆˆÎ âˆ†Ïµ(c)(S1 âˆªS2) such that |r| â‰¥Ïµm/2 (i.e., the region r
is hit at least Ïµm/2 times) and S1 âˆ©r = âˆ…. This is because Î âˆ†Ïµ(c)(S1 âˆªS2)
contains all the subsets of S1 âˆªS2 realized as intersections over all regions
in âˆ†Ïµ(c). Thus even though we have an inï¬nite number of regions we still
have a ï¬nite number of subsets. We wish therefore to analyze the following
probability:
prob
 r âˆˆÎ âˆ†Ïµ(c)(S1 âˆªS2) : |r| â‰¥Ïµm/2 and S1 âˆ©r = âˆ…

.
Let S = S1âˆªS2 a random sample of 2m (note that since the sampling is i.i.d.
it is equivalent to sampling S1 and S2 separately) and r satisfying |r| â‰¥Ïµm/2
being ï¬xed. Consider some random partitioning of S into S1 and S2 and
consider then the problem of estimating the probability that S1 âˆ©r = âˆ….
This problem is equivalent to the following combinatorial question: we have
2m balls, each colored Red or Blue, with exaclty l â‰¥Ïµm/2 Red balls. We
divide the 2m balls into groups of equal size S1 and S2 and we are interested
in bounding the probability that all of the l balls fall in S2 (that is, the
probability that S1 âˆ©r = âˆ…). This in turn is equivalent to ï¬rst dividing the
2m uncolored balls into S1 and S2 groups and then randomly choose l of
the balls to be colored Red and analyze the probability that all of the Red
balls fall into S2. This probability is exactly
 m
l

 2m
l
 =
lâˆ’1
Y
i=0
m âˆ’i
2m âˆ’i â‰¤
lâˆ’1
Y
i=0
1
2 = 1
2l = 2âˆ’Ïµm/2.
This probability was evaluated for a ï¬xed S and r. Thus, the probability that
this occurs for some r âˆˆÎ âˆ†Ïµ(c)(S) satisfying |r| â‰¥Ïµm/2 (which is prob(B))
can be calculated by summing over all possible ï¬xed r and applying the
94
The Double-Sampling Theorem
union bound prob(P
i Zi) â‰¤P
i prob(Zi):
prob(B)
â‰¤
|Î âˆ†Ïµ(c)(S)|2âˆ’Ïµm/2 â‰¤|Î âˆ†(c)(S)|2âˆ’Ïµm/2
=
|Î C(S)|2âˆ’Ïµm/2 â‰¤
2Ïµm
d
d
2âˆ’Ïµm/2 â‰¤Î´
2,
from which it follows that:
m = O
1
Ïµ log 1
Î´ + d
Ïµ log 1
Ïµ

.
Few comments are worthwhile at this point:
(i) It is possible to show that the upper bound on the sample complexity
m is tight by showing that the lower bound on m is â„¦(d/Ïµ) (see [[3],
pp. 62]).
(ii) The treatment above holds also for the unrealizable case (target con-
cept c Ì¸âˆˆC) with slight modiï¬cations to the bound. In this context,
the learning algorithm L must simply minimize the sample (empiri-
cal) error Ë†
err(h) deï¬ned:
Ë†
err(h) = 1
m|{i : h(xi) Ì¸= yi}|
xi âˆˆS.
The generalization of the double-sampling theorem (Derroyeâ€™82) states
that the empirical errors converge uniformly to the true errors:
prob

max
hâˆˆC |
Ë†
err(h) âˆ’err(h)| â‰¥Ïµ

â‰¤4e(4Ïµ+4Ïµ2)
Ïµm2
d
d
2âˆ’mÏµ2/2 â‰¤Î´,
from which it follows that
m = O
 1
Ïµ2 log 1
Î´ + d
Ïµ2 log 1
Ïµ

.
Taken together, we have arrived to a fairly remarkable result. Despite the
fact that the distribution D from which the training sample S is drawn from
is unknown (but is known to be ï¬xed), the learner simply needs to minimize
the empirical error. If the sample size m is large enough the learner is guar-
anteed to have minimized the true errors for some accuracy and conï¬dence
parameters which deï¬ne the sample size complexity. Equivalently,
|Opt(C) âˆ’Ë†
err(h)| âˆ’â†’mâ†’âˆ0.
Not only is the convergence is independent of D but also the rate of con-
vergence is independent (namely, it does not matter where the optimal hâˆ—
9.2 Optimality of SVM Revisited
95
is located). The latter is very important because without it one could ar-
bitrarily slow down the convergence rate by maliciously choosing D. The
beauty of the results above is that D does not have an eï¬€ect at all â€” one
simply needs to choose the sample size to be large enough for the accuracy,
conï¬dence and VC dimension of the concept class to be learned over.
9.2 Optimality of SVM Revisited
In Lecture 4 we discussed the large margin principle for ï¬nding an optimal
separating hyperplane. It is natural to ask how does the PAC theory pre-
sented so far explains why a maximal margin hyperplane is optimal with
regard to the formal sense of learning (i.e. to generalization from empirical
errors to true errors)? We saw in the previous section that the sample com-
plexity m(Ïµ, Î´, d) depends also on the VC dimension of the concept class â€”
which is n + 1 for hyperplanes in Rn. Thus, another natural question that
may certainly arise is what is the gain in employing the â€kernel trickâ€? For a
ï¬xed m, mapping the input instance space X of dimension n to some higher
(exponentially higher) feature space might simply mean that we are compro-
mising the accuracy and conï¬dence of the learner (since the VC dimension
is equal to the instance space dimension plus 1).
Given a ï¬xed sample size m, the best the learner can do is to minimize the
empirical error and at the same time to try to minimize the VC dimension d
of the concept class. The smaller d is, for a ï¬xed m, the higher the accuracy
and conï¬dence of the learning algorithm. Likewise, the smaller d is, for a
ï¬xed accuracy and conï¬dence values, the smaller sample size is required.
There are two possible ways to decrease d. First is to decrease the dimen-
sion n of the instance space X. This amounts to â€feature selectionâ€, namely
ï¬nd a subset of coordinates that are the most â€relevantâ€ to the learning
task r perform a dimensionality reduction via PCA, for example. A second
approach is to maximize the margin. Let the margin associated with the
separating hyperplane h (i.e. consistent with the sample S) be Î³. Let the
input vectors x âˆˆX have a bounded norm, |x| â‰¤R. It can be shown that
the VC dimension of the concept class CÎ³ of hyperplanes with margin Î³ is:
CÎ³ = min
R2
Î³2 , n

+ 1.
Thus, if the margin is very small then the VC dimension remains n + 1. As
the margin gets larger, there comes a point where R2/Î³2 < n and as a result
the VC dimension decreases. Moreover, mapping the instance space X to
some higher dimension feature space will not change the VC dimension as
96
The Double-Sampling Theorem
long as the margin remains the same. It is expected that the margin will not
scale down or will not scale down as rapidly as the scaling up of dimension
from image space to feature space.
To conclude, maximizing the margin (while minimizing the empirical er-
ror) is advantageous as it decreases the VC dimension of the concept class
and causes the accuracy and conï¬dence values of the learner to be largely
immune to dimension scaling up while employing the kernel trick.
10
Appendix
97
98
Appendix
A0.1 Variance, Covariance, etc.
Let X, Y be two random variables and let f(x, y) be some function on x âˆˆ
X, y âˆˆY , and let p(x, y) be the probability of the event x and y occurring
together. The expectation E[f(x, y)] is deï¬ned:
E[f(x, y)] =
X
xâˆˆX
X
yâˆˆY
f(x, y)p(x, y)
. The mean, variance and covariance are deï¬ned:
Âµx
=
E[X] =
X
x
X
y
xp(x, y)
Âµy
=
E[Y ] =
X
x
X
y
yp(x, y)
Ïƒ2
x
=
V ar[X] = E[(x âˆ’Âµx)2] =
X
x
X
y
(x âˆ’Âµx)2p(x, y)
Ïƒ2
y
=
V ar[Y ] = E[(y âˆ’Âµy)2] =
X
x
X
y
(y âˆ’Âµy)2p(x, y)
Ïƒxy
=
Cov(XY ) = E[(x âˆ’Âµx)(y âˆ’Âµy)] =
X
x
X
y
(x âˆ’Âµx)(y âˆ’Âµy)p(x, y)
In vector-matrix notation, let x represent the n random variables of X1, ..., Xn,
i.e., x = (x1, ..., xn)âŠ¤is an instance vector and p(x) is the probability of the
instance occurrence. Then the mean is a vector Âµ and the covariance matrix
E are deï¬ned:
Âµ
=
X
xâˆˆ{X1,...,Xn}
xp(x)
E
=
X
x
(x âˆ’Âµ)(x âˆ’Âµ)âŠ¤p(x)
Note that the covariance matrix E is the linear superposition of rank-1
matrices (xâˆ’Âµ)(xâˆ’Âµ)âŠ¤with coeï¬ƒcients p(x). The diagonal of E containes
the variances of the variables x1, ..., xn.
For a uniform distribution and
a sample data S consisting of m points, let A = [x1 âˆ’Âµ, ..., xm âˆ’Âµ] be
the matrix whose columns consist of the points centered around the mean:
Âµ = 1
m
P
i xi. The (sample) covariance matrix is E = 1
mAAâŠ¤.
A0.2 Derivatives of Matrix Operations: Scalar Functions of a Vector
99
A0.2 Derivatives of Matrix Operations: Scalar Functions of a
Vector
The two most important examples of a scalar function of a vector x are the
linear form aâŠ¤x and the quadratic form xâŠ¤Ax for some square matrix A.
d(aâŠ¤x)
=
aâŠ¤dx
d(xâŠ¤Ax)
=
(dx)âŠ¤Ax + xâŠ¤A(dx)
=

(dx)âŠ¤Ax
âŠ¤
+ xâŠ¤A(dx)
=
xâŠ¤(A + AâŠ¤)dx
where the derivative d(xâŠ¤Ax) using the rule of products d(f Â· g) = (df) Â· g +
f Â· (dg) where g = Ax and f = xâŠ¤and noting that d(Ax) = Adx. Thus,
d
dx(aâŠ¤x) = aâŠ¤and
d
dx(xâŠ¤Ax)) = xâŠ¤(A + AâŠ¤). If A is symmetric then
d
dx(xâŠ¤Ax)) = (2Ax)âŠ¤.
A0.3 Primer on Constrained Optimization
A0.3.1 Equality Constraints and Lagrange Multipliers
Consider ï¬rst the general optimization with equality constraints which gives
rise to the notion of Lagrange multipliers.
min
x
f(x)
(0.1)
subject to
h(x) = 0
where f : Rn â†’R and h : Rn â†’Rk where h is a vector function (h1, ..., hk)
each from Rn to R. We want to derive a necessary and suï¬ƒcient constraint
for a point xo to be a local minimum subject to the k equality constraints
h(x) = 0. Assume that xo is a regular point, meaning that the gradient
vectors âˆ‡hj(x) are linearly independent. Note that âˆ‡h(xo) is a kÃ—n matrix
and the null space of this matrix:
null(âˆ‡h(xo)) = {y : âˆ‡h(xo)y = 0}
deï¬nes the tangent plane at the point xo. We have the following fundamental
theorem:
âˆ‡f(xo) âŠ¥null(âˆ‡h(xo))
in other words, all vectors y spanning the tangent plane at the point xo are
also perpendicular to the gradient of f at xo.
100
Appendix
The sketch of the proof is as follows. Let x(t), âˆ’a â‰¤t < a, be a smooth
curve on the surface h(x) = 0, i.e., h(x(t)) = 0. Let xo = x(0) and y =
d
dtx(0) the tangent to the curve at xo. From the deï¬nition of tangency, the
vector y lives in null(âˆ‡h(xo)), i.e., y Â· âˆ‡hj(x(0)) = 0, j = 1, ..., k. Since
xo = x(0) is a local extremum of f(x), then
0 = d
dtf(x(t))|t=0 =
X âˆ‚f
âˆ‚xi
dxi
dt |t=0 = âˆ‡f(xo) Â· y.
As a corollary of this basic theorem, the gradient vector âˆ‡f(xo) âˆˆspan{âˆ‡h1(xo), ..., âˆ‡hk(xo)},
i.e.,
âˆ‡f(xo) +
k
X
i=1
Î»iâˆ‡hi(xo) = 0,
where the coeï¬ƒcients Î»i are called Lagrange Multipliers and the expression:
f(x) +
X
i
Î»ihi(x)
is called the Lagrangian of the optimization problem (0.1).
A0.3.2 Inequality Constraints and KKT conditions
Consider next the general constrained optimization with inequality con-
straints (called â€œnon-linear programmingâ€):
min
x
f(x)
(0.2)
subject to
h(x) = 0
g(x) â‰¤0
where g : Rn â†’Rs.
We will assume that the optimal solution xo is a
regular point which has the following meaning: Let J be the set of indices
j such that gj(xo) = 0, then xo is a regular point if the gradient vectors
âˆ‡hi(xo), âˆ‡gj(xo), i = 1, ..., k and j âˆˆJ are linearly independent. A basic
result (we will not prove here) is the Karush-Kuhn-Tucker (KKT) theorem:
Let xo be a local minimum of the problem and suppose xo is a regular
point. Then, there exist Î»1, ..., Î»k and Âµ1 â‰¥0, ..., Âµs â‰¥0 such that:
A0.3 Primer on Constrained Optimization
101
)
,
(
*
* z
y
)
,
(
z
y
y
z
!
Âµ =
+
y
z
G
n
R
x "
))
(
),
(
(
x
f
x
g
)
(Âµ
#
Fig. A0.1. Geometric interpreatation of Duality (see text).
âˆ‡f(xo) +
k
X
i=1
Î»iâˆ‡hi(xo) +
s
X
j=1
Âµjâˆ‡gj(xo)
=
0,
(0.3)
s
X
j=1
Âµjgj(xo)
=
0.
(0.4)
Note that the condition P Âµjgj(xo) = 0 is equivalent to the condition that
Âµjgj(xo) = 0 (since Âµ â‰¥0 and g(xo) â‰¤0 thus there sum cannot vanish
unless each term vanishes) which in turn implies: Âµj = 0 when gj(xo) < 0.
The expression
L(x, Î», Âµ) = f(x) +
k
X
i=1
Î»ihi(x) +
s
X
j=1
Âµjgj(x)
is the Lagrangian of the problem (0.2) and the associated condition Âµjgj(xo) =
0 is called the KKT condition.
The remaining concepts we need are the â€œdualityâ€ and the â€œLagrangian
Dualâ€ problem.
102
Appendix
A0.3.3 The Langrangian Dual Problem
The optimization problem (0.2) is called the â€œPrimalâ€ problem. The La-
grangian Dual problem is deï¬ned as:
max
Î»,Âµ
Î¸(Î», Âµ)
(0.5)
subject to
Âµ â‰¥0
(0.6)
where
Î¸(Î», Âµ) = min
x {f(x) +
X
i
Î»ihi(x) +
X
j
Âµjgj(x)}.
Note that Î¸(Î», Âµ) may assume the value âˆ’âˆfor some values of Î», Âµ (thus
to be rigorous we should have replaced â€œminâ€ with â€œinfâ€). The ï¬rst basic
result is the weak duality theorem:
Let x be a feasible solution to the primal (i.e., h(x) = 0, g(x) â‰¤0) and let
(Î», Âµ) be a feasible solution to the dual problem (i.e., Âµ â‰¥0), then f(x) â‰¥
Î¸(Î», Âµ)
The proof is immediate:
Î¸(Î», Âµ)
=
min
y {f(y) +
X
i
Î»ihi(y) +
X
j
Âµjgj(y)}
â‰¤
f(x) +
X
i
Î»ihi(x) +
X
j
Âµjgj(x)
â‰¤
f(x)
where the latter inequality follows from h(x) = 0 and P
j Âµjgj(x) â‰¤0
because Âµ â‰¥0 and g(x) â‰¤0. As a corollary of this theorem we have:
min
x {f(x) : h(x) = 0, g(x) â‰¤0} â‰¥max
Î»,Âµ {Î¸(Î», Âµ) : Âµ â‰¥0}.
(0.7)
The next basic result is the strong duality theorem which speciï¬es the con-
ditions for when the inequality in (0.7) becomes equality:
Let f(), g() be convex functions and let h() be aï¬ƒne, i.e., h(x) = Ax âˆ’b
where A is a k Ã— n matrix, then
min
x {f(x) : h(x) = 0, g(x) â‰¤0} = max
Î»,Âµ {Î¸(Î», Âµ) : Âµ â‰¥0}.
The strong duality theorem allows one to solve for the primal problem by
ï¬rst dualizing it and solving for the dual problem instead (we will see exactly
how to do it when we return to solving the primal problem (4.3)). When
A0.3 Primer on Constrained Optimization
103
)
,
(
*
* z
y
y
z
G
*
Âµ
optimal primal
optimal dual
Fig. A0.2. An example of duality gap arising from non-convexity (see text).
the (convexity) conditions above do not hold we obtain
min
x {f(x) : h(x) = 0, g(x) â‰¤0} > max
Î»,Âµ {Î¸(Î», Âµ) : Âµ â‰¥0}
which means that the optimal solution to the dual problem provides only a
lower bound to the primal problem â€” this situation is called a duality gap.
Taken together, the â€duality theoremâ€ summarizes the discussion so far:
Theorem 12 (Duality Theorem) In order for xâˆ—to be an optimal Primal
solution and (Î»âˆ—, Âµâˆ—) to be an optimal Dual solution, it is necessary and
suï¬ƒcient that:
(i) xâˆ—is Primal feasible,
(ii) Âµâˆ—â‰¥0 and Âµâˆ—j = 0 for all gj(xâˆ—) < 0,
(iii) xâˆ—âˆˆargminxL(x, Î»âˆ—, Âµâˆ—).
We will end this section with a geometric interpretation of duality.
A0.3.4 Geometric Interpretation of Duality
For clarity we will consider a primal problem with a single inequality con-
straint: min{f(x) : g(x) â‰¤0} where g : Rn â†’R.
Consider the set G = {(y, z) : y = g(x), z = f(x)} in the (y, z) plane. The
set G is the image of Rn under the (g, f) map (see Fig. A0.1). The primal
104
Appendix
problem is to ï¬nd a point in G that has a y â‰¤0 with the smallest z value
â€” this is the point (yâˆ—, zâˆ—) in the ï¬gure.
In this case Î¸(Âµ) = minx{f(x) + Âµg(x)} which is equivalent to minimize
z + Âµy over points in G. The equation z + Âµy = Î± represents a straight line
with slope âˆ’Âµ and intercept (on z axis) Î±. For a given value Âµ, to minimize
z + Âµy over G we need to move the line z + Âµy = Î± parallel to itself as far
down as possible while it remains in contact with G â€” in other words G
is above the line and touches it. Then, the intercept with the z axis gives
Î¸(Âµ). The dual problem is therefore equivalent to ï¬nding the slope of the
supporting hyperplane such that its intercept on the z axis is maximal.
Consider the non-convex region G in Fig. A0.2 which illustrates a duality
gap condition. The optimal primal is the point (yâˆ—, zâˆ—) which is higher than
the greatest intercept on the z axis achieved by a line that supports G from
below. This is an example of a duality gap caused by the non-convexity of
the functions f(), g() (thereby making the set G non-convex).
Bibliography
M. Anthony and P.L. Bartlett. Neural Neteowk Learning: Theoretical Foundations.
Cambridge University Press, 1999.
K.M. Hall. An r-dimensional quadratic placement algorithm. Manag. Sci., 17:219â€“
229, 1970.
M.J. Kearns and U.V. Vazirani. An Introduction to Computational Learning The-
ory. MIT Press, 1997.
Y. Linde, A. Buzo, and R.M. Gray. An algorithm for vector quantizer design. IEEE
Transactions on Communications, 1:84â€“95, 1980.
A.Y. Ng, M.I. Jordan, and Y. Weiss.
On spectral clustering: Analysis and an
algorithm. In Proceedings of the conference on Neural Information Processing
Systems (NIPS), 2001.
J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 22(8), 2000.
R. Zass and A. Shashua. A unifying approach to hard and probabilistic clustering.
In Proceedings of the International Conference on Computer Vision, Beijing,
China, Oct. 2005.
105
